{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobgreen4477/The-4th-ETRI-AI-Human-Understanding-Competition/blob/main/etri_baseline_v5_0_4(%EC%A6%9D%EA%B0%95).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTgURBTcpY0Q"
      },
      "source": [
        "> title : 111_etri_lifelog_dm_llm-impute_vF (LLM impute)  <br>\n",
        " -  ì½”ë“œ ì‹¤í–‰ ì „ PATH ë³€ê²½í•˜ì„¸ìš”.\n",
        "  - PATH  =  '/content/drive/MyDrive/data/ch2025_data_items/share/submissions/input'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ”¨ PATH ì„¤ì •"
      ],
      "metadata": {
        "id": "gv7qtgKD0q35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH  =  '/content/drive/MyDrive/data/ch2025_data_items/share/submissions/input' ### <---- ì½”ë“œ ì‹¤í–‰ ì „ PATH ë³€ê²½í•˜ì„¸ìš”."
      ],
      "metadata": {
        "id": "24XvbBI00Q_A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„°ëŠ” êµ¬ê¸€ë“œë¼ì´ë¸Œì— ì €ì¥ë˜ì–´ ìˆì–´ì„œ êµ¬ê¸€ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ë¥¼ í•©ë‹ˆë‹¤.\n",
        "# ë°ì´í„° ì €ì¥ PATHë¥¼ ë³€ê²½í•˜ì‹œë©´ ì•„ë˜ êµ¬ê¸€ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ë¥¼ ì£¼ì„ì²˜ë¦¬í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf8L2VLm0RCr",
        "outputId": "b5e716ad-f58b-43f6-8d1a-6923a7cf82b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2k-8IGKV0NZM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HM-tV1OX0Ncp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDVNXLQtLU6X"
      },
      "source": [
        "### ğŸ“¦ LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTardcDyUPKX"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# CPU times: user 1.19 s, sys: 188 ms, total: 1.37 s\n",
        "# Wall time: 4min 36s\n",
        "\n",
        "try:\n",
        "  from vllm import LLM, SamplingParams\n",
        "except:\n",
        "  !pip install -U langchain-community  >/dev/null\n",
        "  !pip install bitsandbytes >/dev/null\n",
        "  !pip install -U transformers accelerate >/dev/null\n",
        "  !pip install faiss-gpu-cu12 --no-deps >/dev/null\n",
        "  !pip install datasets >/dev/null\n",
        "  !pip install vllm >/dev/null\n",
        "  !pip install --upgrade transformers >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YpKr0iTUXT-"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# CPU times: user 12.7 s, sys: 1.53 s, total: 14.2 s\n",
        "# Wall time: 3min 24s\n",
        "\n",
        "import os\n",
        "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
        "\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token = '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')\n",
        "\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# ê²½ë¡œ\n",
        "drive_path = \"/content/drive/MyDrive/models2\"\n",
        "\n",
        "# ëª¨ë¸ëª…\n",
        "# model_id  = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
        "# model_id  = 'Qwen/Qwen2.5-14B-Instruct-1M'\n",
        "model_id   = 'Qwen/Qwen3-8B'\n",
        "\n",
        "\n",
        "# vllm\n",
        "llm = LLM(\n",
        "    model=f\"{drive_path}/{model_id}\",\n",
        "    tokenizer=f\"{drive_path}/{model_id}\",\n",
        "    tensor_parallel_size=1,\n",
        "    dtype=\"bfloat16\",     # \"bfloat16\"\n",
        "    # quantization=\"fp8\",   # fp8\n",
        "    load_format=\"auto\",\n",
        "    gpu_memory_utilization=0.8,\n",
        "    max_model_len=38960, # 6144,12288,38960,32768,40960\n",
        "    enforce_eager=True,  ## ì‹¤í–‰ ì‹œì ì—ì„œ ì¦‰ì‹œ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ì‹(ì‹±í¬ë°©ì‹)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VnBvvgmUZ-K"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# model_id = \"/content/drive/MyDrive/models2/Qwen/Qwen3-8B\"\n",
        "\n",
        "# Tokenizer ë¡œë“œ\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2aCgkAtfc1N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jfv9EJqMUafW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvWaOHaPUkrA"
      },
      "source": [
        "### ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN6iwVhQpR_a"
      },
      "outputs": [],
      "source": [
        "# íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "! pip install haversine >/dev/null\n",
        "! pip install optuna >/dev/null\n",
        "! pip install imbalanced-learn >/dev/null\n",
        "! pip install category_encoders >/dev/null\n",
        "! pip install catboost >/dev/null\n",
        "! pip install h2o >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFvEVmxWsRH4"
      },
      "outputs": [],
      "source": [
        "# Core Libraries\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import ast\n",
        "import glob\n",
        "import random\n",
        "from functools import reduce\n",
        "from io import StringIO\n",
        "from collections import Counter\n",
        "from datetime import datetime, timedelta, time\n",
        "\n",
        "# Numerical Operations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Math & Geospatial\n",
        "from math import radians, cos, sin, asin, sqrt\n",
        "from scipy.stats import entropy\n",
        "from haversine import haversine\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, KFold, StratifiedKFold, cross_val_score\n",
        ")\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import f1_score, roc_auc_score, roc_curve, log_loss, accuracy_score, precision_score, recall_score\n",
        "from lightgbm import LGBMClassifier, log_evaluation, early_stopping\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostClassifier, CatBoostRegressor\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Deep Learning (PyTorch)\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Progress Tracking\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import tqdm\n",
        "from category_encoders import TargetEncoder\n",
        "from enum import Enum\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# seed ê³ ì •\n",
        "SD = 42\n",
        "random.seed(SD)\n",
        "np.random.seed(SD)\n",
        "os.environ['PYTHONHASHSEED'] = str(SD)\n",
        "\n",
        "# pandas ì˜µì…˜\n",
        "pd.set_option('display.max_columns', 999)\n",
        "pd.set_option('display.max_rows', 999)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.float_format', lambda x: '%0.4f' % x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfNPPtgCGTk8"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from six.moves import xrange\n",
        "from catboost import Pool, CatBoostClassifier\n",
        "\n",
        "class FocalLossObjective(object):\n",
        "    def calc_ders_range(self, approxes, targets, weights):\n",
        "        # approxes, targets, weights are indexed containers of floats\n",
        "        # (containers with only __len__ and __getitem__ defined).\n",
        "        # weights parameter can be None.\n",
        "        # Returns list of pairs (der1, der2)\n",
        "        gamma = 2.\n",
        "        # alpha = 1.\n",
        "        assert len(approxes) == len(targets)\n",
        "        if weights is not None:\n",
        "            assert len(weights) == len(approxes)\n",
        "\n",
        "        exponents = []\n",
        "        for index in xrange(len(approxes)):\n",
        "            exponents.append(math.exp(approxes[index]))\n",
        "\n",
        "        result = []\n",
        "        for index in xrange(len(targets)):\n",
        "            p = exponents[index] / (1 + exponents[index])\n",
        "\n",
        "            if targets[index] > 0.0:\n",
        "                der1 = -((1-p)**(gamma-1))*(gamma * math.log(p) * p + p - 1)/p\n",
        "                der2 = gamma*((1-p)**gamma)*((gamma*p-1)*math.log(p)+2*(p-1))\n",
        "            else:\n",
        "                der1 = (p**(gamma-1)) * (gamma * math.log(1 - p) - p)/(1 - p)\n",
        "                der2 = p**(gamma-2)*((p*(2*gamma*(p-1)-p))/(p-1)**2 + (gamma-1)*gamma*math.log(1 - p))\n",
        "\n",
        "            if weights is not None:\n",
        "                der1 *= weights[index]\n",
        "                der2 *= weights[index]\n",
        "\n",
        "            result.append((der1, der2))\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcU4bIojfP2g"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnDtHMLz42G_"
      },
      "outputs": [],
      "source": [
        "def add_noise(series, noise_level, seed=3):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    return series * (1 + noise_level * rng.standard_normal(len(series)))\n",
        "\n",
        "def calculate_averages(data,name):\n",
        "    variables = ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']\n",
        "    variable_averages = {}\n",
        "    total_sum = 0\n",
        "    total_count = 0\n",
        "\n",
        "    for var in variables:\n",
        "        values = []\n",
        "        for entry in data.values():\n",
        "            if var in entry:  # í‚¤ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€\n",
        "                values.append(entry[var])\n",
        "        avg = sum(values) / len(values) if values else None  # ëˆ„ë½ëœ ë³€ìˆ˜ ì²˜ë¦¬\n",
        "        variable_averages[var] = round(avg, 6) if avg is not None else 'Missing'\n",
        "        total_sum += sum(values)\n",
        "        total_count += len(values)\n",
        "\n",
        "    overall_avg = round(total_sum / total_count, 6) if total_count > 0 else None\n",
        "    print(f'# ì „ì²´ í‰ê·  {name}: {overall_avg} {variable_averages}')\n",
        "\n",
        "    return variable_averages, overall_avg\n",
        "\n",
        "def calculate_circular_mean_sleep_time(sleep_times):\n",
        "    sleep_times = pd.Series(sleep_times).dropna()\n",
        "    if len(sleep_times) == 0:\n",
        "        return np.nan  # í˜¹ì€ return 0.0 ë“± ê¸°ë³¸ê°’ ì„¤ì • ê°€ëŠ¥\n",
        "\n",
        "    def hour_to_radian(hour):\n",
        "        return (hour % 24) / 24 * 2 * np.pi\n",
        "\n",
        "    radians = np.array([hour_to_radian(t) for t in sleep_times])\n",
        "    mean_radian = np.arctan2(np.mean(np.sin(radians)), np.mean(np.cos(radians)))\n",
        "    mean_hour = (mean_radian / (2 * np.pi)) * 24 % 24\n",
        "\n",
        "    return mean_hour\n",
        "\n",
        "def circular_mean_sleep_time(times):\n",
        "\n",
        "    # ê²°ì¸¡ì¹˜ ì œê±°\n",
        "    valid_times = [t for t in times if pd.notna(t)]\n",
        "\n",
        "    # ìœ íš¨ ë°ì´í„° ê°œìˆ˜ í™•ì¸\n",
        "    if len(valid_times) == 0:\n",
        "        return None  # ê²°ì¸¡ì¹˜ë§Œ ìˆëŠ” ê²½ìš°\n",
        "\n",
        "    # ì‹œê°„ â†’ ë¼ë””ì•ˆ ë³€í™˜\n",
        "    radians = [(t % 24) / 24 * 2 * np.pi for t in valid_times]\n",
        "\n",
        "    # ì‚¬ì¸/ì½”ì‚¬ì¸ í‰ê·  ê³„ì‚°\n",
        "    sin_sum = np.mean(np.sin(radians))\n",
        "    cos_sum = np.mean(np.cos(radians))\n",
        "\n",
        "    # í‰ê·  ê°ë„ ê³„ì‚°\n",
        "    if sin_sum == 0 and cos_sum == 0:\n",
        "        return np.nan  # ë¶ˆê°€ëŠ¥í•œ ê²½ìš°\n",
        "\n",
        "    mean_radian = np.arctan2(sin_sum, cos_sum)\n",
        "\n",
        "    # í‰ê·  ì‹œê°„ìœ¼ë¡œ ë³€í™˜\n",
        "    mean_hour = (mean_radian / (2 * np.pi)) * 24\n",
        "    if mean_hour < 0:\n",
        "        mean_hour += 24\n",
        "\n",
        "    return f'{int(mean_hour):02d}:{int((mean_hour % 1) * 60):02d}'\n",
        "\n",
        "def calculate_sleep_duration_min(sleep_time, wake_time):\n",
        "    \"\"\"\n",
        "    ì·¨ì¹¨ ì‹œê°(sleep_time)ê³¼ ê¸°ìƒ ì‹œê°(wake_time)ì„ ì…ë ¥ë°›ì•„ ìˆ˜ë©´ ì‹œê°„(ë¶„) ë°˜í™˜\n",
        "    ë‹¨ìœ„ëŠ” float ì‹œê°„ (ì˜ˆ: 23.5, 6.25)\n",
        "    \"\"\"\n",
        "    if pd.isna(sleep_time) or pd.isna(wake_time):\n",
        "        return None\n",
        "    if wake_time < sleep_time:\n",
        "        wake_time += 24  # ìì • ë„˜ê¸´ ê²½ìš° ë³´ì •\n",
        "    duration = (wake_time - sleep_time) * 60\n",
        "    return round(duration)\n",
        "\n",
        "def fill_missing_dates_by_subject(df, date_col='lifelog_date'):\n",
        "\n",
        "    df = df.copy()\n",
        "    df[date_col] = pd.to_datetime(df[date_col])\n",
        "    result = []\n",
        "\n",
        "    for sid, group in df.groupby('subject_id'):\n",
        "        group = group.sort_values(date_col)\n",
        "\n",
        "        # ì—°ì† ë‚ ì§œ ìƒì„±\n",
        "        full_dates = pd.date_range(start=group[date_col].min(), end=group[date_col].max())\n",
        "        full_df = pd.DataFrame({date_col: full_dates})\n",
        "        full_df['subject_id'] = sid\n",
        "\n",
        "        # ë³‘í•©\n",
        "        merged = pd.merge(full_df, group, on=['subject_id', date_col], how='left')\n",
        "\n",
        "        result.append(merged)\n",
        "\n",
        "    # ë³‘í•© ë° ì •ë ¬\n",
        "    final_df = pd.concat(result, ignore_index=True).sort_values(['subject_id', date_col])\n",
        "\n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0rlS5k7e5jw"
      },
      "outputs": [],
      "source": [
        "def get_time_block(hour):\n",
        "    if 1 <= hour < 5:\n",
        "        return 'sleeptime'\n",
        "    else:\n",
        "        return 'activehour'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gltffbQuggnv"
      },
      "outputs": [],
      "source": [
        "SLEEP_HOURS = tuple(range(0, 5)) ### ìˆ˜ì •\n",
        "MIGHT_GO_TO_SLEEP_HOURS = tuple(range(20, 24)) + tuple(range(0, 2))\n",
        "MIGHT_WAKEUP_HOURS = tuple(range(6, 10))\n",
        "ACTIVE_HOURS = tuple(range(7, 24))\n",
        "WORK_HOURS = tuple(range(7, 19))\n",
        "FREE_HOURS = tuple(range(19, 24))\n",
        "\n",
        "HOLIDAY_DATES = [\n",
        "    pd.Timestamp('2024-08-15'),\n",
        "    pd.Timestamp('2024-09-16'),\n",
        "    pd.Timestamp('2024-09-17'),\n",
        "    pd.Timestamp('2024-09-18'),\n",
        "    pd.Timestamp('2024-10-03'),\n",
        "    pd.Timestamp('2024-10-09'),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPGtpuhQeWPV"
      },
      "outputs": [],
      "source": [
        "class DataType(Enum):\n",
        "    mACStatus = \"mACStatus\"\n",
        "    mActivity = \"mActivity\"\n",
        "    mAmbience = \"mAmbience\"\n",
        "    mBle = \"mBle\"\n",
        "    mGps = \"mGps\"\n",
        "    mLight = \"mLight\"\n",
        "    mScreenStatus = \"mScreenStatus\"\n",
        "    mUsageStats = \"mUsageStats\"\n",
        "    mWifi = \"mWifi\"\n",
        "    wHr = \"wHr\"\n",
        "    wLight = \"wLight\"\n",
        "    wPedo = \"wPedo\"\n",
        "\n",
        "def load_data(data_type: DataType):\n",
        "    file_path = f\"{PATH}/ETRI_lifelog_dataset/ch2025_data_items/ch2025_{data_type.value}.parquet\"\n",
        "    df = pd.read_parquet(file_path)\n",
        "    df[\"subject_id\"] = df[\"subject_id\"].astype(\"category\")\n",
        "    df[\"lifelog_date\"] = df[\"timestamp\"].dt.normalize()\n",
        "    df[\"month\"] = df[\"timestamp\"].dt.month\n",
        "    df[\"day\"] = df[\"timestamp\"].dt.day\n",
        "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
        "    df[\"minute\"] = df[\"timestamp\"].dt.minute\n",
        "    df[\"weekday\"] = df[\"timestamp\"].dt.weekday\n",
        "    fixed_columns = [\"subject_id\", \"timestamp\", \"lifelog_date\", \"month\", \"day\", \"hour\", \"minute\", \"weekday\"]\n",
        "    columns = df.columns.tolist()\n",
        "    columns = fixed_columns + [col for col in columns if col not in fixed_columns]\n",
        "    df = df[columns]\n",
        "    df = df.sort_values(by=[\"subject_id\", \"timestamp\"])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf2AzkuceDMI"
      },
      "outputs": [],
      "source": [
        "def describe_df(df):\n",
        "    print(f\"# shape:\\n{df.shape}\\n\")\n",
        "    print(f\"# dtypes:\\n{df.dtypes}\\n\")\n",
        "    # print(f\"# head:\\n{df.head(3)}\\n\")\n",
        "    display(df.head(3))\n",
        "    nan_stats = df.isna().sum().to_frame(name='missing_count')\n",
        "    nan_stats['missing_ratio(%)'] = (df.isna().mean() * 100).round(2)\n",
        "    print(f\"# nan_stats:\\n\" + nan_stats.to_string() + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Tlsrq10eDP1"
      },
      "outputs": [],
      "source": [
        "def shift_lifelog_date(df, target_hours):\n",
        "    df = df.copy()\n",
        "    mask = df[\"hour\"].isin(target_hours) & df[\"hour\"].lt(12)\n",
        "    df.loc[mask, \"lifelog_date\"] = df.loc[mask, \"lifelog_date\"] - pd.Timedelta(days=1)\n",
        "    df.loc[mask, \"day\"] = df.loc[mask, \"day\"] - 1\n",
        "    df = df.sort_values(by=[\"subject_id\", \"lifelog_date\", \"timestamp\"])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBjO4zn95wjJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJUqYMh-5wqH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BodxdJFiv_DJ"
      },
      "source": [
        "### ğŸ“¦ ë°ì´í„° ì½ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw0cx3wwpSE2"
      },
      "outputs": [],
      "source": [
        "# 1\n",
        "mACStatus = pd.read_parquet(f'{PATH}/ETRI_lifelog_dataset/ch2025_data_items/ch2025_mACStatus.parquet')\n",
        "mActivity = pd.read_parquet(f'{PATH}/ETRI_lifelog_dataset/ch2025_data_items/ch2025_mActivity.parquet')\n",
        "mAmbience = pd.read_parquet(f'{PATH}/ETRI_lifelog_dataset/ch2025_data_items/ch2025_mAmbience.parquet')\n",
        "mBle = pd.read_parquet(f'{PATH}/ETRI_lifelog_dataset/ch2025_data_items/ch2025_mBle.parquet')\n",
        "mGps = pd.read_parquet(f'{PATH}/ETRI_lifelog_dataset/ch2025_data_items/ch2025_mGps.parquet')\n",
        "mLight = pd.read_parquet(f'{PATH}/ETRI_lifelog_dataset/ch2025_data_items/ch2025_mLight.parquet')\n",
        "mScreenStatus = pd.read_parquet(f'{PATH}/ETRI_lifelog_dataset/ch2025_data_items/ch2025_mScreenStatus.parquet')\n",
        "mUsageStats = pd.read_parquet(f'{PATH}/ETRI_lifelog_dataset/ch2025_data_items/ch2025_mUsageStats.parquet')\n",
        "mWifi = pd.read_parquet(f'{PATH}/ETRI_lifelog_dataset/ch2025_data_items/ch2025_mWifi.parquet')\n",
        "wHr = pd.read_parquet(f'{PATH}/ETRI_lifelog_dataset/ch2025_data_items/ch2025_wHr.parquet')\n",
        "wLight = pd.read_parquet(f'{PATH}/ETRI_lifelog_dataset/ch2025_data_items/ch2025_wLight.parquet')\n",
        "wPedo = pd.read_parquet(f'{PATH}/ETRI_lifelog_dataset/ch2025_data_items/ch2025_wPedo.parquet')\n",
        "\n",
        "# 2\n",
        "train = pd.read_csv(f'{PATH}/ETRI_lifelog_dataset/ch2025_metrics_train.csv')\n",
        "test = pd.read_csv(f'{PATH}/ETRI_lifelog_dataset/ch2025_submission_sample.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jONSq3_oqdid"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHfbbw3bbVgR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGKJkE3P_vTv"
      },
      "source": [
        "## ğŸ“¦ ë°ì´í„° ì „ì²˜ë¦¬"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W53JPEe7Oq9"
      },
      "source": [
        "### âœ”ï¸ mACStatus í•¸ë“œí° ì¶©ì „ìƒíƒœ\n",
        "- Indicates whether the smartphone is currently being charged.\n",
        "- m_charging : 0/1 ìƒíƒœ\n",
        "- í•¸ë“œí°ì´ ì˜¤ë« ë™ì•ˆ ì¶©ì „í–ˆë‹¤ëŠ” ì˜ë¯¸?\n",
        " - í•œ ìë¦¬ì— ì¥ì‹œê°„ ë¨¸ë¬¼ëŸ¬ ìˆì—ˆë‹¤.\n",
        " - í•¸ë“œí°ì„ ì¥ì‹œê°„ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë‹¤.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbbCp8nmfwCH"
      },
      "outputs": [],
      "source": [
        "def run_length_encoding(arr):\n",
        "    \"\"\"Run-Length Encoding\"\"\"\n",
        "    if len(arr) == 0:\n",
        "        return []\n",
        "\n",
        "    diffs = np.diff(np.concatenate(([0], arr, [0])))\n",
        "    run_starts = np.where(diffs == 1)[0]\n",
        "    run_ends = np.where(diffs == -1)[0]\n",
        "    return run_ends - run_starts\n",
        "\n",
        "def process_mACStatus(df):\n",
        "    status = df[\"m_charging\"].values\n",
        "\n",
        "    def _process_feature(status):\n",
        "        if len(status) == 0:\n",
        "            return 0., 0., 0., 0., 0.\n",
        "\n",
        "        # charging ìƒíƒœ ë¹„ìœ¨, í•©\n",
        "        ratio_charging = status.mean()\n",
        "        sum_charging = status.sum()\n",
        "\n",
        "        # ìƒíƒœì „ì´ íšŸìˆ˜\n",
        "        transitions = (status[1:] != status[:-1]).sum()\n",
        "\n",
        "        lengths = run_length_encoding(status)\n",
        "        avg_charging_duration = np.mean(lengths) if len(lengths) > 0 else 0\n",
        "        max_charging_duration = np.max(lengths) if len(lengths) > 0 else 0\n",
        "\n",
        "        return ratio_charging, sum_charging, transitions, avg_charging_duration, max_charging_duration\n",
        "\n",
        "    # í•˜ë£¨\n",
        "    charging_ratio, charging_sum, chargning_transitions, avg_charging_duration, max_charging_duration = _process_feature(status)\n",
        "\n",
        "    # ì ìëŠ” ì‹œê°„ëŒ€\n",
        "    sleep_status = status[df[\"hour\"].isin(SLEEP_HOURS)]\n",
        "    sleep_charging_ratio, sleep_charging_sum, sleep_charging_transitions, sleep_avg_charging_duration, sleep_max_charging_duration = _process_feature(sleep_status)\n",
        "\n",
        "    return pd.Series({\n",
        "        'charging_ratio': charging_ratio,\n",
        "        'charging_sum': charging_sum,\n",
        "        'charging_transitions': chargning_transitions,\n",
        "        'avg_charging_duration': avg_charging_duration,\n",
        "        'max_charging_duration': max_charging_duration,\n",
        "        'sleep_charging_ratio': sleep_charging_ratio,\n",
        "        'sleep_charging_sum': sleep_charging_sum,\n",
        "        'sleep_charging_transitions': sleep_charging_transitions,\n",
        "        'sleep_avg_charging_duration': sleep_avg_charging_duration,\n",
        "        'sleep_max_charging_duration': sleep_max_charging_duration,\n",
        "    })\n",
        "\n",
        "mACStatus_ori = load_data(DataType.mACStatus)\n",
        "mACStatus_ori = shift_lifelog_date(mACStatus_ori, target_hours=SLEEP_HOURS)\n",
        "\n",
        "mACStatus2  = (\n",
        "    mACStatus_ori\n",
        "    .groupby([\"subject_id\", \"lifelog_date\"], group_keys=False, as_index=False, sort=False, observed=True)\n",
        "    .apply(process_mACStatus)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "describe_df(mACStatus2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uvGhvN07Z4l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D09tdsYf7Z7R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHrp0dO_--pm"
      },
      "source": [
        "### âœ”ï¸ mActivity ì¶”ì •í–‰ë™\n",
        "- Value calculated by the Google Activity Recognition API.\n",
        " - 0 : IN_VEHICLE\n",
        " - 1 : ON_BICYCLE\n",
        " - 2 : ON_FOOT\n",
        " - 3 : STILL (not moving)\n",
        " - 4 : UNKNOWN\n",
        " - 5 : TILTING (This often occurs when a device is picked up from a desk or a user who is sitting stands up.)\n",
        " - 7 : WALKING\n",
        " - 8 : RUNNING\n",
        "- ê·¼ë¬´ì‹œê°„   : ì˜¤ì „ 7ì‹œë¶€í„° ì˜¤í›„ 6ì‹œê¹Œì§€\n",
        "- ê·¼ë¬´ì™¸ì‹œê°„ : ì˜¤í›„6ì‹œë¶€í„° 12ì‹œê¹Œì§€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Qy8JcbJfwEA"
      },
      "outputs": [],
      "source": [
        "def process_mActivity(df):\n",
        "    activity = df[\"m_activity\"].values.astype(\"int8\")\n",
        "\n",
        "    EXCLUDE_ACTIVITY = [3, 4]\n",
        "    WALKING_ACTIVITY = [1, 2, 7, 8]\n",
        "    VEHICLE_ACTIVITY = [0]\n",
        "\n",
        "    def _process_feature(activity):\n",
        "        if len(activity) == 0:\n",
        "            return 0., 0., 0.\n",
        "\n",
        "        # Walking minutes\n",
        "        walking_minutes = np.isin(activity, WALKING_ACTIVITY).sum()\n",
        "\n",
        "        # Vehicle minutes\n",
        "        vehicle_minutes = np.isin(activity, VEHICLE_ACTIVITY).sum()\n",
        "\n",
        "        # Activity minutes\n",
        "        activity_minutes = (1 - np.isin(activity, EXCLUDE_ACTIVITY)).sum()\n",
        "\n",
        "        return walking_minutes, vehicle_minutes, activity_minutes\n",
        "\n",
        "    # í•˜ë£¨\n",
        "    walking_minutes, vehicle_minutes, activity_minutes = _process_feature(activity)\n",
        "\n",
        "    # ì ìëŠ” ì‹œê°„ëŒ€\n",
        "    sleep_walking_minutes, sleep_vehicle_minutes, sleep_activity_minutes = _process_feature(activity[df[\"hour\"].isin(SLEEP_HOURS)])\n",
        "\n",
        "    return pd.Series({\n",
        "        'walking_minutes': walking_minutes,\n",
        "        'vehicle_minutes': vehicle_minutes,\n",
        "        'activity_minutes': activity_minutes,\n",
        "        'sleep_walking_minutes': sleep_walking_minutes,\n",
        "        'sleep_vehicle_minutes': sleep_vehicle_minutes,\n",
        "        'sleep_activity_minutes': sleep_activity_minutes,\n",
        "    })\n",
        "\n",
        "mActivity_ori = load_data(DataType.mActivity)\n",
        "mActivity_ori = shift_lifelog_date(mActivity_ori, target_hours=SLEEP_HOURS)\n",
        "\n",
        "mActivity21 = (\n",
        "    mActivity_ori\n",
        "    .groupby([\"subject_id\", \"lifelog_date\"], group_keys=False, as_index=False, sort=False, observed=True)\n",
        "    .apply(process_mActivity)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "describe_df(mActivity21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVoCLn4lb8bZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjtYW0clb8fQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN91Gb-kb4bV"
      },
      "source": [
        "### âœ”ï¸ mActivity ì¶”ì •í–‰ë™2 (NEW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U23OujeaZXRR"
      },
      "outputs": [],
      "source": [
        "mActivity = pd.read_parquet(f'{PATH}/ETRI_lifelog_dataset/ch2025_data_items/ch2025_mActivity.parquet')\n",
        "mActivity['lifelog_date'] = mActivity['timestamp'].astype(str).str[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOdmdtx9ZXW3"
      },
      "outputs": [],
      "source": [
        "# í™œë™ ë°ì´í„° ì›-í•« ì¸ì½”ë”©\n",
        "\"\"\"í™œë™ ì½”ë“œ(m_activity)ë¥¼ ì›-í•« ì¸ì½”ë”©í•˜ì—¬ ê° í™œë™ ìœ í˜•ë³„ ì»¬ëŸ¼ ìƒì„±\"\"\"\n",
        "\n",
        "mActivity = pd.merge(\n",
        "    mActivity,\n",
        "    pd.get_dummies(mActivity, columns=[\"m_activity\"], prefix=\"m_activity\", dtype=int),\n",
        "    how=\"left\",\n",
        "    on=[\"subject_id\", \"timestamp\",\"lifelog_date\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIowSdSDxq1M"
      },
      "outputs": [],
      "source": [
        "# ë°ì´í„° ì§‘ê³„ í•¨ìˆ˜ ì •ì˜\n",
        "def fn_love_aespa(\n",
        "    df_input: pd.DataFrame, # ì…ë ¥ ë°ì´í„°í”„ë ˆì„\n",
        "    str_value_col: str, # ì§‘ê³„í•  ì»¬ëŸ¼ëª…\n",
        "    str_agg_func: str = \"mean\", # ì§‘ê³„ í•¨ìˆ˜ (mean, median, mode, min, max, std, sum)\n",
        "    str_freq: str = \"30min\", # ì‹œê°„ ê°„ê²© (30min, 60min, 120min, 240min, 360min ë“±)\n",
        ") -> pd.DataFrame:\n",
        "    # ë°ì´í„°í”„ë ˆì„ ë³µì‚¬ ë° timestamp ì—´ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
        "    df_input_copy = df_input.copy()\n",
        "    df_input_copy[\"timestamp\"] = pd.to_datetime(df_input_copy[\"timestamp\"])\n",
        "\n",
        "    # ì§‘ê³„ ê²°ê³¼ ì»¬ëŸ¼ëª… ìƒì„±: @ì»¬ëŸ¼ëª…@ì‹œê°„ê°„ê²©@ì§‘ê³„í•¨ìˆ˜\n",
        "    str_agg_col_name = f\"@{str_value_col}@{str_freq}@{str_agg_func}\"\n",
        "\n",
        "    # ì§‘ê³„ í•¨ìˆ˜ ì„¤ì • (modeëŠ” ë³„ë„ ì²˜ë¦¬ í•„ìš”)\n",
        "    dict_aggregation = {}\n",
        "    if str_agg_func == \"mode\":\n",
        "        mode_agg_func = lambda x: (x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
        "        dict_aggregation[str_agg_col_name] = (str_value_col, mode_agg_func)\n",
        "    else:\n",
        "        dict_aggregation[str_agg_col_name] = (str_value_col, str_agg_func)\n",
        "\n",
        "    # ê·¸ë£¹ë³„ ë°ì´í„° ì§‘ê³„ ìˆ˜í–‰\n",
        "    df_agg = (\n",
        "        df_input_copy.groupby([\"subject_id\", pd.Grouper(key=\"timestamp\", freq=str_freq)]).agg(**dict_aggregation).reset_index()\n",
        "    )\n",
        "\n",
        "    # ë‚ ì§œ ë° ì‹œê°„ ì •ë³´ ì¶”ì¶œ\n",
        "    df_agg[\"lifelog_date\"] = df_agg[\"timestamp\"].dt.date.astype(str)\n",
        "    df_agg[\"hh24mi\"] = df_agg[\"timestamp\"].dt.strftime(\"%Hh%Mm\")\n",
        "\n",
        "    # í”¼ë²— í…Œì´ë¸”ë¡œ ë°ì´í„° ì¬êµ¬ì„± (subject_id, lifelog_date ê¸°ì¤€ìœ¼ë¡œ ì‹œê°„ëŒ€ë³„ ê°’ ë°°ì¹˜)\n",
        "    df_pivot = df_agg.pivot_table(\n",
        "        index=[\"subject_id\", \"lifelog_date\"],\n",
        "        columns=\"hh24mi\",\n",
        "        values=str_agg_col_name,\n",
        "    )\n",
        "\n",
        "    # ì»¬ëŸ¼ ì´ë¦„ ì¬êµ¬ì„± ë° ì¸ë±ìŠ¤ ì´ˆê¸°í™”\n",
        "    list_hh23mi_col = list(df_pivot.columns)\n",
        "    df_pivot = df_pivot.reindex(columns=list_hh23mi_col).reset_index()\n",
        "    list_hour_col = {hh24mi: f\"{str_value_col}@{str_freq}@{str_agg_func}@{hh24mi}\" for hh24mi in list_hh23mi_col}\n",
        "    df_pivot = df_pivot.rename(columns=list_hour_col)\n",
        "\n",
        "    return df_pivot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGkwDwF0Xxmf"
      },
      "outputs": [],
      "source": [
        "# MET ê°’ ë§¤í•‘\n",
        "\"\"\"\n",
        "ê° í™œë™ ì½”ë“œì— í•´ë‹¹í•˜ëŠ” MET(Metabolic Equivalent of Task) ê°’ í• ë‹¹\n",
        "METëŠ” ì‹ ì²´ í™œë™ì˜ ì—ë„ˆì§€ ì†Œë¹„ëŸ‰ì„ ì¸¡ì •í•˜ëŠ” ë‹¨ìœ„\n",
        "\n",
        "í™œë™ ì½”ë“œë³„ MET ê°’:\n",
        "    0: 1.3 MET (ê°€ë²¼ìš´ ì¢Œì‹ í™œë™)\n",
        "    1: 8.0 MET (ê²©ë ¬í•œ í™œë™)\n",
        "    3: 1.2 MET (ë§¤ìš° ê°€ë²¼ìš´ í™œë™)\n",
        "    4: 3.0 MET (ì¤‘ê°„ ê°•ë„ í™œë™)\n",
        "    7: 3.5 MET (ì¤‘ê°„ ê°•ë„ í™œë™)\n",
        "    8: 10.0 MET (ë§¤ìš° ê²©ë ¬í•œ í™œë™)\n",
        "\"\"\"\n",
        "\n",
        "dict_met_value = {0: 1.3, 1: 8.0, 3: 1.2, 4: 3.0, 7: 3.5, 8: 10.0}\n",
        "for activity, met in dict_met_value.items():\n",
        "    mActivity.loc[mActivity[\"m_activity\"].isin([activity]), \"m_activity_met\"] = met\n",
        "\n",
        "mActivity.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpQgcJ5WXXF_"
      },
      "outputs": [],
      "source": [
        "# í™œë™ ë°ì´í„° ì§‘ê³„\n",
        "df_agg_activity_std = fn_love_aespa(df_input=mActivity,\n",
        "                                    str_value_col=\"m_activity\",\n",
        "                                    # \"mean\", \"median\", \"mode\", \"min\", \"max\", \"std\"\n",
        "                                    str_agg_func=\"std\",\n",
        "                                    # \"30min\", \"60min\", \"120min\", \"240min\", \"360min\", \"480min\", \"720min\", \"1440min\"\n",
        "                                    str_freq=\"240min\",\n",
        "                                    )\n",
        "\n",
        "df_agg_activity_met_std = fn_love_aespa(df_input=mActivity,\n",
        "                                    str_value_col=\"m_activity_met\",\n",
        "                                    # \"mean\", \"median\", \"mode\", \"sum\", \"min\", \"max\", \"std\"\n",
        "                                    str_agg_func=\"std\",\n",
        "                                    # \"30min\", \"60min\", \"120min\", \"240min\", \"360min\", \"480min\", \"720min\", \"1440min\"\n",
        "                                    str_freq=\"240min\",\n",
        "                                    )\n",
        "\n",
        "df_agg_activity_met_sum = fn_love_aespa(df_input=mActivity,\n",
        "                                    str_value_col=\"m_activity_met\",\n",
        "                                    # \"mean\", \"median\", \"mode\", \"sum\", \"min\", \"max\", \"std\"\n",
        "                                    str_agg_func=\"sum\",\n",
        "                                    # \"30min\", \"60min\", \"120min\", \"240min\", \"360min\", \"480min\", \"720min\", \"1440min\"\n",
        "                                    str_freq=\"240min\",\n",
        "                                    )\n",
        "\n",
        "df_agg_activity_0_std = fn_love_aespa(df_input=mActivity,\n",
        "                                    str_value_col=\"m_activity_0\",\n",
        "                                    # \"mean\", \"median\", \"mode\", \"sum\", \"min\", \"max\", \"std\"\n",
        "                                    str_agg_func=\"std\",\n",
        "                                    # \"30min\", \"60min\", \"120min\", \"240min\", \"360min\", \"480min\", \"720min\", \"1440min\"\n",
        "                                    str_freq=\"240min\",\n",
        "                                    )\n",
        "\n",
        "df_agg_activity_0_sum = fn_love_aespa(df_input=mActivity,\n",
        "                                    str_value_col=\"m_activity_0\",\n",
        "                                    # \"mean\", \"median\", \"mode\", \"sum\", \"min\", \"max\", \"std\"\n",
        "                                    str_agg_func=\"sum\",\n",
        "                                    # \"30min\", \"60min\", \"120min\", \"240min\", \"360min\", \"480min\", \"720min\", \"1440min\"\n",
        "                                    str_freq=\"240min\",\n",
        "                                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KR_SFDDUZsZc"
      },
      "outputs": [],
      "source": [
        "# ë³‘í•© ê¸°ì¤€ key\n",
        "merge_keys = ['subject_id', 'lifelog_date']\n",
        "\n",
        "# ì„¸ ê°œ ë°ì´í„°í”„ë ˆì„ ìˆœì°¨ ë³‘í•©\n",
        "mActivity22 = (\n",
        "    df_agg_activity_std\n",
        "    .merge(df_agg_activity_met_std, on=merge_keys, how='outer')\n",
        "    .merge(df_agg_activity_met_sum, on=merge_keys, how='outer')\n",
        "    .merge(df_agg_activity_0_std, on=merge_keys, how='outer')\n",
        "    .merge(df_agg_activity_0_sum, on=merge_keys, how='outer')\n",
        ")\n",
        "\n",
        "# check\n",
        "print(mActivity22.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5FJK5PXXXOr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMLE3NPHW5uV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEu5F-6-hmgI"
      },
      "source": [
        "### âœ”ï¸ mAmbience ì£¼ë³€ì†Œë¦¬ (ìˆ˜ì •)\n",
        "- Ambient sound identification labels and their respective probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snxx7CH6gtif"
      },
      "outputs": [],
      "source": [
        "def process_mAmbience(df):\n",
        "    ambience = df[\"m_ambience\"].values  # [[label, prob], ...], [[label, prob], ...]\n",
        "\n",
        "    def _process_feature(ambience):\n",
        "        labels = set()\n",
        "\n",
        "        for amb in ambience:\n",
        "            labels_, _ = zip(*amb)\n",
        "            labels.update(labels_)\n",
        "\n",
        "        unique_label_count = len(labels)\n",
        "        snor_count = len(list(filter(lambda x: \"snor\" in x.lower(), labels)))\n",
        "\n",
        "        return unique_label_count, snor_count\n",
        "\n",
        "    # í™œë™ì‹œê°„\n",
        "    active_hour_unique_label_count, active_hour_snor_count = _process_feature(ambience[df[\"hour\"].isin(ACTIVE_HOURS)])\n",
        "\n",
        "    # ì ìëŠ”ì‹œê°„\n",
        "    sleep_hour_unique_label_count, sleep_hour_snor_count = _process_feature(ambience[df[\"hour\"].isin(SLEEP_HOURS)])\n",
        "\n",
        "    return pd.Series({\n",
        "        'active_hour_unique_label_count': active_hour_unique_label_count,\n",
        "        'active_hour_snor_count': active_hour_snor_count,\n",
        "        'sleep_hour_unique_label_count': sleep_hour_unique_label_count,\n",
        "        'sleep_hour_snor_count': sleep_hour_snor_count,\n",
        "    })\n",
        "\n",
        "mAmbience_ori = load_data(DataType.mAmbience)\n",
        "mAmbience_ori = shift_lifelog_date(mAmbience_ori, target_hours=SLEEP_HOURS)\n",
        "\n",
        "mAmbience2 = (\n",
        "    mAmbience_ori\n",
        "    .groupby([\"subject_id\", \"lifelog_date\"], group_keys=False, as_index=False, sort=False, observed=True)\n",
        "    .apply(process_mAmbience)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "describe_df(mAmbience2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCxHeB836ekw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QnUfNL-ceIA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfbNZ1WsceMN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yh2wt8LycePt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyS90xE7WAJV"
      },
      "source": [
        "### âœ”ï¸ mBle ë¸”ë£¨íˆ¬ìŠ¤ (ìˆ˜ì •)\n",
        "- Bluetooth devices around individual subject.\n",
        " - 7936 : Wearable, Headset, AV Device\n",
        " - 1796 : Peripheral (ì…ë ¥ì¥ì¹˜) ê³„ì—´\n",
        " - 0 : ì •ë³´ ì—†ìŒ ë˜ëŠ” ì•Œ ìˆ˜ ì—†ìŒ(Unknown)\n",
        " - 1084 : Audio/Video (ìŠ¤í”¼ì»¤, í—¤ë“œì…‹, ì´ì–´í°, TV ë“±)\n",
        " - 524 : Phone (íœ´ëŒ€í°, ìŠ¤ë§ˆíŠ¸í°)\n",
        " - 1060 : Headphones\n",
        " - 284 : commputer (PC, ë…¸íŠ¸ë¶, PDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKFv8buPkLEG"
      },
      "outputs": [],
      "source": [
        "def process_mBle(df):\n",
        "    ble = df[\"m_ble\"].values  # [[{\"address\": \"xx:xx:xx:xx:xx:xx\", \"device_class\": \"0\", \"rssi\": -70}, ...], [...], ...]\n",
        "\n",
        "    def _process_feature(ble):\n",
        "        if len(ble) == 0:\n",
        "            return 0., 0., 0., 0., 0.\n",
        "\n",
        "        rssi = []\n",
        "        devices = []\n",
        "        for ble_data in ble:\n",
        "            for device in ble_data:\n",
        "                rssi.append(device[\"rssi\"])\n",
        "                devices.append(device[\"device_class\"])\n",
        "\n",
        "        rssi = np.array(rssi)\n",
        "        rssi_mean = rssi.mean() if len(rssi) > 0 else 0\n",
        "        rssi_min = rssi.min() if len(rssi) > 0 else 0\n",
        "        rssi_max = rssi.max() if len(rssi) > 0 else 0\n",
        "\n",
        "        unknown_count = devices.count(\"0\")\n",
        "        others_count = len(devices) - unknown_count\n",
        "        others_ratio = others_count / len(devices) if len(devices) > 0 else 0\n",
        "        unknown_ratio = unknown_count / len(devices) if len(devices) > 0 else 0\n",
        "\n",
        "        return rssi_mean, rssi_min, rssi_max, others_ratio, unknown_ratio\n",
        "\n",
        "    # ì¼í• ë•Œ\n",
        "    work_hour_rssi_mean, work_hour_rssi_min, work_hour_rssi_max, work_hour_others_ratio, work_hour_unknown_ratio = _process_feature(ble[df[\"hour\"].isin(WORK_HOURS)])\n",
        "\n",
        "    # í‡´ê·¼í›„\n",
        "    free_hour_rssi_mean, free_hour_rssi_min, free_hour_rssi_max, free_hour_others_ratio, free_hour_unknown_ratio = _process_feature(ble[df[\"hour\"].isin(FREE_HOURS)])\n",
        "\n",
        "    # ì ìëŠ”ì‹œê°„\n",
        "    sleep_hour_rssi_mean, sleep_hour_rssi_min, sleep_hour_rssi_max, sleep_hour_others_ratio, sleep_hour_unknown_ratio = _process_feature(ble[df[\"hour\"].isin(SLEEP_HOURS)])\n",
        "\n",
        "    return pd.Series({\n",
        "        'work_hour_rssi_mean': work_hour_rssi_mean,\n",
        "        'work_hour_rssi_min': work_hour_rssi_min,\n",
        "        'work_hour_rssi_max': work_hour_rssi_max,\n",
        "        'work_hour_others_ratio': work_hour_others_ratio,\n",
        "        'work_hour_unknown_ratio': work_hour_unknown_ratio,\n",
        "        'free_hour_rssi_mean': free_hour_rssi_mean,\n",
        "        'free_hour_rssi_min': free_hour_rssi_min,\n",
        "        'free_hour_rssi_max': free_hour_rssi_max,\n",
        "        'free_hour_others_ratio': free_hour_others_ratio,\n",
        "        'free_hour_unknown_ratio': free_hour_unknown_ratio,\n",
        "        'sleep_hour_rssi_mean': sleep_hour_rssi_mean,\n",
        "        'sleep_hour_rssi_min': sleep_hour_rssi_min,\n",
        "        'sleep_hour_rssi_max': sleep_hour_rssi_max,\n",
        "        'sleep_hour_others_ratio': sleep_hour_others_ratio,\n",
        "        'sleep_hour_unknown_ratio': sleep_hour_unknown_ratio\n",
        "    })\n",
        "\n",
        "mBle_ori = load_data(DataType.mBle)\n",
        "mBle_ori = shift_lifelog_date(mBle_ori, target_hours=SLEEP_HOURS)\n",
        "\n",
        "mBle2 = (\n",
        "    mBle_ori\n",
        "    .groupby([\"subject_id\", \"lifelog_date\"], group_keys=False, as_index=False, sort=False, observed=True)\n",
        "    .apply(process_mBle)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "describe_df(mBle2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ak08iDZ1kLHh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WQEISbYkLRW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiCtsengLveO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5vsYKHoH8lz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOiajFjeRFi-"
      },
      "source": [
        "### âœ”ï¸ mGps, GPS ê¸°ë°˜ í•¸ë“œí° ìœ„ì¹˜\n",
        "- Multiple GPS coordinates measured within a single minute using the smartphone.\n",
        "- speedê°€ 1ë³´ë‹¤ í°ê²½ìš° ì •ì§€ ìƒíƒœê°€ ì•„ë‹ˆê³  ì›€ì§ì´ê³  ìˆë‹¤ê³  íŒë‹¨\n",
        " - 0.5-2 : ê±¸ì–´ì„œ ì´ë™í•˜ëŠ” ê²½ìš°  \n",
        " - 2-5 : ì¡°ê¹…\n",
        " - 5 ì´ìƒ : ì°¨ë¥¼ íƒ€ê³  ì´ë™í•˜ëŠ” ê²½ìš°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hV_VT5fZH8rj"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def haversine_np(lon1, lat1, lon2, lat2, radius=6371):\n",
        "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
        "\n",
        "    dlon = lon2 - lon1\n",
        "    dlat = lat2 - lat1\n",
        "\n",
        "    a = np.sin(dlat / 2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0)**2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "\n",
        "    return radius * c\n",
        "\n",
        "def process_mGps(df):\n",
        "    gps = df[\"m_gps\"].values  # [[{'altitude': 110.6, 'latitude': 0.2077385, 'longitude': 0.170027, 'speed': 0.0}, ...], ...]\n",
        "    timestamps = df[\"timestamp\"].values\n",
        "\n",
        "    def _process_feature(gps, timestamps):\n",
        "        if len(gps) == 0:\n",
        "            return 0., 0., 0., 0., 0., 0., 0., np.array([])\n",
        "\n",
        "        # n-ë¶„ ë‹¨ìœ„\n",
        "        latitudes = []\n",
        "        longitudes = []\n",
        "        altitudes = []\n",
        "        speeds = []\n",
        "        minutes = []  # ëˆ„ì  ë¶„\n",
        "\n",
        "        for i, (gps_data, timestamp) in enumerate(zip(gps, timestamps)):\n",
        "            _latitudes = []\n",
        "            _longitudes = []\n",
        "            _altitudes = []\n",
        "            _speeds = []\n",
        "            for data in gps_data:\n",
        "                _latitudes.append(data[\"latitude\"])\n",
        "                _longitudes.append(data[\"longitude\"])\n",
        "                _altitudes.append(data[\"altitude\"])\n",
        "                _speeds.append(data[\"speed\"])\n",
        "\n",
        "            latitudes.append(np.mean(_latitudes))\n",
        "            longitudes.append(np.mean(_longitudes))\n",
        "            altitudes.append(np.mean(_altitudes))\n",
        "            speeds.append(np.mean(_speeds))\n",
        "            minutes.append(1 if i == 0 else pd.Timedelta(timestamps[i] - timestamps[i-1]).total_seconds() / 60)\n",
        "\n",
        "        latitudes = np.array(latitudes)\n",
        "        longitudes = np.array(longitudes)\n",
        "        altitudes = np.array(altitudes)\n",
        "        speeds = np.array(speeds)\n",
        "        minutes = np.array(minutes)\n",
        "\n",
        "        walk_minutes = minutes[(speeds >= 0.5) & (speeds < 2.0)].sum()\n",
        "        jog_minutes = minutes[(2.0 <= speeds) & (speeds < 5.0)].sum()\n",
        "        vehicle_minutes = minutes[(5.0 <= speeds)].sum()\n",
        "\n",
        "        # ì†ë„\n",
        "        mean_speed = speeds.mean() if len(speeds) > 0 else 0\n",
        "        max_speed = speeds.max() if len(speeds) > 0 else 0\n",
        "        min_speed = speeds.min() if len(speeds) > 0 else 0\n",
        "\n",
        "        # ì´ë™ê±°ë¦¬\n",
        "        distance = haversine_np(longitudes[:-1], latitudes[:-1], longitudes[1:], latitudes[1:]).sum()\n",
        "\n",
        "        return walk_minutes, jog_minutes, vehicle_minutes, mean_speed, max_speed, min_speed, distance, speeds\n",
        "\n",
        "    # í•˜ë£¨\n",
        "    active_hour_walk_minutes, active_hour_jog_minutes, active_hour_vehicle_minutes, active_hour_mean_speed, active_hour_max_speed, active_hour_min_speed, active_hour_distance, _ = _process_feature(gps[df[\"hour\"].isin(ACTIVE_HOURS)], timestamps[df[\"hour\"].isin(ACTIVE_HOURS)])\n",
        "\n",
        "    # ì ìëŠ” ì‹œê°„ëŒ€\n",
        "    sleep_hour_walk_minutes, sleep_hour_jog_minutes, sleep_hour_vehicle_minutes, sleep_hour_mean_speed, sleep_hour_max_speed, sleep_hour_min_speed, sleep_hour_distance, _ = _process_feature(gps[df[\"hour\"].isin(SLEEP_HOURS)], timestamps[df[\"hour\"].isin(SLEEP_HOURS)])\n",
        "\n",
        "    # ì¼ì–´ë‚  ë•Œ\n",
        "    _, _, _, _, _, _, _, might_wakeup_speeds = _process_feature(gps[df[\"hour\"].isin(MIGHT_WAKEUP_HOURS)], timestamps[df[\"hour\"].isin(MIGHT_WAKEUP_HOURS)])\n",
        "    might_wakeup_timestamps = timestamps[df[\"hour\"].isin(MIGHT_WAKEUP_HOURS)]\n",
        "    wakeup_timestamps = might_wakeup_timestamps[(might_wakeup_speeds > 1.0)]\n",
        "    first_move_datetime = (\n",
        "        pd.to_datetime(wakeup_timestamps[0]) if len(wakeup_timestamps) > 0\n",
        "        else pd.to_datetime(might_wakeup_timestamps[-1]) if len(might_wakeup_timestamps) > 0\n",
        "        else pd.to_datetime(datetime(2024, 1, 1, MIGHT_WAKEUP_HOURS[-1], 0, 0))  # default to the last hour of the range\n",
        "    )\n",
        "    first_wakeup_minutes = (first_move_datetime.hour if first_move_datetime.hour > 12 else first_move_datetime.hour + 24) * 60 + first_move_datetime.minute\n",
        "\n",
        "    return pd.Series({\n",
        "        'active_hour_walk_minutes': active_hour_walk_minutes,\n",
        "        'active_hour_jog_minutes': active_hour_jog_minutes,\n",
        "        'active_hour_vehicle_minutes': active_hour_vehicle_minutes,\n",
        "        'active_hour_mean_speed': active_hour_mean_speed,\n",
        "        'active_hour_max_speed': active_hour_max_speed,\n",
        "        'active_hour_min_speed': active_hour_min_speed,\n",
        "        'active_hour_distance': active_hour_distance,\n",
        "        'exercise_flag': 1 if active_hour_jog_minutes > 10 else 0,  # në¶„ ì´ìƒ ì¡°ê¹…í•œ ê²½ìš°\n",
        "        'sleep_hour_walk_minutes': sleep_hour_walk_minutes,\n",
        "        'sleep_hour_jog_minutes': sleep_hour_jog_minutes,\n",
        "        'sleep_hour_vehicle_minutes': sleep_hour_vehicle_minutes,\n",
        "        'sleep_hour_mean_speed': sleep_hour_mean_speed,\n",
        "        'sleep_hour_max_speed': sleep_hour_max_speed,\n",
        "        'sleep_hour_min_speed': sleep_hour_min_speed,\n",
        "        'sleep_hour_distance': sleep_hour_distance,\n",
        "        \"mgps_first_wakeup_minutes\": first_wakeup_minutes,\n",
        "    })\n",
        "\n",
        "\n",
        "mGps_ori = load_data(DataType.mGps)\n",
        "mGps_ori = shift_lifelog_date(mGps_ori, target_hours=SLEEP_HOURS)\n",
        "\n",
        "mGps2 = (\n",
        "    mGps_ori\n",
        "    .groupby([\"subject_id\", \"lifelog_date\"], group_keys=False, as_index=False, sort=False, observed=True)\n",
        "    .apply(process_mGps)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "describe_df(mGps2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riN0msO6wodn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTvpgUsLwvNU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oHiJLPfaoSw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0k3YkQqaodZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSRRKaDlaorj"
      },
      "source": [
        "### âœ”ï¸ mLight ì£¼ë³€ ë°ê¸°\n",
        "- Ambient light measured by the smartphone.\n",
        " - ì–´ë‘ìš´ ë°¤\t0.1 ~ 1 lux\tìº„ìº„í•œ ë°©, ë‹¬ë¹› ì—†ëŠ” ë°¤\n",
        " - ê°€ë¡œë“± ì¼œì§„ ê±°ë¦¬\t10 ~ 20 lux\tíë¦¿í•œ ì™¸ë¶€ ì¡°ëª…\n",
        " - ì‹¤ë‚´ ì¡°ëª…\t100 ~ 500 lux\tì‚¬ë¬´ì‹¤, ì¼ë°˜ ê±°ì‹¤\n",
        " - ë°ì€ ì‹¤ì™¸\t10,000 ~ 25,000 lux\të§‘ì€ ë‚  í–‡ë¹›\n",
        " - ì§ì‚¬ê´‘ì„  ì•„ë˜\t30,000 ~ 100,000 lux\tì—¬ë¦„ í•œë‚®, ë§¤ìš° ê°•í•œ í–‡ë¹›"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0PzJaYUWfVZ"
      },
      "outputs": [],
      "source": [
        "mLight['lifelog_date'] = mLight['timestamp'].astype(str).str[:10]\n",
        "# mLight = fill_missing_dates_by_subject(mLight)\n",
        "mLight.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIGfYpnFufpT"
      },
      "outputs": [],
      "source": [
        "def process_mLight(df):\n",
        "    df = df.copy()\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['lifelog_date'] = df['timestamp'].dt.date\n",
        "    df['hour'] = df['timestamp'].dt.hour\n",
        "    df['is_night'] = df['hour'].apply(lambda h: h >= 22 or h < 6)\n",
        "\n",
        "    # í•˜ë£¨ ìš”ì•½ í†µê³„\n",
        "    daily_light = df.groupby(['subject_id', 'lifelog_date']).agg(\n",
        "        light_mean=('m_light', 'mean'),\n",
        "        light_std=('m_light', 'std'),\n",
        "        light_max=('m_light', 'max'),\n",
        "        light_min=('m_light', 'min'),\n",
        "        light_night_mean=('m_light', lambda x: x[df.loc[x.index, 'is_night']].mean()),\n",
        "        light_day_mean=('m_light', lambda x: x[~df.loc[x.index, 'is_night']].mean()),\n",
        "        light_night_ratio=('is_night', 'mean')\n",
        "    ).reset_index()\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for subject_id, group in tqdm(df.groupby('subject_id'), desc=\"Processing light-based sleep detection\"):\n",
        "        group = group.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "        recorded_dates = set()\n",
        "        sleeping = False\n",
        "        zero_count = 0\n",
        "        first_zero_time = None\n",
        "\n",
        "        for i in range(len(group)):\n",
        "            light = group.loc[i, 'm_light']\n",
        "            hour = group.loc[i, 'hour']\n",
        "\n",
        "            if light == 0:\n",
        "                zero_count += 1\n",
        "                if zero_count == 1:\n",
        "                    first_zero_time = group.loc[i, 'timestamp']\n",
        "                if zero_count >= 6 and not sleeping:\n",
        "                    sleep_hour = first_zero_time.hour\n",
        "                    if (sleep_hour >= 21 or sleep_hour <= 2):\n",
        "                        sleeping = True\n",
        "            else:\n",
        "                if sleeping:\n",
        "                    candidate_wakeup = group.loc[i, 'timestamp']\n",
        "                    wake_hour = candidate_wakeup.hour\n",
        "\n",
        "                    if 5 <= wake_hour <= 9 and first_zero_time is not None:\n",
        "                        wake_time = candidate_wakeup\n",
        "                        sleep_time = first_zero_time\n",
        "                        duration_min = (wake_time - sleep_time).total_seconds() / 60\n",
        "\n",
        "                        if 0 < duration_min <= 840:\n",
        "                            sleep_duration = duration_min\n",
        "                        else:\n",
        "                            sleep_duration = np.nan\n",
        "\n",
        "                        lifelog_date = wake_time.date() + pd.Timedelta(days=-1)\n",
        "\n",
        "                        if lifelog_date not in recorded_dates:\n",
        "                            results.append({\n",
        "                                'subject_id': subject_id,\n",
        "                                'lifelog_date': lifelog_date,\n",
        "                                'sleep_duration_min_mLight': sleep_duration,\n",
        "                                'sleep_time_min_mLight': sleep_time.hour * 60 + sleep_time.minute,\n",
        "                                'wake_time_min_mLight': wake_time.hour * 60 + wake_time.minute,\n",
        "                                'hour_slept_mLight': sleep_time.hour + sleep_time.minute / 60,\n",
        "                                'hour_woke_up_mLight': wake_time.hour + wake_time.minute / 60\n",
        "                            })\n",
        "                            recorded_dates.add(lifelog_date)\n",
        "\n",
        "                        sleeping = False\n",
        "                        zero_count = 0\n",
        "                        first_zero_time = None\n",
        "\n",
        "            if light > 0:\n",
        "                zero_count = 0\n",
        "                first_zero_time = None\n",
        "\n",
        "    sleep_df = pd.DataFrame(results)\n",
        "\n",
        "    # ì •ë ¬ + ë³´ê°„\n",
        "    sleep_df = sleep_df.sort_values(['subject_id', 'lifelog_date'])\n",
        "    sleep_df['sleep_duration_interp_mLight'] = sleep_df.groupby('subject_id')['sleep_duration_min_mLight'].transform(lambda x: x.interpolate())\n",
        "\n",
        "    # ì‹œê°„ ë‹¨ìœ„ íŒŒìƒ ì»¬ëŸ¼\n",
        "    sleep_df['sleep_duration_hour_mLight'] = sleep_df['sleep_duration_min_mLight'] / 60\n",
        "    sleep_df['sleep_duration_interp_hour_mLight'] = sleep_df['sleep_duration_interp_mLight'] / 60\n",
        "\n",
        "    # ë³‘í•©\n",
        "    final = pd.merge(daily_light, sleep_df, on=['subject_id', 'lifelog_date'], how='left')\n",
        "\n",
        "    return final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gYPb8vQP5Ls"
      },
      "outputs": [],
      "source": [
        "def process_mLight2(df):\n",
        "    from datetime import datetime, timedelta\n",
        "\n",
        "    df = df.copy()\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['lifelog_date'] = pd.to_datetime(df['lifelog_date'])\n",
        "\n",
        "    # m_light > 0 â†’ m_screen_useë¡œ ë³€í™˜\n",
        "    df['m_light_on'] = (df['m_light'] > 0).astype(int)\n",
        "\n",
        "    # base key í™•ë³´\n",
        "    base_keys = df[['subject_id', 'lifelog_date']].drop_duplicates()\n",
        "    base_keys['lifelog_date'] = base_keys['lifelog_date'].dt.date\n",
        "\n",
        "    # ë°¤ 9ì‹œ ~ ë‹¤ìŒë‚  ì˜¤ì „ 11ì‹œ í•„í„°ë§\n",
        "    df['hour'] = df['timestamp'].dt.hour\n",
        "    df = df[(df['hour'] >= 21) | (df['hour'] < 11)].copy()\n",
        "    df.loc[df['hour'] < 11, 'lifelog_date'] -= pd.Timedelta(days=1)\n",
        "\n",
        "    df.sort_values(['subject_id', 'timestamp'], inplace=True)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for (subject_id, lifelog_date), group in df.groupby(['subject_id', 'lifelog_date']):\n",
        "        group = group.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "        # 1. ì¤‘ê°„ ê°ì„± ì œê±°\n",
        "        prev = group['m_light_on'].shift(1)\n",
        "        next_ = group['m_light_on'].shift(-1)\n",
        "        mask = (group['m_light_on'] == 1) & (prev == 0) & (next_ == 0)\n",
        "        group.loc[mask, 'm_light_on'] = 0\n",
        "\n",
        "        # 2. ì§§ì€ ê°ì„± ë¸”ë¡ ì œê±°\n",
        "        group['is_sleep'] = group['m_light_on'] == 0\n",
        "        group['block'] = (group['is_sleep'] != group['is_sleep'].shift()).cumsum()\n",
        "        block_info = group.groupby('block').agg(\n",
        "            is_sleep=('is_sleep', 'first'),\n",
        "            size=('is_sleep', 'size')\n",
        "        )\n",
        "\n",
        "        for i in range(1, len(block_info) - 1):\n",
        "            if (\n",
        "                block_info.iloc[i]['is_sleep'] == False and\n",
        "                block_info.iloc[i]['size'] <= 2 and\n",
        "                block_info.iloc[i - 1]['is_sleep'] and\n",
        "                block_info.iloc[i + 1]['is_sleep']\n",
        "            ):\n",
        "                group.loc[group['block'] == block_info.index[i], 'm_light_on'] = 0\n",
        "\n",
        "        # 3. ìˆ˜ë©´ ë¸”ë¡ ì¶”ì •\n",
        "        group['is_sleep'] = group['m_light_on'] == 0\n",
        "        group['block'] = (group['is_sleep'] != group['is_sleep'].shift()).cumsum()\n",
        "        sleep_blocks = group[group['is_sleep']].groupby('block').agg(\n",
        "            sleep_start=('timestamp', 'first'),\n",
        "            sleep_end=('timestamp', 'last'),\n",
        "            duration_min=('timestamp', lambda x: (x.max() - x.min()).total_seconds() / 60)\n",
        "        )\n",
        "\n",
        "        sleep_time = wake_time = duration_min = None\n",
        "        if not sleep_blocks.empty:\n",
        "            longest_sleep = sleep_blocks.loc[sleep_blocks['duration_min'].idxmax()]\n",
        "            sleep_time = longest_sleep['sleep_start'].time()\n",
        "            wake_time = longest_sleep['sleep_end'].time()\n",
        "            duration_min = longest_sleep['duration_min']\n",
        "\n",
        "            # ìœ íš¨ì„± ì¡°ê±´\n",
        "            if not (4 <= wake_time.hour < 11):\n",
        "                wake_time = None\n",
        "            if not (sleep_time.hour >= 21 or sleep_time.hour < 3):\n",
        "                sleep_time = None\n",
        "            if duration_min < 100:\n",
        "                sleep_time = None\n",
        "                wake_time = None\n",
        "                duration_min = None\n",
        "\n",
        "        results.append({\n",
        "            'subject_id': subject_id,\n",
        "            'lifelog_date': lifelog_date.date(),\n",
        "            'sleep_time': sleep_time,\n",
        "            'wake_time': wake_time,\n",
        "            'sleep_duration_min': round(duration_min, 1) if duration_min is not None else None\n",
        "        })\n",
        "\n",
        "    sleep_df = pd.DataFrame(results)\n",
        "    result_df = base_keys.merge(sleep_df, on=['subject_id', 'lifelog_date'], how='left')\n",
        "\n",
        "    # ì‹œê°„ â†’ ì‹¤ìˆ˜í˜• ìˆ«ì ë³€í™˜\n",
        "    def time_to_float(t):\n",
        "        if pd.isna(t):\n",
        "            return None\n",
        "        return round(t.hour + t.minute / 60 + t.second / 3600, 4)\n",
        "\n",
        "    result_df['sleep_time'] = result_df['sleep_time'].apply(time_to_float)\n",
        "    result_df['wake_time'] = result_df['wake_time'].apply(time_to_float)\n",
        "\n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxL0TrRUTrhz"
      },
      "outputs": [],
      "source": [
        "def add_ratios(df):\n",
        "    df = df.copy()\n",
        "    df['lifelog_date'] = pd.to_datetime(df['lifelog_date'])\n",
        "    df['weekday'] = df['lifelog_date'].dt.weekday\n",
        "    df['week_type'] = df['weekday'].apply(lambda x: 'weekend' if x >= 5 else 'weekday')\n",
        "    df['month'] = df['lifelog_date'].dt.month\n",
        "\n",
        "    # í‰ê·  ê³„ì‚°\n",
        "    avg_duration = df.groupby(['subject_id', 'month', 'week_type'])['sleep_duration_min'].mean().reset_index(name='avg_sleep_duration')\n",
        "    sleep_time_avg = df.groupby(['subject_id', 'month', 'week_type'])['sleep_time'].apply(calculate_circular_mean_sleep_time).reset_index(name='avg_sleep_time')\n",
        "    wake_time_avg = df.groupby(['subject_id', 'month', 'week_type'])['wake_time'].apply(calculate_circular_mean_sleep_time).reset_index(name='avg_wake_time')\n",
        "    avg_df = sleep_time_avg.merge(wake_time_avg, on=['subject_id', 'month', 'week_type']).merge(avg_duration, on=['subject_id', 'month', 'week_type'])\n",
        "    df = df.merge(avg_df, on=['subject_id', 'month', 'week_type'], how='left')\n",
        "\n",
        "    # ë¹„ìœ¨ ë° ì°¨ì´\n",
        "    df['sleep_time_diff'] = df['avg_sleep_time'] - df['sleep_time']\n",
        "    df['wake_time_diff'] = df['avg_wake_time'] - df['wake_time']\n",
        "    df['sleep_duration_diff'] = df['avg_sleep_duration'] - df['sleep_duration_min']\n",
        "    df['sleep_time_ratio'] = df['sleep_time'] / df['avg_sleep_time']\n",
        "    df['wake_time_ratio'] = df['wake_time'] / df['avg_wake_time']\n",
        "    df['sleep_duration_ratio'] = df['sleep_duration_min'] / df['avg_sleep_duration']\n",
        "\n",
        "    # ì •ë ¬ í›„ lag/ë³€í™”ëŸ‰\n",
        "    df = df.sort_values(['subject_id', 'lifelog_date'])\n",
        "    for lag in [1, 2]:\n",
        "        df[f'sleep_time_lag{lag}'] = df.groupby('subject_id')['sleep_time'].shift(lag)\n",
        "        df[f'wake_time_lag{lag}'] = df.groupby('subject_id')['wake_time'].shift(lag)\n",
        "        df[f'sleep_duration_lag{lag}'] = df.groupby('subject_id')['sleep_duration_min'].shift(lag)\n",
        "        df[f'sleep_time_diff_lag{lag}'] = df.groupby('subject_id')['sleep_time'].diff(lag)\n",
        "        df[f'wake_time_diff_lag{lag}'] = df.groupby('subject_id')['wake_time'].diff(lag)\n",
        "        df[f'sleep_duration_diff_lag{lag}'] = df.groupby('subject_id')['sleep_duration_min'].diff(lag)\n",
        "    df['week_type_lag1'] = df.groupby('subject_id')['week_type'].shift(1)\n",
        "\n",
        "    # ì´ë™ í‰ê·  (2,3)\n",
        "    for window in [2, 3]:\n",
        "        df[f'rolling_sleep_time_{window}d'] = df.groupby('subject_id')['sleep_time'].rolling(window=window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "        df[f'rolling_wake_time_{window}d'] = df.groupby('subject_id')['wake_time'].rolling(window=window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "        df[f'rolling_sleep_duration_{window}d'] = df.groupby('subject_id')['sleep_duration_min'].rolling(window=window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "\n",
        "    # ìš”ì¼ë³„ í‰ê·  ìˆ˜ë©´ ë¹„êµ\n",
        "    weekday_avg = df.groupby(['subject_id', 'weekday'])['sleep_duration_min'].mean().reset_index(name='weekday_avg_sleep')\n",
        "    df = df.merge(weekday_avg, on=['subject_id', 'weekday'], how='left')\n",
        "    df['sleep_duration_vs_weekday_avg'] = df['sleep_duration_min'] - df['weekday_avg_sleep']\n",
        "\n",
        "    # ê¸‰ê²©í•œ ìˆ˜ë©´ì‹œê°„ ë³€í™” ì—¬ë¶€ (60ë¶„ ì´ìƒ ë³€í™”)\n",
        "    df['is_sleep_duration_change_large'] = (df['sleep_duration_diff_lag1'].abs() > 60).astype(int)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJ7DnRKMQPax"
      },
      "outputs": [],
      "source": [
        "mLight21 = process_mLight(mLight)\n",
        "\n",
        "# check\n",
        "print(f'\\n # mLight21 shape: {mLight21.shape}')\n",
        "mLight21.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11K9FshnP5bF"
      },
      "outputs": [],
      "source": [
        "mLight22 = process_mLight2(mLight)\n",
        "mLight22['sleep_time'] = np.where(mLight22['sleep_time']<10,mLight22['sleep_time']+24,mLight22['sleep_time'])\n",
        "mLight22['sleep_duration_min'] = mLight22.apply(lambda x: calculate_sleep_duration_min(x['sleep_time'],x['wake_time']),axis=1)\n",
        "mLight22 = add_ratios(mLight22)\n",
        "mLight22 = mLight22.drop(columns=['week_type','wake_time_lag1'])\n",
        "mLight22.columns = ['subject_id', 'lifelog_date']+['light_'+i for i in mLight22.columns if i not in ['subject_id', 'lifelog_date']]\n",
        "mLight22['lifelog_date'] = mLight22['lifelog_date'].astype(str)\n",
        "\n",
        "# check\n",
        "# mLight22 shape: (700, 55)\n",
        "print(f'\\n # mLight22 shape: {mLight22.shape}')\n",
        "mLight22.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6Cb1b3RXHEY"
      },
      "outputs": [],
      "source": [
        "def estimate_lights_off_time(df, light_threshold=2):\n",
        "\n",
        "    # ì‹œê°„ â†’ ì‹¤ìˆ˜í˜• (ì˜ˆ: 23:30 â†’ 23.5)\n",
        "    def time_to_float(t):\n",
        "        if pd.isna(t):\n",
        "            return None\n",
        "        return round(t.hour + t.minute / 60 + t.second / 3600, 4)\n",
        "\n",
        "    df = df.copy()\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['lifelog_date'] = pd.to_datetime(df['lifelog_date'])\n",
        "\n",
        "    # ë°¤ ì‹œê°„ëŒ€ í•„í„° (21ì‹œ~23ì‹œ or 0~3ì‹œ)\n",
        "    df['hour'] = df['timestamp'].dt.hour\n",
        "    df = df[(df['hour'] >= 21) | (df['hour'] <= 3)].copy()\n",
        "\n",
        "    # ìì • ì´í›„ëŠ” ì „ë‚  nightë¡œ ì²˜ë¦¬\n",
        "    df.loc[df['hour'] <= 3, 'lifelog_date'] -= pd.Timedelta(days=1)\n",
        "\n",
        "    # ë‚®ì€ ì¡°ë„ ì¡°ê±´\n",
        "    df = df[df['m_light'] <= light_threshold]\n",
        "\n",
        "    # ê° (subject_id, lifelog_date)ë³„ ë¶ˆ ëˆ ì‹œê° ì¶”ì¶œ\n",
        "    lights_off_df = (\n",
        "        df.groupby(['subject_id', 'lifelog_date'])['timestamp']\n",
        "        .min()\n",
        "        .reset_index(name='lights_off_time')\n",
        "    )\n",
        "\n",
        "    # ì‹¤ìˆ˜í˜• ì‹œê°ìœ¼ë¡œ ë³€í™˜\n",
        "    lights_off_df['lights_off_time'] = lights_off_df['lights_off_time'].dt.time.apply(time_to_float)\n",
        "\n",
        "    return lights_off_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npFr7zVBXHHt"
      },
      "outputs": [],
      "source": [
        "mLight23 = estimate_lights_off_time(mLight)\n",
        "mLight23['lights_off_time'] = np.where(mLight23['lights_off_time']<10,mLight23['lights_off_time']+24,mLight23['lights_off_time'])\n",
        "mLight23.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5ZTeeq0XHLk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSosIjMtXHTs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgoIsV5bgXSA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHIH4SYXbHUK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUlU9AKma3Tg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHYijr_sa3sz"
      },
      "source": [
        "### ğŸ“Œ mScreenStatus í™”ë©´ ì‚¬ìš©ì—¬ë¶€ (LLM impute)\n",
        "\n",
        "- Indicates whether the smartphone screen is in use.\n",
        " - ê¸°ìƒì‹œê°„, ì·¨ì¹¨ì‹œê°„, ìˆ˜ë©´ì‹œê°„\n",
        " - íœ´ëŒ€í° ì´ìš©íšŸìˆ˜, ì´ìš©ì‹œê°„\n",
        " - 00 - 05 ì‚¬ì´ì— íœ´ëŒ€í° ì´ìš©í•œ ê±´ìˆ˜\n",
        " - ê²°ì¸¡ì¹˜ ì²˜ë¦¬ x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGEyFT4ha4bU"
      },
      "outputs": [],
      "source": [
        "mScreenStatus = pd.read_parquet(f'{PATH}/ETRI_lifelog_dataset/ch2025_data_items/ch2025_mScreenStatus.parquet')\n",
        "\n",
        "mScreenStatus['lifelog_date'] = mScreenStatus['timestamp'].astype(str).str[:10]\n",
        "# mScreenStatus = fill_missing_dates_by_subject(mScreenStatus)\n",
        "mScreenStatus.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cWpAYZxy32U"
      },
      "outputs": [],
      "source": [
        "def preprocess_mScreenStatus(df):\n",
        "    from datetime import datetime, time as dtime, timedelta\n",
        "\n",
        "    df = df.copy()\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['lifelog_date'] = pd.to_datetime(df['lifelog_date'])\n",
        "\n",
        "    # base key í™•ë³´\n",
        "    base_keys = df[['subject_id', 'lifelog_date']].drop_duplicates()\n",
        "    base_keys['lifelog_date'] = base_keys['lifelog_date'].dt.date\n",
        "\n",
        "    # ë°¤ 9ì‹œë¶€í„° ë‹¤ìŒë‚  ì˜¤ì „ 11ì‹œ í•„í„°ë§\n",
        "    df['hour'] = df['timestamp'].dt.hour\n",
        "    df = df[(df['hour'] >= 21) | (df['hour'] < 11)].copy()\n",
        "    df.loc[df['hour'] < 11, 'lifelog_date'] -= pd.Timedelta(days=1)\n",
        "\n",
        "    df.sort_values(['subject_id', 'timestamp'], inplace=True)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for (subject_id, lifelog_date), group in df.groupby(['subject_id', 'lifelog_date']):\n",
        "        group = group.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "        # 1. ì¤‘ê°„ ê°ì„±(ì•ë’¤ 0, ë³¸ì¸ 1) ì œê±°\n",
        "        prev = group['m_screen_use'].shift(1)\n",
        "        next_ = group['m_screen_use'].shift(-1)\n",
        "        mask = (group['m_screen_use'] == 1) & (prev == 0) & (next_ == 0)\n",
        "        group.loc[mask, 'm_screen_use'] = 0\n",
        "\n",
        "        # 2. ë¸”ë¡ ë‹¨ìœ„ë¡œ ì§§ì€ ê°ì„± ë¸”ë¡ ì œê±°\n",
        "        group['is_sleep'] = group['m_screen_use'] == 0\n",
        "        group['block'] = (group['is_sleep'] != group['is_sleep'].shift()).cumsum()\n",
        "        block_info = group.groupby('block').agg(\n",
        "            is_sleep=('is_sleep', 'first'),\n",
        "            size=('is_sleep', 'size')\n",
        "        )\n",
        "\n",
        "        for i in range(1, len(block_info) - 1):\n",
        "            if (\n",
        "                block_info.iloc[i]['is_sleep'] == False and\n",
        "                block_info.iloc[i]['size'] <= 2 and\n",
        "                block_info.iloc[i - 1]['is_sleep'] and\n",
        "                block_info.iloc[i + 1]['is_sleep']\n",
        "            ):\n",
        "                group.loc[group['block'] == block_info.index[i], 'm_screen_use'] = 0\n",
        "\n",
        "        # ë‹¤ì‹œ ë¸”ë¡ ì¬ê³„ì‚° í›„ ìˆ˜ë©´ ì¶”ì •\n",
        "        group['is_sleep'] = group['m_screen_use'] == 0\n",
        "        group['block'] = (group['is_sleep'] != group['is_sleep'].shift()).cumsum()\n",
        "        sleep_blocks = group[group['is_sleep']].groupby('block').agg(\n",
        "            sleep_start=('timestamp', 'first'),\n",
        "            sleep_end=('timestamp', 'last'),\n",
        "            duration_min=('timestamp', lambda x: (x.max() - x.min()).total_seconds() / 60)\n",
        "        )\n",
        "\n",
        "        sleep_time = wake_time = duration_min = None\n",
        "        if not sleep_blocks.empty:\n",
        "            longest_sleep = sleep_blocks.loc[sleep_blocks['duration_min'].idxmax()]\n",
        "            sleep_time = longest_sleep['sleep_start'].time()\n",
        "            wake_time = longest_sleep['sleep_end'].time()\n",
        "            duration_min = (\n",
        "                datetime.combine(datetime.today(), wake_time) - datetime.combine(datetime.today(), sleep_time)\n",
        "            ).total_seconds() / 60\n",
        "            if duration_min < 0:\n",
        "                duration_min += 1440\n",
        "\n",
        "            if not (4 <= wake_time.hour < 11):\n",
        "                wake_time = None\n",
        "            if not (sleep_time.hour >= 21 or sleep_time.hour < 3):\n",
        "                sleep_time = None\n",
        "            if duration_min < 100:\n",
        "                sleep_time = None\n",
        "                wake_time = None\n",
        "                duration_min = None\n",
        "\n",
        "        results.append({\n",
        "            'subject_id': subject_id,\n",
        "            'lifelog_date': lifelog_date.date(),\n",
        "            'sleep_time': sleep_time,\n",
        "            'wake_time': wake_time,\n",
        "            'sleep_duration_min': round(duration_min, 1) if duration_min is not None else None\n",
        "        })\n",
        "\n",
        "\n",
        "    sleep_df = pd.DataFrame(results)\n",
        "    result_df = base_keys.merge(sleep_df, on=['subject_id', 'lifelog_date'], how='left')\n",
        "\n",
        "    # ì‹œê°„ â†’ ì‹¤ìˆ˜í˜• ìˆ«ì (ì˜ˆ: 23:30 â†’ 23.5)\n",
        "    def time_to_float(t):\n",
        "        if pd.isna(t):\n",
        "            return None\n",
        "        return round(t.hour + t.minute / 60 + t.second / 3600, 4)\n",
        "\n",
        "    result_df['sleep_time'] = result_df['sleep_time'].apply(time_to_float)\n",
        "    result_df['wake_time'] = result_df['wake_time'].apply(time_to_float)\n",
        "\n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5EqRmCxgYoq"
      },
      "outputs": [],
      "source": [
        "def preprocess_mScreenStatus(df):\n",
        "    from datetime import datetime, timedelta\n",
        "\n",
        "    df = df.copy()\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['lifelog_date'] = pd.to_datetime(df['lifelog_date'])\n",
        "\n",
        "    base_keys = df[['subject_id', 'lifelog_date']].drop_duplicates()\n",
        "    base_keys['lifelog_date'] = base_keys['lifelog_date'].dt.date\n",
        "\n",
        "    # ë°¤ 9ì‹œ ~ ë‹¤ìŒë‚  ì˜¤ì „ 11ì‹œ í•„í„°ë§\n",
        "    df['hour'] = df['timestamp'].dt.hour\n",
        "    df = df[(df['hour'] >= 21) | (df['hour'] < 11)].copy()\n",
        "    df.loc[df['hour'] < 11, 'lifelog_date'] -= pd.Timedelta(days=1)\n",
        "    df.sort_values(['subject_id', 'timestamp'], inplace=True)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for (subject_id, lifelog_date), group in df.groupby(['subject_id', 'lifelog_date']):\n",
        "        group = group.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "        # ì¤‘ê°„ ê°ì„± ì œê±°\n",
        "        prev = group['m_screen_use'].shift(1)\n",
        "        next_ = group['m_screen_use'].shift(-1)\n",
        "        mask = (group['m_screen_use'] == 1) & (prev == 0) & (next_ == 0)\n",
        "        group.loc[mask, 'm_screen_use'] = 0\n",
        "\n",
        "        # ì§§ì€ ê°ì„± ë¸”ë¡ ì œê±°\n",
        "        group['is_sleep'] = group['m_screen_use'] == 0\n",
        "        group['block'] = (group['is_sleep'] != group['is_sleep'].shift()).cumsum()\n",
        "        block_info = group.groupby('block').agg(\n",
        "            is_sleep=('is_sleep', 'first'),\n",
        "            size=('is_sleep', 'size')\n",
        "        )\n",
        "\n",
        "        for i in range(1, len(block_info) - 1):\n",
        "            if (\n",
        "                block_info.iloc[i]['is_sleep'] == False and\n",
        "                block_info.iloc[i]['size'] <= 2 and\n",
        "                block_info.iloc[i - 1]['is_sleep'] and\n",
        "                block_info.iloc[i + 1]['is_sleep']\n",
        "            ):\n",
        "                group.loc[group['block'] == block_info.index[i], 'm_screen_use'] = 0\n",
        "\n",
        "        # ë¸”ë¡ ì¬ê³„ì‚°\n",
        "        group['is_sleep'] = group['m_screen_use'] == 0\n",
        "        group['block'] = (group['is_sleep'] != group['is_sleep'].shift()).cumsum()\n",
        "\n",
        "        sleep_blocks = group[group['is_sleep']].groupby('block').agg(\n",
        "            sleep_start=('timestamp', 'first'),\n",
        "            sleep_end=('timestamp', 'last'),\n",
        "            duration_min=('timestamp', lambda x: (x.max() - x.min()).total_seconds() / 60)\n",
        "        )\n",
        "\n",
        "        sleep_time = wake_time = duration_min = None\n",
        "        if not sleep_blocks.empty:\n",
        "            longest_sleep = sleep_blocks.loc[sleep_blocks['duration_min'].idxmax()]\n",
        "            sleep_time = longest_sleep['sleep_start'].time()\n",
        "            wake_time = longest_sleep['sleep_end'].time()\n",
        "            duration_min = longest_sleep['duration_min']  # âœ… ì •í™•í•˜ê²Œ ìì • ë„˜ëŠ” ê²½ìš°ë„ ë°˜ì˜ë¨\n",
        "\n",
        "            # ìœ íš¨ ì‹œê°„ ë²”ìœ„ ì¡°ê±´\n",
        "            if not (4 <= wake_time.hour < 11):\n",
        "                wake_time = None\n",
        "            if not (sleep_time.hour >= 21 or sleep_time.hour < 3):\n",
        "                sleep_time = None\n",
        "            if duration_min < 100:\n",
        "                sleep_time = None\n",
        "                wake_time = None\n",
        "                duration_min = None\n",
        "\n",
        "        results.append({\n",
        "            'subject_id': subject_id,\n",
        "            'lifelog_date': lifelog_date.date(),\n",
        "            'sleep_time': sleep_time,\n",
        "            'wake_time': wake_time,\n",
        "            'sleep_duration_min': round(duration_min, 1) if duration_min is not None else None\n",
        "        })\n",
        "\n",
        "    sleep_df = pd.DataFrame(results)\n",
        "    result_df = base_keys.merge(sleep_df, on=['subject_id', 'lifelog_date'], how='left')\n",
        "\n",
        "    # ì‹œê°„ â†’ ì‹¤ìˆ˜í˜• ìˆ«ì ë³€í™˜\n",
        "    def time_to_float(t):\n",
        "        if pd.isna(t):\n",
        "            return None\n",
        "        return round(t.hour + t.minute / 60 + t.second / 3600, 4)\n",
        "\n",
        "    result_df['sleep_time'] = result_df['sleep_time'].apply(time_to_float)\n",
        "    result_df['wake_time'] = result_df['wake_time'].apply(time_to_float)\n",
        "\n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcw25BCASra"
      },
      "outputs": [],
      "source": [
        "def add_ratios(df):\n",
        "    df = df.copy()\n",
        "    df['lifelog_date'] = pd.to_datetime(df['lifelog_date'])\n",
        "    df['weekday'] = df['lifelog_date'].dt.weekday  # 0=ì›” ~ 6=ì¼\n",
        "    df['week_type'] = df['weekday'].apply(lambda x: 'weekend' if x in [4,5] else 'weekday') # ê¸ˆ,í† \n",
        "    df['month'] = df['lifelog_date'].dt.month\n",
        "\n",
        "    # í‰ê·  ê³„ì‚°\n",
        "    avg_duration = df.groupby(['subject_id', 'month', 'week_type'])['sleep_duration_min'].mean().reset_index(name='avg_sleep_duration')\n",
        "    sleep_time_avg = df.groupby(['subject_id', 'month', 'week_type'])['sleep_time'].apply(calculate_circular_mean_sleep_time).reset_index(name='avg_sleep_time')\n",
        "    wake_time_avg = df.groupby(['subject_id', 'month', 'week_type'])['wake_time'].apply(calculate_circular_mean_sleep_time).reset_index(name='avg_wake_time')\n",
        "    avg_df = sleep_time_avg.merge(wake_time_avg, on=['subject_id', 'month', 'week_type']).merge(avg_duration, on=['subject_id', 'month', 'week_type'])\n",
        "    df = df.merge(avg_df, on=['subject_id', 'month', 'week_type'], how='left')\n",
        "\n",
        "    # ë¹„ìœ¨ ë³€ìˆ˜\n",
        "    df['sleep_time_diff'] = df['avg_sleep_time'] - df['sleep_time']\n",
        "    df['wake_time_diff'] = df['avg_wake_time'] - df['wake_time']\n",
        "    df['sleep_duration_diff'] = df['avg_sleep_duration'] - df['sleep_duration_min']\n",
        "    df['sleep_time_ratio'] = df['sleep_time'] / df['avg_sleep_time']\n",
        "    df['wake_time_ratio'] = df['wake_time'] / df['avg_wake_time']\n",
        "    df['sleep_duration_ratio'] = df['sleep_duration_min'] / df['avg_sleep_duration']\n",
        "\n",
        "    # ì •ë ¬ ë° lag/ë³€í™”ëŸ‰\n",
        "    df = df.sort_values(['subject_id', 'lifelog_date'])\n",
        "\n",
        "    # ì‹œì°¨ ë³€ìˆ˜\n",
        "    for lag in [1, 2]:\n",
        "        df[f'sleep_time_lag{lag}'] = df.groupby('subject_id')['sleep_time'].shift(lag)\n",
        "        df[f'wake_time_lag{lag}'] = df.groupby('subject_id')['wake_time'].shift(lag)\n",
        "        df[f'sleep_duration_lag{lag}'] = df.groupby('subject_id')['sleep_duration_min'].shift(lag)\n",
        "\n",
        "        df[f'sleep_time_diff_lag{lag}'] = df.groupby('subject_id')['sleep_time_diff'].shift(lag)\n",
        "        df[f'wake_time_diff_lag{lag}'] = df.groupby('subject_id')['wake_time_diff'].shift(lag)\n",
        "        df[f'sleep_duration_diff_lag{lag}'] = df.groupby('subject_id')['sleep_duration_diff'].shift(lag)\n",
        "\n",
        "        df[f'sleep_time_ratio_lag{lag}'] = df.groupby('subject_id')['sleep_time_ratio'].shift(lag)\n",
        "        df[f'wake_time_ratio_lag{lag}'] = df.groupby('subject_id')['wake_time_ratio'].shift(lag)\n",
        "        df[f'sleep_duration_ratio_lag{lag}'] = df.groupby('subject_id')['sleep_duration_ratio'].shift(lag)\n",
        "\n",
        "    # ì´ë™ í‰ê· \n",
        "    for window in [2,3,5,7]:\n",
        "        df[f'sleep_time_mean{window}d'] = df.groupby('subject_id')['sleep_time'].rolling(window=window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "        df[f'wake_time_mean{window}d'] = df.groupby('subject_id')['wake_time'].rolling(window=window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "        df[f'sleep_duration_min_mean{window}d'] = df.groupby('subject_id')['sleep_duration_min'].rolling(window=window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "\n",
        "        df[f'sleep_time_diff_mean{window}d'] = df.groupby('subject_id')['sleep_time_diff'].rolling(window=window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "        df[f'wake_time_diff_mean{window}d'] = df.groupby('subject_id')['wake_time_diff'].rolling(window=window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "        df[f'sleep_duration_diff_mean{window}d'] = df.groupby('subject_id')['sleep_duration_diff'].rolling(window=window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "\n",
        "        df[f'sleep_time_ratio_mean{window}d'] = df.groupby('subject_id')['sleep_time_ratio'].rolling(window=window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "        df[f'wake_time_ratio_mean{window}d'] = df.groupby('subject_id')['wake_time_ratio'].rolling(window=window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "        df[f'sleep_duration_ratio_mean{window}d'] = df.groupby('subject_id')['sleep_duration_ratio'].rolling(window=window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "\n",
        "        # ----\n",
        "\n",
        "        df[f'sleep_time_std{window}d'] = df.groupby('subject_id')['sleep_time'].rolling(window=window, min_periods=1).std().reset_index(level=0, drop=True)\n",
        "        df[f'wake_time_std{window}d'] = df.groupby('subject_id')['wake_time'].rolling(window=window, min_periods=1).std().reset_index(level=0, drop=True)\n",
        "        df[f'sleep_duration_min_std{window}d'] = df.groupby('subject_id')['sleep_duration_min'].rolling(window=window, min_periods=1).std().reset_index(level=0, drop=True)\n",
        "\n",
        "        df[f'sleep_time_diff_std{window}d'] = df.groupby('subject_id')['sleep_time_diff'].rolling(window=window, min_periods=1).std().reset_index(level=0, drop=True)\n",
        "        df[f'wake_time_diff_std{window}d'] = df.groupby('subject_id')['wake_time_diff'].rolling(window=window, min_periods=1).std().reset_index(level=0, drop=True)\n",
        "        df[f'sleep_duration_diff_std{window}d'] = df.groupby('subject_id')['sleep_duration_diff'].rolling(window=window, min_periods=1).std().reset_index(level=0, drop=True)\n",
        "\n",
        "        df[f'sleep_time_ratio_std{window}d'] = df.groupby('subject_id')['sleep_time_ratio'].rolling(window=window, min_periods=1).std().reset_index(level=0, drop=True)\n",
        "        df[f'wake_time_ratio_std{window}d'] = df.groupby('subject_id')['wake_time_ratio'].rolling(window=window, min_periods=1).std().reset_index(level=0, drop=True)\n",
        "        df[f'sleep_duration_ratio_std{window}d'] = df.groupby('subject_id')['sleep_duration_ratio'].rolling(window=window, min_periods=1).std().reset_index(level=0, drop=True)\n",
        "\n",
        "    # ìš”ì¼ë³„ í‰ê·  ìˆ˜ë©´ ë¹„êµ\n",
        "    weekday_avg = df.groupby(['subject_id', 'weekday'])['sleep_duration_min'].mean().reset_index(name='weekday_avg_sleep')\n",
        "    df = df.merge(weekday_avg, on=['subject_id', 'weekday'], how='left')\n",
        "    df['sleep_duration_weekday_avg_diff'] = df['sleep_duration_min'] - df['weekday_avg_sleep']\n",
        "    df['sleep_duration_weekday_avg_div'] = df['sleep_duration_min'] / df['weekday_avg_sleep']\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clNnftHfukWb"
      },
      "outputs": [],
      "source": [
        "mScreenStatus2 = preprocess_mScreenStatus(mScreenStatus)\n",
        "print(mScreenStatus2.shape)\n",
        "\n",
        "# mScreenStatus2 = fill_missing_dates_by_subject(mScreenStatus2)\n",
        "# print(mScreenStatus2.shape)\n",
        "\n",
        "# weekday_map = {\n",
        "#     0: 'ì›”ìš”ì¼', 1: 'í™”ìš”ì¼', 2: 'ìˆ˜ìš”ì¼', 3: 'ëª©ìš”ì¼',\n",
        "#     4: 'ê¸ˆìš”ì¼', 5: 'í† ìš”ì¼', 6: 'ì¼ìš”ì¼'\n",
        "# }\n",
        "# mScreenStatus2['weekday'] = mScreenStatus2['lifelog_date'].dt.dayofweek.map(weekday_map)\n",
        "# mScreenStatus2['month'] = mScreenStatus2['lifelog_date'].dt.month\n",
        "\n",
        "# mScreenStatus2['lifelog_date'] = mScreenStatus2['lifelog_date'].astype(str)\n",
        "# train['lifelog_date'] = train['lifelog_date'].astype(str)\n",
        "\n",
        "# mScreenStatus2 = mScreenStatus2.merge(train[['subject_id','lifelog_date','Q1','Q2','Q3','S1','S2','S3']],on=['subject_id','lifelog_date'],how='left')\n",
        "# print(mScreenStatus2.shape)\n",
        "\n",
        "# a1_map = {\n",
        "#     'sleep_duration_min':'ìˆ˜ë©´ì‹œê°„(ë¶„)',\n",
        "#     'sleep_time':'ì·¨ì¹¨ì‹œê°„',\n",
        "#     'wake_time':'ê¸°ìƒì‹œê°„',\n",
        "#     'Q1':'Q1(ê¸°ìƒì§í›„ ìˆ˜ë©´ì˜ì§ˆ)',\n",
        "#     'Q2':'Q2(ì·¨ì¹¨ì§ì „ ì‹ ì²´ì í”¼ë¡œ)',\n",
        "#     'Q3':'Q3(ì·¨ì¹¨ì§ì „ ìŠ¤íŠ¸ë ˆìŠ¤)',\n",
        "#     'S1':'S1(ê¸°ìƒì§í›„ ìˆ˜ë©´ì‹œê°„)',\n",
        "#     'S2':'S2(ê¸°ìƒì§í›„ ìˆ˜ë©´íš¨ìœ¨)',\n",
        "#     'S3':'S3(ê¸°ìƒì§í›„ ìˆ˜ë©´ì§€ì—°ì‹œê°„)',\n",
        "# }\n",
        "\n",
        "# mScreenStatus2 = mScreenStatus2.rename(columns=a1_map)\n",
        "mScreenStatus2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9VZmhGoPtbM"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# CPU times: user 40.4 s, sys: 7.17 s, total: 47.6 s\n",
        "# Wall time: 59min 11s\n",
        "\n",
        "\"\"\"\n",
        "- mACStatus: Indicates whether the smartphone is currently being charged.\n",
        "- mActivity: Value calculated by the Google Activity Recognition API.\n",
        "- mAmbience: Ambient sound identification labels and their respective probabilities.\n",
        "- mBle: Bluetooth devices around individual subject.\n",
        "- mGps: Multiple GPS coordinates measured within a single minute using the smartphone.\n",
        "- mLight: Ambient light measured by the smartphone.\n",
        "- (âœ”ï¸) mScreenStatus: Indicates whether the smartphone screen is in use.\n",
        "- mUsageStats: Indicates which apps were used on the smartphone and for how long.\n",
        "- mWifi: Wifi devices around individual subject.\n",
        "- wHr: Heart rate readings recorded by the smartwatch.\n",
        "- wLight: Ambient light measured by the smartwatch.\n",
        "- wPedo: Step data recorded by the smartwatch.\n",
        "\"\"\"\n",
        "\n",
        "system_message = f\"\"\"\n",
        "# ğŸ”ˆì§€ì¹¨: ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
        "- ëª¨ë¸ ì„¤ëª… : For the purpose of training a learning model to predict sleep health, fatigue, and stress, the following six metrics were derived from sleep sensor data and self-reported survey records.\n",
        "- [ë°ì´í„°]ì—ëŠ” [ì·¨ì¹¨ì‹œê°„], [ê¸°ìƒì‹œê°„] ê²°ì¸¡ì¹˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ë¥¼ ì±„ì›Œì•¼ í•©ë‹ˆë‹¤.\n",
        "- [ì·¨ì¹¨ì‹œê°„]ê³¼ [ê¸°ìƒì‹œê°„]ì€ **24ì‹œê°„ì œë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•œ 'ì†Œìˆ˜ ì‹œê°„(decimal hour)' í˜•ì‹**ì…ë‹ˆë‹¤.\n",
        "    ì˜ˆ) 23.50 â†’ 23ì‹œ 30ë¶„, 0.75 â†’ 0ì‹œ 45ë¶„, 1.0 â†’ 1ì‹œ 0ë¶„\n",
        "\n",
        "# ğŸ” í‰ê· ê°’ ì„¤ëª…\n",
        "- í‰ê·  ì·¨ì¹¨ì‹œê°„/ê¸°ìƒì‹œê°„ì€ `21.0~2.0` ë˜ëŠ” `3.0~11.0` ë²”ìœ„ì—ì„œ ë‚˜íƒ€ë‚˜ë©°, ì´ëŠ” **ë‹¤ìŒë‚ ë¡œ ë„˜ì–´ê°€ëŠ” ì›í˜• ì‹œê°„ ë²”ìœ„ì…ë‹ˆë‹¤**.\n",
        "    ì˜ˆ) 1.4776ì€ 1ì‹œ 28ë¶„ì„ ì˜ë¯¸í•˜ë©°, ì´ëŠ” 24ì‹œë¥¼ ë„˜ì–´ì„  ê²ƒì´ ì•„ë‹ˆë¼ **ìì • ì´í›„ ì •ìƒì ì¸ ì‹œê°„ëŒ€**ì…ë‹ˆë‹¤.\n",
        "\n",
        "# ğŸ”ˆê²°ì¸¡ì¹˜ ë³´ì™„ ê·œì¹™\n",
        "- í‰ê·  ì·¨ì¹¨ì‹œê°„/ê¸°ìƒì‹œê°„ì€ ì´ë¯¸ ì˜¬ë°”ë¥¸ í¬ë§·ì´ë©°, 21.0~2.0(ì·¨ì¹¨), 3.0~11.0(ê¸°ìƒ) ë²”ìœ„ ë‚´ì˜ ê°’ì…ë‹ˆë‹¤. **ì¶©ë¶„íˆ í—ˆìš© ê°€ëŠ¥í•œ ê°’ì…ë‹ˆë‹¤. í˜¼ë€ì„ ëŠë¼ì§€ ë§ê³  ê·¸ëŒ€ë¡œ í™œìš©í•˜ì„¸ìš”.**\n",
        "- í‰ê· ê°’ì´ 0ë³´ë‹¤ ì‘ì€ ê²½ìš°ëŠ” ì—†ìœ¼ë©°, 1.4ì™€ ê°™ì€ ê°’ì€ 1ì‹œ 24ë¶„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
        "\n",
        "# ğŸ”ˆê¸ˆì§€ ì‚¬í•­\n",
        "- ë°ì´í„° í˜•ì‹ì´ë‚˜ ë²”ìœ„ë¥¼ ì˜ì‹¬í•˜ê±°ë‚˜, í‰ê· ê°’ì´ í—ˆìš© ë²”ìœ„ë¥¼ ë²—ì–´ë‚œë‹¤ê³  ê°€ì •í•˜ì§€ ë§ˆì„¸ìš”.\n",
        "- ë°˜ë³µì ìœ¼ë¡œ ì‹œê°„ í¬ë§·ì„ ì¬í•´ì„í•˜ê±°ë‚˜ 'ì´ ê°’ì´ ì˜ëª»ëœ ê²ƒ ê°™ë‹¤'ëŠ” ë‚´ì  íŒë‹¨ì„ í•˜ì§€ ë§ˆì„¸ìš”.\n",
        "\n",
        "# ğŸ”ˆì£¼ì˜ì‚¬í•­ (ì¤‘ìš”!!)\n",
        "- [ì·¨ì¹¨ì‹œê°„] ê²°ì¸¡ê°’ ì¶”ì • ì‹œ, 21.0 ~ 2.0 ë²”ìœ„ ë‚´ì˜ ê°’ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "- [ê¸°ìƒì‹œê°„] ê²°ì¸¡ê°’ ì¶”ì • ì‹œ, 3.0 ~ 11.0 ë²”ìœ„ ë‚´ì˜ ê°’ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "- [ì·¨ì¹¨ì‹œê°„], [ê¸°ìƒì‹œê°„] ê²°ì¸¡ê°’ì„ ì¶”ì •í•  ë•Œ ë‹¤ì–‘í•œ ì •ë³´ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "  â†’ ë‹¤ì–‘í•œ ì •ë³´: ì „ì¼ í•™ìŠµë°ì´í„°, í†µê³„ë°ì´í„°, Q1~S1 ì •ë³´, ì£¼ë§ ìœ ë¬´(ê¸ˆìš”ì¼ í¬í•¨), 7~8ì›” ìœ ë¬´ ë“±\n",
        "- [ì·¨ì¹¨ì‹œê°„], [ê¸°ìƒì‹œê°„]ì˜ **ê¸°ì¡´ ê°’ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš° ì ˆëŒ€ë¡œ ìˆ˜ì •í•˜ì§€ ë§ˆì„¸ìš”.**\n",
        "- ê¸°ì¡´ê°’ì´ ìˆëŠ” ê²½ìš°ì—ëŠ” **í•´ë‹¹ ì…€ì„ ê·¸ëŒ€ë¡œ ìœ ì§€**í•˜ë©°, **ë¹ˆì¹¸(null)**ì¸ ê²½ìš°ì—ë§Œ ë³´ì™„í•©ë‹ˆë‹¤.\n",
        "- ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ì§„ í…Œì´ë¸”ì˜ ë‹¤ë¥¸ ê°’ë“¤ë„ ì ˆëŒ€ ìˆ˜ì •í•˜ì§€ ë§ˆì„¸ìš”. **ì˜¤ì§ ê²°ì¸¡ì¹˜ë§Œ ì±„ì›Œì•¼ í•©ë‹ˆë‹¤.**\n",
        "- ëˆ„ë½ëœ ì…€ì€ ë°˜ë“œì‹œ ì±„ì›Œì•¼ í•˜ë©°, **ë¹ˆ ì…€ ì—†ì´ ëª¨ë“  ì…€ì„ ì±„ì›Œì§„ ìƒíƒœë¡œ ì¶œë ¥**í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "- ì¶œë ¥ì€ í•­ìƒ **ëª¨ë“  ì…€ì— ê°’ì´ ì±„ì›Œì§„ ìƒíƒœ**ì—¬ì•¼ í•©ë‹ˆë‹¤. **ë¹ˆì¹¸ì„ ë‚¨ê¸°ì§€ ë§ˆì„¸ìš”.**\n",
        "\n",
        "### ğŸ”ˆë‹µë³€ ì‘ì„± ì–‘ì‹\n",
        "- ë‹µë³€ì— ì§€ì¹¨ë‚´ìš©ì„ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
        "- ê²°ì¸¡ì¹˜ë§Œ ì±„ìš°ê³  ê¸°ì¡´ê°’ì´ ì¡´ì¬í•˜ëŠ”ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "\"\"\"\n",
        "\n",
        "# ================================================================\n",
        "# ì„¤ì •ê°’\n",
        "save_path = '/content/drive/MyDrive/data/ch2025_data_items/fillna'\n",
        "dataname  = 'mScreenStatus'\n",
        "version   = '20250607_v4'\n",
        "data      = mScreenStatus2.copy()\n",
        "# ================================================================\n",
        "\n",
        "# run\n",
        "parsed_results = []\n",
        "for subject_id in tqdm(data['subject_id'].unique(), desc=\"Processing each subject\"):\n",
        "# for subject_id in tqdm(['id06'], desc=\"Processing each subject\"):\n",
        "\n",
        "    print(f'# subject_id:{subject_id}')\n",
        "\n",
        "    sub1 = data[data['subject_id'] == subject_id]\n",
        "    sub1 = sub1.drop(columns=['ìˆ˜ë©´ì‹œê°„(ë¶„)'])\n",
        "    í•™ìŠµë°ì´í„° = sub1.to_csv(index=False, sep=\"\\t\")\n",
        "\n",
        "    # ----------------------------------------------------------------------------------------------\n",
        "    # í†µê³„ë°ì´í„°\n",
        "    a1 = data[data['subject_id'] == subject_id].groupby(['weekday']).apply(lambda x:pd.Series({\n",
        "    'í‰ê· ì·¨ì¹¨ì‹œê°„': calculate_circular_mean_sleep_time(x['ì·¨ì¹¨ì‹œê°„'])\n",
        "    ,'í‰ê· ê¸°ìƒì‹œê°„': calculate_circular_mean_sleep_time(x['ê¸°ìƒì‹œê°„'])\n",
        "    })).reset_index()\n",
        "\n",
        "    a2 = data[data['subject_id'] == subject_id].groupby(['month','weekday']).apply(lambda x:pd.Series({\n",
        "    'í‰ê· ì·¨ì¹¨ì‹œê°„': calculate_circular_mean_sleep_time(x['ì·¨ì¹¨ì‹œê°„'])\n",
        "    ,'í‰ê· ê¸°ìƒì‹œê°„': calculate_circular_mean_sleep_time(x['ê¸°ìƒì‹œê°„'])\n",
        "    })).reset_index()\n",
        "\n",
        "    # a1ì„ weekday ê¸°ì¤€ìœ¼ë¡œ merge\n",
        "    a2_filled = a2.merge(\n",
        "        a1,\n",
        "        on='weekday',\n",
        "        suffixes=('', '_a1'),\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # ê²°ì¸¡ê°’ì´ ìˆëŠ” ê²½ìš° a1 ê°’ìœ¼ë¡œ ëŒ€ì²´\n",
        "    a2_filled['í‰ê· ì·¨ì¹¨ì‹œê°„'] = a2_filled['í‰ê· ì·¨ì¹¨ì‹œê°„'].fillna(a2_filled['í‰ê· ì·¨ì¹¨ì‹œê°„_a1'])\n",
        "    a2_filled['í‰ê· ê¸°ìƒì‹œê°„'] = a2_filled['í‰ê· ê¸°ìƒì‹œê°„'].fillna(a2_filled['í‰ê· ê¸°ìƒì‹œê°„_a1'])\n",
        "\n",
        "    # ë³´ì¡° ì»¬ëŸ¼ ì œê±°\n",
        "    sub2 = a2_filled.drop(columns=['í‰ê· ì·¨ì¹¨ì‹œê°„_a1', 'í‰ê· ê¸°ìƒì‹œê°„_a1'])\n",
        "    í†µê³„ë°ì´í„° = sub2.to_csv(index=False, sep=\"\\t\")\n",
        "    # ----------------------------------------------------------------------------------------------\n",
        "\n",
        "    user_message = f\"\"\"\n",
        "    # ğŸ”ˆì‘ì—… ìˆœì„œ\n",
        "    1. ê²°ì¸¡ì¹˜ [ì·¨ì¹¨ì‹œê°„]ì„ ì¶”ì •í•˜ì‹œì˜¤.\n",
        "    2. ê²°ì¸¡ì¹˜ [ê¸°ìƒì‹œê°„]ì„ ì¶”ì •í•˜ì‹œì˜¤ (ì¶”ê°€ì§€ì¹¨: ì „ ë‹¨ê³„ì—ì„œ ìƒì„± ëœ [ì·¨ì¹¨ì‹œê°„]ì„ ì°¸ê³ í•´ì„œ [ê¸°ìƒì‹œê°„] ê²°ì¸¡ê°’ì„ ì¶”ì •í•˜ì‹œì˜¤.)\n",
        "\n",
        "    # ğŸ”ˆ xdata ì„¤ëª…\n",
        "    - mScreenStatus(Indicates whether the smartphone screen is in use)ê¸°ë°˜ìœ¼ë¡œ íŒŒìƒëœ ì¶”ì • [ì·¨ì¹¨ì‹œê°„], [ê¸°ìƒì‹œê°„]\n",
        "    - [ì·¨ì¹¨ì‹œê°„]ê³¼ [ê¸°ìƒì‹œê°„]ì€ ì†Œìˆ˜ì ìœ¼ë¡œ í‘œí˜„ëœ 24ì‹œê°„ì œ ì‹œê°„ì…ë‹ˆë‹¤. (ì˜ˆ: 22.6500 â†’ 22ì‹œ 39ë¶„)\n",
        "\n",
        "    # ğŸ”ˆ ydata ì„¤ëª… (ì˜ˆì¸¡íƒ€ê²Ÿ)\n",
        "    - Q1: ê¸°ìƒ ì§í›„ ìˆ˜ë©´ì˜ ì§ˆ (0: í‰ê·  ì´í•˜, 1: í‰ê·  ì´ìƒ)\n",
        "    - Q2: ì·¨ì¹¨ ì§ì „ ì‹ ì²´ í”¼ë¡œ ìˆ˜ì¤€ (0: ë†’ì€ í”¼ë¡œ, 1: ë‚®ì€ í”¼ë¡œ)\n",
        "    - Q3: ì·¨ì¹¨ ì§ì „ ìŠ¤íŠ¸ë ˆìŠ¤ ìˆ˜ì¤€ (0: ë†’ì€ ìŠ¤íŠ¸ë ˆìŠ¤, 1: ë‚®ì€ ìŠ¤íŠ¸ë ˆìŠ¤)\n",
        "    - S1: ìˆ˜ë©´ì‹œê°„ ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜ì—¬ë¶€ (0: ë¯¸ì¤€ìˆ˜, 1: ë¶€ë¶„ ì¤€ìˆ˜, 2: ì™„ì „ ì¤€ìˆ˜)\n",
        "    - S2: ìˆ˜ë©´ íš¨ìœ¨ ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜ì—¬ë¶€ (0: ë¯¸ì¤€ìˆ˜, 1: ì¤€ìˆ˜)\n",
        "    - S3: ìˆ˜ë©´ ì ë“¤ê¸° ì§€ì—°ì‹œê°„ ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜ì—¬ë¶€ (0: ë¯¸ì¤€ìˆ˜, 1: ì¤€ìˆ˜)\n",
        "\n",
        "    # ğŸ”ˆë„ë©”ì¸ ì§€ì‹\n",
        "    - S1(ìˆ˜ë©´ì‹œê°„)ì´ 2(ì™„ì „ì¤€ìˆ˜)ì¸ ê²½ìš° -> ê¸°ìƒê¸°ìƒì´ í‰ì†Œë³´ë‹¤ ëŠ¦ì–´ì„œ ìˆ˜ë©´ì‹œê°„ì´ ì¶©ë¶„í•œ ê²½ìš°\n",
        "    - ê¸ˆìš”ì¼, í† ìš”ì¼ì—ëŠ” ë‹¤ìŒë‚ ì´ íœ´ì¼ì´ê¸° ë•Œë¬¸ì— ê¸°ìƒì‹œê°„ì„ í‰ì†Œë³´ë‹¤ ëŠ¦ê²Œ ì„¤ì •\n",
        "    - 7ì›”, 8ì›”ì—ëŠ” ë¬´ë”ìœ„(ì—¬ë¦„ì²  ê³ ì˜¨ ì‹œê¸°)ì—ëŠ” ê¸°ìƒì‹œê°„ì´ ì¼ë°˜ì ìœ¼ë¡œ ë” ë¹¨ë¼ì§€ëŠ” ê²½í–¥ ì¡´ì¬\n",
        "    - ì „ì¼ ìƒíƒœê°€ ì˜¤ëŠ˜ ìƒíƒœì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŒ (ì „ì¼ ëª¸ì´ ì¢‹ì§€ ì•Šìœ¼ë©´ ë‹¤ìŒë‚ ë„ ëª¸ì´ ì¢‹ì§€ ì•ŠëŠ” ê²ƒê³¼ ë™ì¼í•œ ì›ë¦¬)\n",
        "\n",
        "    # ğŸ”ˆí†µê³„ë°ì´í„°\n",
        "    {í†µê³„ë°ì´í„°}\n",
        "\n",
        "    # ğŸ”ˆë°ì´í„°\n",
        "    {í•™ìŠµë°ì´í„°}\n",
        "\n",
        "    # ğŸ”ˆë‹µë³€ ì¶œë ¥ í˜•ì‹\n",
        "    lifelog_date\\tsubject_id\\tì·¨ì¹¨ì‹œê°„\\tê¸°ìƒì‹œê°„\\n\n",
        "    2024-06-26\\tid01\\t23.4500\\t5.2500\\n\n",
        "    2024-06-27\\tid01\\t23.1333\\t5.3000\\n\n",
        "    2024-06-28\\tid01\\t23.0000\\t5.9167\\n\n",
        "\n",
        "    # ë‹µë³€:\n",
        "    \"\"\"\n",
        "\n",
        "    # ìµœëŒ€ 3íšŒ ì‹œë„\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            # ì§ˆì˜\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": user_message}\n",
        "            ]\n",
        "\n",
        "            sampling_params = SamplingParams(max_tokens=37000, temperature=0, seed=42)\n",
        "            outputs = llm.chat(messages, sampling_params=sampling_params)\n",
        "\n",
        "            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì €ì¥\n",
        "            result_text = outputs[0].outputs[0].text\n",
        "            with open(f\"{save_path}/{dataname}_{subject_id}_result_try{attempt+1}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(result_text)\n",
        "\n",
        "            # <think> ì œê±°\n",
        "            cleaned_text = re.sub(r\"<think>.*?</think>\", \"\", result_text, flags=re.DOTALL).strip()\n",
        "\n",
        "            # íŒŒì‹±\n",
        "            df_parsed = pd.read_csv(StringIO(cleaned_text), sep=\"\\t\")\n",
        "\n",
        "            # ê²°ì¸¡ì´ ë§ì„ ê²½ìš° ì¬ì‹œë„\n",
        "            # 1. ê²°ì¸¡ê°’ì´ 10ê°œ ë„˜ê±°ë‚˜\n",
        "            # 2. ìƒì„±í•œ ê°œìˆ˜ê°€ ì œê³µí•œ ìƒ˜í”Œë³´ë‹¤ 10ê°œ ì´ìƒ ì ì„ë•Œ\n",
        "            if (df_parsed['ê¸°ìƒì‹œê°„'].isna().sum() > 10) | ((len(sub1)-len(df_parsed)) > 10):\n",
        "                print(f\"[RETRY] ê²°ì¸¡ì¹˜ ë§ìŒ â†’ subject_id: {subject_id} (attempt {attempt+1})\")\n",
        "                continue\n",
        "\n",
        "            parsed_results.append(df_parsed)\n",
        "            break  # ì„±ê³µí•˜ë©´ ë°˜ë³µ ì¢…ë£Œ\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Parsing failed for subject {subject_id} (attempt {attempt+1}): {e}\")\n",
        "            if attempt == 1:\n",
        "                print(f\"[FAIL] ìµœì¢… ì‹¤íŒ¨ â†’ subject_id: {subject_id}\")\n",
        "            import time\n",
        "            time.sleep(2)  # ì¬ì‹œë„ ì „ì— ëŒ€ê¸°\n",
        "\n",
        "# ëª¨ë“  subject ê²°ê³¼ ë³‘í•©\n",
        "data_filled_na = pd.concat(parsed_results, ignore_index=True)\n",
        "data_filled_na = data_filled_na.rename(columns={'ì·¨ì¹¨ì‹œê°„':'ì·¨ì¹¨ì‹œê°„_llm','ê¸°ìƒì‹œê°„':'ê¸°ìƒì‹œê°„_llm'})\n",
        "data_filled_na.to_excel(f'{save_path}/{dataname}_llm_{version}.xlsx',index=False)\n",
        "\n",
        "# ë¹„êµê²€ì¦ë°ì´í„° ì €ì¥\n",
        "data_filled_na['lifelog_date'] = data_filled_na['lifelog_date'].astype(str)\n",
        "data['lifelog_date'] = data['lifelog_date'].astype(str)\n",
        "data_filled_na['subject_id'] = data_filled_na['subject_id'].astype(str)\n",
        "data['subject_id'] = data['subject_id'].astype(str)\n",
        "data = data.merge(data_filled_na,on=['lifelog_date','subject_id'],how='left')\n",
        "data['ìˆ˜ë©´ì‹œê°„_llm'] = data.apply(lambda x: calculate_sleep_duration_min(x['ì·¨ì¹¨ì‹œê°„_llm'],x['ê¸°ìƒì‹œê°„_llm']),axis=1)\n",
        "data.to_excel(f'{save_path}/{dataname}_llm_check_{version}.xlsx',index=False)\n",
        "\n",
        "# check\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ7ttdeHd-25"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbNH1smPd-9_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckIM5mjVd_F6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmNGOT1e6ww0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ“Œ LLM ê²°ì¸¡ê°’ ëŒ€ì²´"
      ],
      "metadata": {
        "id": "rVeJUPVQYUXO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vl6PJvtVQkXA"
      },
      "outputs": [],
      "source": [
        "# ì „ì²˜ë¦¬\n",
        "\n",
        "mScreenStatus2_llm = pd.read_excel('/content/drive/MyDrive/data/ch2025_data_items/share/hjy/mScreenStatus_llm_20250607_v4.xlsx')\n",
        "mScreenStatus2_llm.head()\n",
        "\n",
        "mScreenStatus2_llm['pk'] = mScreenStatus2_llm['subject_id']+mScreenStatus2_llm['lifelog_date']\n",
        "mScreenStatus2_llm = mScreenStatus2_llm.set_index('pk')\n",
        "\n",
        "mScreenStatus2 = preprocess_mScreenStatus(mScreenStatus)\n",
        "\n",
        "# ì·¨ì¹¨ì‹œê°„, ê¸°ìƒì‹œê°„ êµì²´\n",
        "\n",
        "mScreenStatus2['lifelog_date'] = mScreenStatus2['lifelog_date'].astype(str)\n",
        "mScreenStatus2['pk'] = mScreenStatus2['subject_id']+mScreenStatus2['lifelog_date']\n",
        "\n",
        "a1_map = mScreenStatus2_llm['ì·¨ì¹¨ì‹œê°„_llm'].to_dict()\n",
        "mScreenStatus2['sleep_time'] = np.where(mScreenStatus2['sleep_time'].isna(),mScreenStatus2['pk'].map(a1_map),mScreenStatus2['sleep_time'])\n",
        "\n",
        "a1_map = mScreenStatus2_llm['ê¸°ìƒì‹œê°„_llm'].to_dict()\n",
        "mScreenStatus2['wake_time'] = np.where(mScreenStatus2['wake_time'].isna(),mScreenStatus2['pk'].map(a1_map),mScreenStatus2['wake_time'])\n",
        "\n",
        "mScreenStatus2 = mScreenStatus2.drop(columns='pk')\n",
        "\n",
        "# ìˆ˜ë©´ì‹œê°„ ì¬ê³„ì‚°\n",
        "mScreenStatus2['sleep_time'] = np.where(mScreenStatus2['sleep_time']<10,mScreenStatus2['sleep_time']+24,mScreenStatus2['sleep_time'])         ### ìˆ˜ì •\n",
        "mScreenStatus2['sleep_duration_min'] = mScreenStatus2.apply(lambda x: calculate_sleep_duration_min(x['sleep_time'],x['wake_time']),axis=1)\n",
        "\n",
        "# ë¹„ìœ¨ ë³€ìˆ˜ ì¶”ê°€\n",
        "\n",
        "mScreenStatus2 = add_ratios(mScreenStatus2)\n",
        "\n",
        "# check\n",
        "print(f'\\n # mScreenStatus2 shape: {mScreenStatus2.shape}')\n",
        "mScreenStatus2.head(1)\n",
        "\n",
        "# í‰ê· ìˆ˜ë©´ì‹œê°„\n",
        "mScreenStatus2í‰ê· ìˆ˜ë©´ì‹œê°„ = mScreenStatus2.groupby(['subject_id','week_type']).apply(lambda x: pd.Series({\n",
        "     'í‰ê·  ì·¨ì¹¨ì‹œê°„':circular_mean_sleep_time(x['sleep_time'])\n",
        "    ,'í‰ê·  ê¸°ìƒì‹œê°„':circular_mean_sleep_time(x['wake_time'])\n",
        "    ,'í‰ê·  ìˆ˜ë©´ì‹œê°„':x['sleep_duration_min'].mean()\n",
        "})).reset_index()\n",
        "\n",
        "# ì €ì¥\n",
        "fname = f'{path}mScreenStatus2í‰ê· ìˆ˜ë©´ì‹œê°„.xlsx'\n",
        "print(fname)\n",
        "mScreenStatus2í‰ê· ìˆ˜ë©´ì‹œê°„.to_excel(fname, index=False)\n",
        "\n",
        "# check\n",
        "mScreenStatus2í‰ê· ìˆ˜ë©´ì‹œê°„.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dvQ-tBxQx6n"
      },
      "outputs": [],
      "source": [
        "mScreenStatus2.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1rMa4hHZEJh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7_ExIp_brjK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUxk1jIqa4jT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orMu9LKiW58-"
      },
      "source": [
        "### âœ”ï¸ mUsageStats ì•±ì‚¬ìš©í†µê³„\n",
        "- mUsageStats: Indicates which apps were used on the smartphone and for how long.\n",
        "\n",
        " - ëª‡ì‹œê¹Œì§€ í•¸ë“œí° ë³´ë‹¤ê°€ ì ì¤ëŠ”ì§€\n",
        " - í†µí™”, ì „í™” ì–¼ë§ˆë‚˜ í–ˆëŠ”ì§€\n",
        " - YouTube ì–¼ë§ˆë‚˜ ë´¤ëŠ”ì§€\n",
        " - ë©”ì‹œì§€, ì¹´ì¹´ì˜¤í†¡ ì–¼ë§ˆë‚˜ í–ˆëŠ”ì§€\n",
        " - NAVER ì–¼ë§ˆë‚˜ í–ˆëŠ”ì§€\n",
        " - í‰ì†Œë³´ë‹¤ ì–¼ë§ˆë‚˜ ë§ì€ ì•±ì„ ì´ìš©í–ˆëŠ”ì§€\n",
        " - ì œì™¸? -> ì‹œìŠ¤í…œ UI,One UI í™ˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr1oENhJWfh0"
      },
      "outputs": [],
      "source": [
        "def extract_mUsageStats_info(row):\n",
        "    m_data = row['m_usage_stats']\n",
        "    app_name = [item['app_name'] for item in m_data]\n",
        "    total_time = [item['total_time'] for item in m_data]\n",
        "    return pd.Series({'app_name': app_name, 'total_time': total_time})\n",
        "\n",
        "mUsageStats[['app_name', 'total_time']] = mUsageStats.apply(extract_mUsageStats_info, axis=1)\n",
        "mUsageStats['lifelog_date'] = mUsageStats['timestamp'].astype(str).str[:10]\n",
        "# mUsageStats = fill_missing_dates_by_subject(mUsageStats)\n",
        "mUsageStats.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUX_rMJg4I0D"
      },
      "outputs": [],
      "source": [
        "def process_mUsageStats(df):\n",
        "    df = df.copy()\n",
        "    df['lifelog_date'] = pd.to_datetime(df['lifelog_date'])\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['ìš”ì¼'] = df['lifelog_date'].dt.day_name()\n",
        "    df['hour'] = df['timestamp'].dt.hour\n",
        "\n",
        "    # ì‹œê°„ëŒ€ ë¶„ë¥˜\n",
        "    def map_time_period(row):\n",
        "        if 20 <= row['hour'] <= 23:\n",
        "            return 'beforebed'\n",
        "        else:\n",
        "            return 'activehour'\n",
        "\n",
        "    df['time_period'] = df.apply(map_time_period, axis=1)\n",
        "\n",
        "    # ë¦¬ìŠ¤íŠ¸ í‰íƒ„í™”\n",
        "    exploded_df = df.explode(['app_name', 'total_time'])\n",
        "    exploded_df['total_time'] = exploded_df['total_time'].astype(float)\n",
        "    exploded_df['total_time'] = exploded_df['total_time'] * 0.001 / 60  # ë°€ë¦¬ì´ˆ â†’ ì´ˆ â†’ ë¶„ ë³€í™˜\n",
        "\n",
        "    # app_name íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
        "    exploded_df['app_name'] = exploded_df['app_name'].astype(str).apply(\n",
        "        lambda x: re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', x)\n",
        "    )\n",
        "\n",
        "    # ì‹œìŠ¤í…œ ì•± ì œê±°\n",
        "    filtered_df = exploded_df[~exploded_df['app_name'].isin(['ì‹œìŠ¤í…œUI'])]\n",
        "\n",
        "    # ì£¼ìš” íŒŒìƒë³€ìˆ˜ ìƒì„±\n",
        "    def calculate_daily_metrics(group):\n",
        "        app_times = {\n",
        "\n",
        "            'í†µí™”&ì „í™”_ì•±ì´ìš©ì‹œê°„': group[group['app_name'].isin(['í†µí™”','ì „í™”'])]['total_time'].sum(),\n",
        "            'ë©”ì‹ ì €&ì¹´ì¹´ì˜¤í†¡_ì•±ì´ìš©ì‹œê°„': group[group['app_name'].isin(['ë©”ì‹œì§€','ì¹´ì¹´ì˜¤í†¡'])]['total_time'].sum(),\n",
        "            'YouTube_ì•±ì´ìš©ì‹œê°„': group[group['app_name'] == 'YouTube']['total_time'].sum(),\n",
        "            'NAVER_ì•±ì´ìš©ì‹œê°„': group[group['app_name'] == 'NAVER']['total_time'].sum(),\n",
        "            'ìºì‹œì›Œí¬_ì•±ì´ìš©ì‹œê°„': group[group['app_name'] == 'ìºì‹œì›Œí¬']['total_time'].sum(),\n",
        "            'ì„±ê²½ì¼ë…Q_ì•±ì´ìš©ì‹œê°„': group[group['app_name'] == 'ì„±ê²½ì¼ë…Q']['total_time'].sum(),\n",
        "        }\n",
        "\n",
        "        return pd.Series({\n",
        "            **app_times,\n",
        "            'ì´ìš©ì•±ê°œìˆ˜': group['app_name'].nunique(),\n",
        "            'í•¸ë“œí°ì‚¬ìš©ì‹œê°„': group['total_time'].sum()\n",
        "        })\n",
        "\n",
        "    # ì¼ì/ì‹œê°„ëŒ€ë³„ ìš”ì•½\n",
        "    daily_stats = filtered_df.groupby(['subject_id', 'lifelog_date', 'time_period']).apply(calculate_daily_metrics).reset_index()\n",
        "\n",
        "    # subject_idë³„ í‰ê·  ì´í™”ë©´ì‹œê°„\n",
        "    avg_screen_time = daily_stats.groupby('subject_id')['í•¸ë“œí°ì‚¬ìš©ì‹œê°„'].mean().to_dict()\n",
        "\n",
        "    # í‰ê·  ëŒ€ë¹„ ë¹„ìœ¨\n",
        "    def compute_screen_usage(row):\n",
        "        avg_time = avg_screen_time.get(row['subject_id'], np.nan)\n",
        "        if pd.isna(avg_time) or avg_time == 0:\n",
        "            return np.nan\n",
        "        return round((row['í•¸ë“œí°ì‚¬ìš©ì‹œê°„'] / avg_time - 1) * 100, 1)\n",
        "\n",
        "    daily_stats['í‰ê· ëŒ€ë¹„í•¸ë“œí°ì‚¬ìš©ì‹œê°„'] = daily_stats.apply(compute_screen_usage, axis=1)\n",
        "\n",
        "    # í”¼ë²—\n",
        "    daily_stats = daily_stats.pivot(index=['subject_id', 'lifelog_date'], columns='time_period')\n",
        "    daily_stats.columns = [f\"{tp}_{metric}\" for metric, tp in daily_stats.columns]\n",
        "    daily_stats = daily_stats.reset_index()\n",
        "\n",
        "    feats = [\n",
        "      'subject_id',\n",
        "      'lifelog_date',\n",
        "      'beforebed_í†µí™”&ì „í™”_ì•±ì´ìš©ì‹œê°„',\n",
        "      'beforebed_ë©”ì‹ ì €&ì¹´ì¹´ì˜¤í†¡_ì•±ì´ìš©ì‹œê°„',\n",
        "      'beforebed_YouTube_ì•±ì´ìš©ì‹œê°„',\n",
        "      'beforebed_í•¸ë“œí°ì‚¬ìš©ì‹œê°„',\n",
        "    ]\n",
        "\n",
        "    return daily_stats[feats]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8CRKpC4iDKa"
      },
      "outputs": [],
      "source": [
        "mUsageStats2 = process_mUsageStats(mUsageStats)\n",
        "\n",
        "# check\n",
        "print(f'\\n # mUsageStats2 shape: {mUsageStats2.shape}')\n",
        "mUsageStats2.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8uHsK9wc0Ls"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubjD-ZwOc0RP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ecpVUVhRo4e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1PcAUtkH81I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZHkETiDH84G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFVRGf8cbJMo"
      },
      "source": [
        "### âœ”ï¸ mWifi ì£¼ë³€wifi (ìˆ˜ì •)\n",
        "- Wifi devices around individual subject.\n",
        " - -30 ~ -50 dBm\të§¤ìš° ê°•í•œ ì‹ í˜¸ (ìµœì )\n",
        " - -51 ~ -60 dBm\tê°•í•œ ì‹ í˜¸ (ë¬¸ì œ ì—†ìŒ)\n",
        " - -61 ~ -70 dBm\tê´œì°®ì€ ì‹ í˜¸ (ì•½ê°„ ëŠë¦´ ìˆ˜ ìˆìŒ)\n",
        " - -71 ~ -80 dBm\tì•½í•œ ì‹ í˜¸ (ëŠê¹€ ì£¼ì˜)\n",
        " - -81 dBm ì´í•˜\të§¤ìš° ì•½í•œ ì‹ í˜¸ (ê±°ì˜ ëŠê¹€)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCIgP5FaH86F"
      },
      "outputs": [],
      "source": [
        "def extract_wifi_info(row):\n",
        "    wifi_data = row['m_wifi']\n",
        "    bssids = [item['bssid'] for item in wifi_data]\n",
        "    rssis = [item['rssi'] for item in wifi_data]\n",
        "    return pd.Series({'bssid': bssids, 'rssi': rssis})\n",
        "\n",
        "mWifi = pd.read_parquet(f'{PATH}/ETRI_lifelog_dataset/ch2025_data_items/ch2025_mWifi.parquet')\n",
        "mWifi[['bssid', 'rssi']] = mWifi.apply(extract_wifi_info, axis=1)\n",
        "mWifi['lifelog_date'] = mWifi['timestamp'].astype(str).str[:10]\n",
        "# mWifi = fill_missing_dates_by_subject(mWifi)\n",
        "mWifi.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeDXOGXDcYmo"
      },
      "outputs": [],
      "source": [
        "def process_mWifi(df,threshold):\n",
        "\n",
        "    df = df.copy()\n",
        "    df['lifelog_date'] = pd.to_datetime(df['lifelog_date'])\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['ìš”ì¼'] = df['lifelog_date'].dt.day_name()\n",
        "    df['hour'] = df['timestamp'].dt.hour\n",
        "\n",
        "    # ì‹œê°„ëŒ€ ë¶„ë¥˜\n",
        "    def map_time_period(row):\n",
        "        if 20 <= row['hour'] <= 23:\n",
        "            return 'beforebed'\n",
        "        else:\n",
        "            return 'activehour'\n",
        "\n",
        "    df['time_period'] = df.apply(map_time_period, axis=1)\n",
        "\n",
        "    features = []\n",
        "    grouped = df.groupby(['subject_id', 'lifelog_date','time_period'])\n",
        "\n",
        "    for (subject_id, date, period), group in grouped:\n",
        "        scan_count = len(group)\n",
        "        bssid_flat = sum(group['bssid'], [])  # flatten\n",
        "        rssi_flat = sum(group['rssi'], [])    # flatten\n",
        "\n",
        "        unique_bssid_count = len(set(bssid_flat))\n",
        "        avg_rssi = sum(rssi_flat) / len(rssi_flat) if rssi_flat else None\n",
        "        max_rssi = max(rssi_flat) if rssi_flat else None\n",
        "        min_rssi = min(rssi_flat) if rssi_flat else None\n",
        "        strong_rssi_ratio = sum(1 for r in rssi_flat if r > -60) / len(rssi_flat) if rssi_flat else 0\n",
        "        empty_scan_count = sum(1 for b in group['bssid'] if len(b) == 0)\n",
        "\n",
        "        # ê°€ì¥ ë§ì´ íƒì§€ëœ BSSID\n",
        "        bssid_counter = Counter(bssid_flat)\n",
        "        top_bssid, top_bssid_count = bssid_counter.most_common(1)[0] if bssid_counter else (None, 0)\n",
        "\n",
        "        first_time = group['timestamp'].min()\n",
        "        last_time = group['timestamp'].max()\n",
        "        hour_span = (last_time - first_time).total_seconds() / 60  # ë¶„ ë‹¨ìœ„\n",
        "\n",
        "        features.append({\n",
        "            'subject_id': subject_id,\n",
        "            'lifelog_date': date,\n",
        "            'time_period': period,  #\n",
        "            'scan_count': scan_count,\n",
        "            'unique_bssid_count': unique_bssid_count,\n",
        "            'avg_rssi': avg_rssi,\n",
        "            'max_rssi': max_rssi,\n",
        "            'min_rssi': min_rssi,\n",
        "            'strong_signal_ratio': strong_rssi_ratio,\n",
        "            'empty_scan_count': empty_scan_count,\n",
        "            'top_bssid': top_bssid,\n",
        "            'top_bssid_count': top_bssid_count,\n",
        "            'hour_span_minutes': hour_span\n",
        "        })\n",
        "\n",
        "    daily_stats = pd.DataFrame(features)\n",
        "\n",
        "    # í”¼ë²—\n",
        "    daily_stats = daily_stats.pivot(index=['subject_id', 'lifelog_date'], columns='time_period')\n",
        "    daily_stats.columns = [f\"{tp}_{metric}\" for metric, tp in daily_stats.columns]\n",
        "    daily_stats = daily_stats.reset_index()\n",
        "\n",
        "    return daily_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJujoyH8H9Cw"
      },
      "outputs": [],
      "source": [
        "mWifi2 = process_mWifi(mWifi,threshold=-60)\n",
        "\n",
        "# check\n",
        "print(f'\\n # mWifi2 shape: {mWifi2.shape}')\n",
        "mWifi2.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsAzWwCFYL2q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDYMQG6EYMVT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWKrkwFSiA3H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0TwqdaHiA6a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQkZX8zrb53H"
      },
      "source": [
        "### âœ”ï¸ wHr ì‹¬ë°•ë™ìˆ˜ (ìˆ˜ì •)\n",
        "- Heart rate readings recorded by the smartwatch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQTSKYiTiA-D"
      },
      "outputs": [],
      "source": [
        "def process_wHr(df):\n",
        "    heart_rate = df[\"heart_rate\"].values  # [[0, 1, 2, ...], ...]\n",
        "\n",
        "    def _process_feature(heart_rate):\n",
        "        if len(heart_rate) == 0:\n",
        "            return 0., 0., 0., 0., 0.\n",
        "\n",
        "        heart_rate = np.array(sum(map(lambda x: x.tolist(), heart_rate), []))\n",
        "        mean_hr = heart_rate.mean() if len(heart_rate) > 0 else 0\n",
        "        min_hr = heart_rate.min() if len(heart_rate) > 0 else 0\n",
        "        max_hr = heart_rate.max() if len(heart_rate) > 0 else 0\n",
        "        std_hr = heart_rate.std() if len(heart_rate) > 0 else 0\n",
        "        high_hr = heart_rate[heart_rate > 100].sum()\n",
        "\n",
        "        return mean_hr, min_hr, max_hr, std_hr, high_hr\n",
        "\n",
        "    # í•˜ë£¨\n",
        "    active_hour_mean_hr, active_hour_min_hr, active_hour_max_hr, active_hour_std_hr, active_hour_high_hr = _process_feature(heart_rate[df[\"hour\"].isin(ACTIVE_HOURS)])\n",
        "\n",
        "    # ì ìëŠ” ì‹œê°„ëŒ€\n",
        "    sleep_hour_mean_hr, sleep_hour_min_hr, sleep_hour_max_hr, sleep_hour_std_hr, sleep_hour_high_hr = _process_feature(heart_rate[df[\"hour\"].isin(SLEEP_HOURS)])\n",
        "\n",
        "    return pd.Series({\n",
        "        'active_hour_mean_hr': active_hour_mean_hr,\n",
        "        'active_hour_min_hr': active_hour_min_hr,\n",
        "        'active_hour_max_hr': active_hour_max_hr,\n",
        "        'active_hour_std_hr': active_hour_std_hr,\n",
        "        'active_hour_high_hr': active_hour_high_hr,\n",
        "        'sleep_hour_mean_hr': sleep_hour_mean_hr,\n",
        "        'sleep_hour_min_hr': sleep_hour_min_hr,\n",
        "        'sleep_hour_max_hr': sleep_hour_max_hr,\n",
        "        'sleep_hour_std_hr': sleep_hour_std_hr,\n",
        "        'sleep_hour_high_hr': sleep_hour_high_hr\n",
        "    })\n",
        "\n",
        "wHr_ori = load_data(DataType.wHr)\n",
        "wHr_ori = shift_lifelog_date(wHr_ori, target_hours=SLEEP_HOURS)\n",
        "\n",
        "wHr2 = (\n",
        "    wHr_ori\n",
        "    .groupby([\"subject_id\", \"lifelog_date\"], group_keys=False, as_index=False, sort=False, observed=True)\n",
        "    .apply(process_wHr)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "describe_df(wHr2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT7srO3MeuXD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0QjiWkCmPW6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koL6BKdTmPcY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRO-TD8_cEIH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYFlbAd_bYca"
      },
      "source": [
        "### âœ”ï¸ wLight ë¼ì´íŠ¸ (ìˆ˜ì •)\n",
        "- Ambient light measured by the smartwatch.  \n",
        "  - ì–´ë‘ìš´ ë°¤ 0.1 ~ 1 lux ìº„ìº„í•œ ë°©, ë‹¬ë¹› ì—†ëŠ” ë°¤\n",
        "  - ê°€ë¡œë“± ì¼œì§„ ê±°ë¦¬ 10 ~ 20 lux íë¦¿í•œ ì™¸ë¶€ ì¡°ëª…\n",
        "  - ì‹¤ë‚´ ì¡°ëª… 100 ~ 500 lux ì‚¬ë¬´ì‹¤, ì¼ë°˜ ê±°ì‹¤\n",
        "  - ë°ì€ ì‹¤ì™¸ 10,000 ~ 25,000 lux ë§‘ì€ ë‚  í–‡ë¹›\n",
        "  - ì§ì‚¬ê´‘ì„  ì•„ë˜ 30,000 ~ 100,000 lux ì—¬ë¦„ í•œë‚®, ë§¤ìš° ê°•í•œ í–‡ë¹›"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfAGaJQ0iBHP"
      },
      "outputs": [],
      "source": [
        "def process_mLight(df):\n",
        "    light = df[\"m_light\"].values  # [534.0, 224, ...]\n",
        "\n",
        "    def _process_feature(light):\n",
        "        if len(light) == 0:\n",
        "            return 0., 0., 0., 0., np.array([])\n",
        "\n",
        "        ligths = np.array(light)\n",
        "        mean_light = ligths.mean() if len(ligths) > 0 else 0\n",
        "        min_light = ligths.min() if len(ligths) > 0 else 0\n",
        "        max_light = ligths.max() if len(ligths) > 0 else 0\n",
        "        std_light = ligths.std() if len(ligths) > 0 else 0\n",
        "\n",
        "        return mean_light, min_light, max_light, std_light, ligths\n",
        "\n",
        "    # í•˜ë£¨\n",
        "    active_hour_mean_light, active_hour_min_light, active_hour_max_light, active_hour_std_light, _ = _process_feature(light[df[\"hour\"].isin(ACTIVE_HOURS)])\n",
        "\n",
        "    # ì ìëŠ” ì‹œê°„ëŒ€\n",
        "    sleep_hour_mean_light, sleep_hour_min_light, sleep_hour_max_light, sleep_hour_std_light, _= _process_feature(light[df[\"hour\"].isin(SLEEP_HOURS)])\n",
        "\n",
        "    # ì  ìëŸ¬ê°ˆ ë•Œ\n",
        "    might_go_to_sleep_light = light[df[\"hour\"].isin(MIGHT_GO_TO_SLEEP_HOURS)]\n",
        "    might_go_to_sleep_timestamps = df[\"timestamp\"].values[df[\"hour\"].isin(MIGHT_GO_TO_SLEEP_HOURS)]\n",
        "    _, _, _, _, might_go_to_sleep_lights = _process_feature(might_go_to_sleep_light)\n",
        "    first_sleep_timestamps = might_go_to_sleep_timestamps[(might_go_to_sleep_lights < 10.0)]\n",
        "    first_sleep_datetime = (\n",
        "        pd.to_datetime(first_sleep_timestamps[0]) if len(first_sleep_timestamps) > 0\n",
        "        else pd.to_datetime(might_go_to_sleep_timestamps[-1]) if len(might_go_to_sleep_timestamps) > 0\n",
        "        else pd.to_datetime(datetime(2024, 1, 1, MIGHT_GO_TO_SLEEP_HOURS[-1], 0, 0))  # default to the last hour of the range\n",
        "    )\n",
        "    first_sleep_minutes = (first_sleep_datetime.hour * 60 if first_sleep_datetime.hour > 12 else (first_sleep_datetime.hour + 24) * 60) + first_sleep_datetime.minute\n",
        "\n",
        "    # ì¼ì–´ë‚  ë•Œ\n",
        "    might_wakeup_light = light[df[\"hour\"].isin(MIGHT_WAKEUP_HOURS)]\n",
        "    might_wakeup_timestamps = df[\"timestamp\"].values[df[\"hour\"].isin(MIGHT_WAKEUP_HOURS)]\n",
        "    _, _, _, _, might_wakeup_lights = _process_feature(might_wakeup_light)\n",
        "    wakeup_timestamps = might_wakeup_timestamps[(might_wakeup_lights > 10.0)]\n",
        "    first_move_datetime = (\n",
        "        pd.to_datetime(wakeup_timestamps[0]) if len(wakeup_timestamps) > 0\n",
        "        else pd.to_datetime(might_wakeup_timestamps[-1]) if len(might_wakeup_timestamps) > 0\n",
        "        else pd.to_datetime(datetime(2024, 1, 1, MIGHT_WAKEUP_HOURS[-1], 0, 0))  # default to the last hour of the range\n",
        "    )\n",
        "    first_wakeup_minutes = (first_move_datetime.hour if first_move_datetime.hour > 12 else first_move_datetime.hour + 24) * 60 + first_move_datetime.minute\n",
        "\n",
        "    return pd.Series({\n",
        "        'active_hour_mean_light': active_hour_mean_light,\n",
        "        'active_hour_min_light': active_hour_min_light,\n",
        "        'active_hour_max_light': active_hour_max_light,\n",
        "        'active_hour_std_light': active_hour_std_light,\n",
        "        'sleep_hour_mean_light': sleep_hour_mean_light,\n",
        "        'sleep_hour_min_light': sleep_hour_min_light,\n",
        "        'sleep_hour_max_light': sleep_hour_max_light,\n",
        "        'sleep_hour_std_light': sleep_hour_std_light,\n",
        "        'mlight_first_sleep_minutes': first_sleep_minutes,\n",
        "        'mlight_first_wakeup_minutes': first_wakeup_minutes,\n",
        "    })\n",
        "\n",
        "wLight_ori = load_data(DataType.wLight)\n",
        "wLight_ori = shift_lifelog_date(wLight_ori, target_hours=SLEEP_HOURS)\n",
        "\n",
        "wLight2 = (\n",
        "    wLight_ori\n",
        "    .rename(columns={\"w_light\": \"m_light\"})\n",
        "    .groupby([\"subject_id\", \"lifelog_date\"], group_keys=False, as_index=False, sort=False, observed=True)\n",
        "    .apply(process_mLight)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "wLight2.rename(\n",
        "    columns={\n",
        "        col: \"w\" + col.replace(\"mlight_\", \"wlight_\")\n",
        "        for col in wLight.columns if col not in [\"subject_id\", \"lifelog_date\"]\n",
        "    }, inplace=True\n",
        ")\n",
        "\n",
        "describe_df(wLight2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAiZk_kWfR8m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcU6IavGfSAw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp0et2XoiBZ7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eX2gVvMzcF7h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQvyhHhBbmzN"
      },
      "source": [
        "### âœ”ï¸ wPedo ê±¸ìŒìˆ˜\n",
        "- Step data recorded by the smartwatch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3poFH3LmiBc2"
      },
      "outputs": [],
      "source": [
        "def process_mPedo(df):\n",
        "\n",
        "    def _process_feature(df):\n",
        "        if len(df) == 0:\n",
        "            return 0., 0., 0.\n",
        "\n",
        "        steps = df[\"step\"].values\n",
        "        distances = df[\"distance\"].values\n",
        "        calories = df[\"burned_calories\"].values\n",
        "\n",
        "        steps = steps.sum() if len(steps) > 0 else 0\n",
        "        distance = distances.sum() if len(distances) > 0 else 0\n",
        "        burned_calories = calories.sum() if len(calories) > 0 else 0\n",
        "\n",
        "        return steps, distance, burned_calories\n",
        "\n",
        "    # í•˜ë£¨\n",
        "    active_hour_steps, active_hour_distance, active_hour_burned_calories = _process_feature(df[df[\"hour\"].isin(ACTIVE_HOURS)])\n",
        "\n",
        "    # ì ìëŠ” ì‹œê°„ëŒ€\n",
        "    sleep_hour_steps, sleep_hour_distance, sleep_hour_burned_calories = _process_feature(df[df[\"hour\"].isin(SLEEP_HOURS)])\n",
        "\n",
        "    return pd.Series({\n",
        "        'active_hour_steps': active_hour_steps,\n",
        "        'active_hour_distance': active_hour_distance,\n",
        "        'active_hour_burned_calories': active_hour_burned_calories,\n",
        "        'sleep_hour_steps': sleep_hour_steps,\n",
        "        'sleep_hour_distance': sleep_hour_distance,\n",
        "        'sleep_hour_burned_calories': sleep_hour_burned_calories\n",
        "    })\n",
        "\n",
        "wPedo_ori = load_data(DataType.wPedo)\n",
        "wPedo_ori = shift_lifelog_date(wPedo_ori, target_hours=SLEEP_HOURS)\n",
        "\n",
        "wPedo2 = (\n",
        "    wPedo_ori\n",
        "    .groupby([\"subject_id\", \"lifelog_date\"], group_keys=False, as_index=False, sort=False, observed=True)\n",
        "    .apply(process_mPedo)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "describe_df(wPedo2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msyUxfoLgpq1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLPmg7oWhFMa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db44ae3XDtKE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L_sSEhnDtN5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zbaes4GnDtuy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahIQqHGXGSez"
      },
      "source": [
        "### âœ”ï¸ Sleeptime ì¼ì–´ë‚œ ê±´ìˆ˜\n",
        "\n",
        "- Sleeptimeì— (mLight ì£¼ë³€ ë°ê¸°), (wLight ì•°ë¹„ì–¸íŠ¸ ë¼ì´íŠ¸) ë³€í™” ê±´ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFvyBXsEGPXi"
      },
      "outputs": [],
      "source": [
        "def compute_night_awake_features(df, prefix):\n",
        "\n",
        "    df = df.copy()\n",
        "    df['lifelog_date'] = df['timestamp'].astype(str).str[:10]\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "    # 00ì‹œ~06ì‹œ í•„í„°\n",
        "    df['hour'] = df['timestamp'].dt.hour\n",
        "    df_night = df[(df['hour'] >= 0) & (df['hour'] < 6)].copy()\n",
        "\n",
        "    # ê¹¨ì–´ìˆëŠ” ë¶„ ê³„ì‚°\n",
        "    df_night['awake_minute'] = (df_night[prefix] > 0).astype(int)\n",
        "\n",
        "    # ê¹¨ì–´ë‚œ íšŸìˆ˜ ê³„ì‚° (0 â†’ ì–‘ìˆ˜ ì „í™˜)\n",
        "    def count_awake_blocks(x):\n",
        "        return ((x > 0) & (x.shift(fill_value=0) == 0)).sum()\n",
        "\n",
        "    # ê·¸ë£¹ë³„ ì§‘ê³„\n",
        "    result = df_night.groupby(['subject_id', 'lifelog_date']).agg(\n",
        "        awake_minutes=('awake_minute', 'sum'),\n",
        "        awake_blocks=(prefix, count_awake_blocks)\n",
        "    ).reset_index()\n",
        "\n",
        "    # ì»¬ëŸ¼ëª… ë³€ê²½\n",
        "    result = result.rename(columns={\n",
        "        'awake_minutes': f'{prefix}_awake_minutes',\n",
        "        'awake_blocks': f'{prefix}_awake_blocks'\n",
        "    })\n",
        "\n",
        "    # trainì— ê²°ê³¼ í•©ì¹˜ê¸° ìœ„í•´ì„œ -1 day í•˜ê¸°\n",
        "    result['lifelog_date'] = pd.to_datetime(result['lifelog_date'])\n",
        "    result['lifelog_date'] = result['lifelog_date'] + pd.Timedelta(days=-1)\n",
        "    result['lifelog_date'] = result['lifelog_date'].astype(str)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CpwIQM4GPbo"
      },
      "outputs": [],
      "source": [
        "mAwakeBlocks = compute_night_awake_features(mLight,'m_light')\n",
        "wAwakeBlocks = compute_night_awake_features(wLight,'w_light')\n",
        "AwakeBlocks = mAwakeBlocks.merge(wAwakeBlocks, on=['subject_id','lifelog_date'], how='outer')\n",
        "AwakeBlocks['light_awake_minutes'] = AwakeBlocks[['m_light_awake_minutes','w_light_awake_minutes']].max(axis=1)\n",
        "AwakeBlocks['light_awake_blocks'] = AwakeBlocks[['m_light_awake_blocks','w_light_awake_blocks']].max(axis=1)\n",
        "\n",
        "# check\n",
        "print(mAwakeBlocks.shape)\n",
        "print(wAwakeBlocks.shape)\n",
        "print(AwakeBlocks.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMlyCuPPDt5i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0ICeW88gttt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhcVIyeuguwP"
      },
      "source": [
        "### ğŸ“Œ merge ë°ì´í„°\n",
        "- train, test ê¸°ê°„ ì„œë¡œ ê²¹ì¹¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylxgRVjO0cPS"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/data/ch2025_metrics_train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/data/ch2025_submission_sample.csv')\n",
        "\n",
        "# ì¼ìë³€ìˆ˜ íƒ€ì… ë³€í™˜\n",
        "mACStatus2['lifelog_date'] = mACStatus2['lifelog_date'].astype(str)\n",
        "mActivity21['lifelog_date'] = mActivity21['lifelog_date'].astype(str)\n",
        "mActivity22['lifelog_date'] = mActivity22['lifelog_date'].astype(str)\n",
        "mAmbience2['lifelog_date'] = mAmbience2['lifelog_date'].astype(str)\n",
        "mBle2['lifelog_date'] = mBle2['lifelog_date'].astype(str)\n",
        "mGps2['lifelog_date'] = mGps2['lifelog_date'].astype(str)\n",
        "mLight21['lifelog_date'] = mLight21['lifelog_date'].astype(str)\n",
        "mLight22['lifelog_date'] = mLight22['lifelog_date'].astype(str)\n",
        "mLight23['lifelog_date'] = mLight23['lifelog_date'].astype(str)\n",
        "mScreenStatus2['lifelog_date'] = mScreenStatus2['lifelog_date'].astype(str)\n",
        "mUsageStats2['lifelog_date'] = mUsageStats2['lifelog_date'].astype(str)\n",
        "mWifi2['lifelog_date'] = mWifi2['lifelog_date'].astype(str)\n",
        "wHr2['lifelog_date'] = wHr2['lifelog_date'].astype(str)\n",
        "wLight2['lifelog_date'] = wLight2['lifelog_date'].astype(str)\n",
        "wPedo2['lifelog_date'] = wPedo2['lifelog_date'].astype(str)\n",
        "\n",
        "# ---- new ----\n",
        "\n",
        "AwakeBlocks['lifelog_date'] = AwakeBlocks['lifelog_date'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyqyR1HyKtYM"
      },
      "outputs": [],
      "source": [
        "df_list = [\n",
        "    mACStatus2,       # 1\n",
        "    mActivity21,       # 2\n",
        "    mActivity22,       # 2\n",
        "    mAmbience2,       # 3\n",
        "    mBle2,            # 4\n",
        "    mGps2,            # 5\n",
        "    mLight21,          # 6\n",
        "    mLight22,          # 6\n",
        "    mLight23,          # 6\n",
        "    mScreenStatus2,   # 7\n",
        "    mUsageStats2,     # 8\n",
        "    mWifi2,           # 9\n",
        "    wHr2,             # 10\n",
        "    wLight2,          # 11\n",
        "    wPedo2,           # 12\n",
        "    # ---- new ----\n",
        "    AwakeBlocks\n",
        "]\n",
        "\n",
        "data = reduce(lambda left, right: pd.merge(left, right, on=['subject_id', 'lifelog_date'], how='outer'), df_list)\n",
        "data['lifelog_date'] = data['lifelog_date'].astype(str)\n",
        "\n",
        "# ì¤‘ë³µì²´í¬\n",
        "print(data.shape)\n",
        "print(data[['subject_id','lifelog_date']].drop_duplicates().shape)\n",
        "\n",
        "# merge\n",
        "train2 = train.merge(data, on=['subject_id','lifelog_date'], how='left')\n",
        "test2 = test.merge(data, on=['subject_id','lifelog_date'], how='left')\n",
        "\n",
        "# ì €ì¥\n",
        "print('# train  shape:',train.shape)\n",
        "print('# train2 shape:',test2.shape)\n",
        "print('# test   shape:',test.shape)\n",
        "print('# test2  shape:',test2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJHeu7wQpSdL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JApa_DpNpSf8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFN_nDFO8bPE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO4m6vmGjlkC"
      },
      "source": [
        "## ğŸ“¦ í•™ìŠµ(ì „ì²˜ë¦¬ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4qv4t3ULZsR"
      },
      "outputs": [],
      "source": [
        "train = train2.copy()\n",
        "test = test2.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85lZxaHllP-B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DryVNJfxoP8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IStr2HXVgRd"
      },
      "source": [
        "### ğŸ”¥ ì´ë¯¸ì§€ íŒŒìƒë³€ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJ5plBR_Ve7k"
      },
      "outputs": [],
      "source": [
        "img_model = 'resnet50' # resnet50, xception, resnext101_32x32d efficientnet_b0 convnext_base convnext_tiny\n",
        "\n",
        "# ë°ì´í„° ì½ê¸°\n",
        "img_features = pd.read_csv(f'{PATH}/img_features_ch5_sleeptime_{img_model}_5.csv')\n",
        "img_features = img_features[sorted(img_features.columns,reverse=True)]\n",
        "img_features.columns = ['image_path']+['img'+i for i in img_features.columns if i not in ['image_path']]\n",
        "\n",
        "# ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œ\n",
        "img_features['subject_id'] = img_features['image_path'].str.extract(r'user(id\\d+)_')[0]\n",
        "img_features['lifelog_date'] = img_features['image_path'].str.extract(r'_(\\d{4}-\\d{2}-\\d{2})_')[0]\n",
        "\n",
        "# ì •ë ¬\n",
        "img_features = img_features.sort_values(['subject_id', 'lifelog_date'])\n",
        "\n",
        "# # lag\n",
        "# for lag in [1, 2]:\n",
        "#     img_features[f'img0_lag{lag}'] = img_features.groupby('subject_id')['img0'].shift(lag)\n",
        "#     img_features[f'img1_lag{lag}'] = img_features.groupby('subject_id')['img1'].shift(lag)\n",
        "\n",
        "# check\n",
        "img_features = img_features.drop(columns=['image_path'])\n",
        "print(len(img_features))\n",
        "display(img_features.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC-W7SFpVe_q"
      },
      "outputs": [],
      "source": [
        "# add img features\n",
        "train['lifelog_date'] = train['lifelog_date'].astype(str)\n",
        "test['lifelog_date'] = test['lifelog_date'].astype(str)\n",
        "train = train.merge(img_features,on=['subject_id','lifelog_date'],how='left')\n",
        "test = test.merge(img_features,on=['subject_id','lifelog_date'],how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptqByvx6K1Zz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "st8rfdHRIV6i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW3hjnwSIXVt"
      },
      "source": [
        "### âœ”ï¸ ì¶”ì •ìˆ˜ë©´íš¨ìœ¨\n",
        "- ì¶”ì •ìˆ˜ë©´íš¨ìœ¨ (S2) : (ë¶ˆëˆ ì‹œê°„ - í•¸ë“œí° ì´ìš©í•œ ë§ˆì§€ë§‰ ì‹œê°„) / ì¶”ì •ìˆ˜ë©´ì‹œê°„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_Q-BsnUIWIy"
      },
      "outputs": [],
      "source": [
        "def calculate_sleep_duration_min(sleep_time, wake_time):\n",
        "    \"\"\"\n",
        "    ì·¨ì¹¨ ì‹œê°(sleep_time)ê³¼ ê¸°ìƒ ì‹œê°(wake_time)ì„ ì…ë ¥ë°›ì•„ ìˆ˜ë©´ ì‹œê°„(ë¶„) ë°˜í™˜\n",
        "    ë‹¨ìœ„ëŠ” float ì‹œê°„ (ì˜ˆ: 23.5, 6.25)\n",
        "    \"\"\"\n",
        "    if pd.isna(sleep_time) or pd.isna(wake_time):\n",
        "        return None\n",
        "    if wake_time < sleep_time:\n",
        "        wake_time += 24  # ìì • ë„˜ê¸´ ê²½ìš° ë³´ì •\n",
        "    duration = (wake_time - sleep_time) * 60\n",
        "    return round(duration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0syM1G_IWMJ"
      },
      "outputs": [],
      "source": [
        "train['ë¶ˆëˆì‹œê°„ë¶€í„°ê¸°ìƒì‹œê°„'] = train.apply(lambda x: calculate_sleep_duration_min(x['lights_off_time'],x['wake_time']),axis=1)\n",
        "test['ë¶ˆëˆì‹œê°„ë¶€í„°ê¸°ìƒì‹œê°„'] = test.apply(lambda x: calculate_sleep_duration_min(x['lights_off_time'],x['wake_time']),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkjezDwrIWPd"
      },
      "outputs": [],
      "source": [
        "train['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'] = train['ë¶ˆëˆì‹œê°„ë¶€í„°ê¸°ìƒì‹œê°„']/train['sleep_duration_min']\n",
        "test['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'] = test['ë¶ˆëˆì‹œê°„ë¶€í„°ê¸°ìƒì‹œê°„']/test['sleep_duration_min']\n",
        "\n",
        "# ì´ìƒê°’ ì œê±°\n",
        "train['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'] = np.where(train['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨']<-5,np.nan,train['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'])\n",
        "test['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'] = np.where(test['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨']<-5,np.nan,test['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'])\n",
        "train['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'] = np.where(train['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨']>5,np.nan,train['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'])\n",
        "test['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'] = np.where(test['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨']>55,np.nan,test['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJEMl83rVwxq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYG8tHPsrogR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kQ_O5iAVpHH"
      },
      "source": [
        "### âœ”ï¸ ì¶”ê°€ íŒŒìƒë³€ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjG-oEFkDskF"
      },
      "outputs": [],
      "source": [
        "# sleep duration\n",
        "\n",
        "train['sleep_time_m_light_sleep_time'] = train['sleep_time'] - train['light_sleep_time']\n",
        "test['sleep_time_m_light_sleep_time'] = test['sleep_time'] - test['light_sleep_time']\n",
        "\n",
        "train['wake__time_m_light_wake__time'] = train['wake_time'] - train['light_wake_time']\n",
        "test['wake__time_m_light_wake__time'] = test['wake_time'] - test['light_wake_time']\n",
        "\n",
        "train['sleep_duration_min_m_light_sleep_duration_min'] = train['sleep_duration_min'] - train['light_sleep_duration_min']\n",
        "test['sleep_duration_min_m_light_sleep_duration_min'] = test['sleep_duration_min'] - test['light_sleep_duration_min']\n",
        "\n",
        "train['sleep_time_d_light_sleep_time'] = train['sleep_time'] / train['light_sleep_time']\n",
        "test['sleep_time_d_light_sleep_time'] = test['sleep_time'] / test['light_sleep_time']\n",
        "\n",
        "train['wake__time_d_light_wake__time'] = train['wake_time'] / train['light_wake_time']\n",
        "test['wake__time_d_light_wake__time'] = test['wake_time'] / test['light_wake_time']\n",
        "\n",
        "train['sleep_duration_min_d_light_sleep_duration_min'] = train['sleep_duration_min'] / train['light_sleep_duration_min']\n",
        "test['sleep_duration_min_d_light_sleep_duration_min'] = test['sleep_duration_min'] / test['light_sleep_duration_min']\n",
        "\n",
        "train['sleep_time_min'] = train[['sleep_time','light_sleep_time']].min(axis=1)\n",
        "train['sleep_time_max'] = train[['sleep_time','light_sleep_time']].max(axis=1)\n",
        "\n",
        "train['wake_time_min'] = train[['wake_time','light_wake_time']].min(axis=1)\n",
        "train['wake_time_max'] = train[['wake_time','light_wake_time']].max(axis=1)\n",
        "\n",
        "train['sleep_duration_min_min'] = train[['sleep_duration_min','light_sleep_duration_min']].min(axis=1)\n",
        "train['sleep_duration_min_max'] = train[['sleep_duration_min','light_sleep_duration_min']].max(axis=1)\n",
        "\n",
        "test['sleep_time_min'] = test[['sleep_time','light_sleep_time']].min(axis=1)\n",
        "test['sleep_time_max'] = test[['sleep_time','light_sleep_time']].max(axis=1)\n",
        "\n",
        "test['wake_time_min'] = test[['wake_time','light_wake_time']].min(axis=1)\n",
        "test['wake_time_max'] = test[['wake_time','light_wake_time']].max(axis=1)\n",
        "\n",
        "test['sleep_duration_min_min'] = test[['sleep_duration_min','light_sleep_duration_min']].min(axis=1)\n",
        "test['sleep_duration_min_max'] = test[['sleep_duration_min','light_sleep_duration_min']].max(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI616WgshZtT"
      },
      "outputs": [],
      "source": [
        "# ìš”ì¼ ì»¬ëŸ¼ ì¶”ê°€ (ì˜ˆ: ì›”ìš”ì¼, í™”ìš”ì¼, ...)\n",
        "train['lifelog_date'] = pd.to_datetime(train['lifelog_date'])\n",
        "test['lifelog_date'] = pd.to_datetime(test['lifelog_date'])\n",
        "\n",
        "# ìš”ì¼\n",
        "weekday_map = {\n",
        "    0: 'ì›”ìš”ì¼', 1: 'í™”ìš”ì¼', 2: 'ìˆ˜ìš”ì¼', 3: 'ëª©ìš”ì¼',\n",
        "    4: 'ê¸ˆìš”ì¼', 5: 'í† ìš”ì¼', 6: 'ì¼ìš”ì¼'\n",
        "}\n",
        "train['weekday'] = train['lifelog_date'].dt.dayofweek.map(weekday_map)\n",
        "test['weekday'] = test['lifelog_date'].dt.dayofweek.map(weekday_map)\n",
        "\n",
        "# ì›”\n",
        "train['month'] = train['lifelog_date'].dt.month\n",
        "test['month'] = test['lifelog_date'].dt.month\n",
        "\n",
        "# weekend\n",
        "train['weekend'] = np.where(train['weekday'].isin(['í† ìš”ì¼','ì¼ìš”ì¼']),1,0)\n",
        "test['weekend'] = np.where(test['weekday'].isin(['í† ìš”ì¼','ì¼ìš”ì¼']),1,0)\n",
        "\n",
        "# weekend\n",
        "train['weekend2'] = np.where(train['weekday'].isin(['í† ìš”ì¼','ê¸ˆìš”ì¼']),1,0)\n",
        "test['weekend2'] = np.where(test['weekday'].isin(['í† ìš”ì¼','ê¸ˆìš”ì¼']),1,0)\n",
        "\n",
        "# weekend\n",
        "train['weekend3'] = np.where(train['weekday'].isin(['í† ìš”ì¼','ê¸ˆìš”ì¼','ì¼ìš”ì¼']),1,0)\n",
        "test['weekend3'] = np.where(test['weekday'].isin(['í† ìš”ì¼','ê¸ˆìš”ì¼','ì¼ìš”ì¼']),1,0)\n",
        "\n",
        "# ê³µíœ´ì¼\n",
        "ê³µíœ´ì¼ = [\n",
        "     '2024-08-15'\n",
        "    ,'2024-09-16'\n",
        "    ,'2024-09-17'\n",
        "    ,'2024-09-18'\n",
        "    ,'2024-10-03'\n",
        "    ,'2024-10-09'\n",
        "]\n",
        "train['ê³µíœ´ì¼'] = np.where(train['lifelog_date'].isin(ê³µíœ´ì¼),1,0)\n",
        "test['ê³µíœ´ì¼'] = np.where(test['lifelog_date'].isin(ê³µíœ´ì¼),1,0)\n",
        "\n",
        "# ì£¼ë§ + ê³µíœ´ì¼ ë¬¶ì–´ì£¼ê¸°\n",
        "train['weekend_holilday'] = np.where( ((train['weekend']==0) & (train['ê³µíœ´ì¼']==1)), 1, train['weekend'])\n",
        "test['weekend_holilday'] = np.where( ((test['weekend']==0) & (test['ê³µíœ´ì¼']==1)), 1, test['weekend'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5zBdTUNJAWD"
      },
      "outputs": [],
      "source": [
        "def add_prev_day_flag(df):\n",
        "    df = df.copy()\n",
        "    df['lifelog_date'] = pd.to_datetime(df['lifelog_date'])\n",
        "\n",
        "    # ê° subject_idë³„ë¡œ ì „ë‚  ë‚ ì§œ ë§Œë“¤ê¸°\n",
        "    df['prev_day'] = df['lifelog_date'] - pd.Timedelta(days=1)\n",
        "\n",
        "    # subject_id + ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì›ë³¸ í‚¤ êµ¬ì„±\n",
        "    key_set = set(zip(df['subject_id'], df['lifelog_date']))\n",
        "\n",
        "    # ì „ë‚  ë°ì´í„°ê°€ ì¡´ì¬í•˜ë©´ 1, ì—†ìœ¼ë©´ 0\n",
        "    df['has_prev_day_data'] = df[['subject_id', 'prev_day']].apply(\n",
        "        lambda row: 1 if (row['subject_id'], row['prev_day']) in key_set else 0, axis=1\n",
        "    )\n",
        "\n",
        "    return df.drop(columns=['prev_day'])\n",
        "\n",
        "train = add_prev_day_flag(train)\n",
        "test = add_prev_day_flag(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHTfSdRPV-7p"
      },
      "outputs": [],
      "source": [
        "# ì¶”ì •íœ´ê°€\n",
        "def rule_based_sum(x):\n",
        "    rules = (\n",
        "        #  (x['sleep_duration_min'] > (x['avg_sleep_duration']))\n",
        "        #  (x['sleep_duration_min'] > (x['avg_sleep_duration']+60))\n",
        "          (x['sleep_duration_diff']>0)\n",
        "        & (x['week_type'] == 'weekday')\n",
        "        # & (x['month'].isin([7,8]))\n",
        "    )\n",
        "    return rules\n",
        "\n",
        "train['vacation'] = train.groupby('subject_id').apply(rule_based_sum).reset_index(level=0, drop=True).astype(int)\n",
        "test['vacation'] = test.groupby('subject_id').apply(rule_based_sum).reset_index(level=0, drop=True).astype(int)\n",
        "\n",
        "# check\n",
        "test.groupby(['subject_id'])['vacation'].sum().head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uNn-DwY620Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XpSKreip20JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“¦ ì €ì¥"
      ],
      "metadata": {
        "id": "xY4_Hcbp20eG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCKYiM1H0aGF"
      },
      "outputs": [],
      "source": [
        "# ì €ì¥\n",
        "train.to_parquet(f\"{PATH}/train_hjy_0603_v1.parquet\")\n",
        "test.to_parquet(f\"{PATH}/test_hjy_0603_v1.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8r1qszJndpcg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ABtAS_Tg2eB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8Cy2M4Xg2iI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmKky7O2wRSx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}