{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5e04b986",
      "metadata": {
        "id": "5e04b986"
      },
      "source": [
        "> title : 200_etri_lifelog_model_vF (Î™®Îç∏ ÌïôÏäµ ÏΩîÎìú) <br>\n",
        " -  ÏΩîÎìú Ïã§Ìñâ Ï†Ñ PATH Î≥ÄÍ≤ΩÌïòÏÑ∏Ïöî.\n",
        "  - PATH  =  '/content/drive/MyDrive/data/ch2025_data_items/share/submissions/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå set path"
      ],
      "metadata": {
        "id": "KD4cPBuEw9E9"
      },
      "id": "KD4cPBuEw9E9"
    },
    {
      "cell_type": "code",
      "source": [
        "PATH  =  '/content/drive/MyDrive/data/ch2025_data_items/share/submissions/input'"
      ],
      "metadata": {
        "id": "XFaSoyxswBnv"
      },
      "id": "XFaSoyxswBnv",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMR_erTYwCLg",
        "outputId": "06bd63e7-3bbd-4c62-bf2f-7e00c0d2131d"
      },
      "id": "bMR_erTYwCLg",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CYwEzMkzwDF4"
      },
      "id": "CYwEzMkzwDF4",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kSIndVOIxBQt"
      },
      "id": "kSIndVOIxBQt",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "21389cf1",
      "metadata": {
        "id": "21389cf1"
      },
      "source": [
        "### üìå libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install haversine >/dev/null\n",
        "! pip install category_encoders >/dev/null\n",
        "! pip install tabpfn  >/dev/null\n",
        "! pip install torchmetrics >/dev/null"
      ],
      "metadata": {
        "id": "fYTUHISTw7P_"
      },
      "id": "fYTUHISTw7P_",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "4f2a25a5",
      "metadata": {
        "id": "4f2a25a5"
      },
      "outputs": [],
      "source": [
        "# Í∏∞Î≥∏ Î™®Îìà\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import ast\n",
        "import glob\n",
        "import random\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from math import radians, cos, sin, asin, sqrt\n",
        "from functools import reduce\n",
        "from datetime import datetime, timedelta, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Î®∏Ïã†Îü¨Îãù\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import f1_score, roc_auc_score, roc_curve\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
        "from category_encoders import TargetEncoder\n",
        "from lightgbm import LGBMClassifier, log_evaluation, early_stopping\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from tabpfn import TabPFNClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "import shap\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Hugging Face\n",
        "from huggingface_hub import login\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    LlamaTokenizer,\n",
        "    LlamaForCausalLM,\n",
        "    LlamaForSequenceClassification\n",
        ")\n",
        "\n",
        "# PEFT (Parameter-Efficient Fine-Tuning)\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    TaskType\n",
        ")\n",
        "\n",
        "# Evaluation & Utilities\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "# Í∏∞ÌÉÄ\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import tqdm as auto_tqdm  # ÌïÑÏöî Ïãú Íµ¨Î∂Ñ\n",
        "from scipy.stats import entropy\n",
        "from haversine import haversine\n",
        "from io import StringIO\n",
        "import gc\n",
        "\n",
        "# ÏòµÏÖò\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "pd.set_option('display.max_columns', 999)\n",
        "pd.set_option('display.max_rows', 999)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.float_format', lambda x: '%0.4f' % x)\n",
        "\n",
        "# Í∏∞ÌÉÄ\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Í∏∞Î≥∏ ÏãúÎìú ÏÑ§Ï†ï\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Ïû¨ÌòÑÏÑ± Ìñ•ÏÉÅ ÏòµÏÖò (Îã®, ÏÑ±Îä• Ï†ÄÌïò Í∞ÄÎä•ÏÑ± ÏûàÏùå)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lPdbUAX21HoA"
      },
      "id": "lPdbUAX21HoA",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rmbwi1s31Hqz"
      },
      "id": "rmbwi1s31Hqz",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "hvECeS-vp4yh",
      "metadata": {
        "id": "hvECeS-vp4yh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mBV4hG-kdATV"
      },
      "id": "mBV4hG-kdATV",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå class : etri_lifelog_pipeline"
      ],
      "metadata": {
        "id": "O3ubt92s7gVq"
      },
      "id": "O3ubt92s7gVq"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "39d79d1f",
      "metadata": {
        "id": "39d79d1f"
      },
      "outputs": [],
      "source": [
        "class etri_lifelog_pipeline:\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    self.valid_id1 = \"\"\"\n",
        "    subject_id,sleep_date\n",
        "    id01,2024-07-24\n",
        "    id01,2024-08-26\n",
        "    id01,2024-08-28\n",
        "    id01,2024-08-29\n",
        "    id02,2024-08-23\n",
        "    id02,2024-09-24\n",
        "    id02,2024-09-26\n",
        "    id02,2024-09-27\n",
        "    id03,2024-08-30\n",
        "    id03,2024-09-01\n",
        "    id03,2024-09-02\n",
        "    id03,2024-09-06\n",
        "    id04,2024-09-03\n",
        "    id04,2024-10-10\n",
        "    id04,2024-10-12\n",
        "    id04,2024-10-13\n",
        "    id05,2024-10-19\n",
        "    id05,2024-10-23\n",
        "    id05,2024-10-24\n",
        "    id05,2024-10-27\n",
        "    id06,2024-07-25\n",
        "    id06,2024-07-26\n",
        "    id06,2024-07-27\n",
        "    id06,2024-07-30\n",
        "    id07,2024-07-07\n",
        "    id07,2024-08-02\n",
        "    id07,2024-08-04\n",
        "    id07,2024-08-05\n",
        "    id08,2024-08-28\n",
        "    id08,2024-08-29\n",
        "    id08,2024-08-30\n",
        "    id08,2024-09-02\n",
        "    id09,2024-08-02\n",
        "    id09,2024-08-31\n",
        "    id09,2024-09-02\n",
        "    id09,2024-09-03\n",
        "    id10,2024-08-28\n",
        "    id10,2024-08-30\n",
        "    id10,2024-08-31\n",
        "    id10,2024-09-03\n",
        "    \"\"\"\n",
        "\n",
        "    self.valid_id2 = \"\"\"\n",
        "    subject_id,sleep_date\n",
        "    id01,2024-07-24\n",
        "    id01,2024-07-27\n",
        "    id01,2024-08-18\n",
        "    id01,2024-08-19\n",
        "    id01,2024-08-20\n",
        "    id01,2024-08-21\n",
        "    id01,2024-08-22\n",
        "    id01,2024-08-24\n",
        "    id01,2024-08-25\n",
        "    id01,2024-08-26\n",
        "    id01,2024-08-27\n",
        "    id01,2024-08-28\n",
        "    id01,2024-08-29\n",
        "    id01,2024-08-30\n",
        "    id02,2024-08-23\n",
        "    id02,2024-08-24\n",
        "    id02,2024-09-16\n",
        "    id02,2024-09-17\n",
        "    id02,2024-09-19\n",
        "    id02,2024-09-20\n",
        "    id02,2024-09-21\n",
        "    id02,2024-09-22\n",
        "    id02,2024-09-23\n",
        "    id02,2024-09-24\n",
        "    id02,2024-09-25\n",
        "    id02,2024-09-26\n",
        "    id02,2024-09-27\n",
        "    id02,2024-09-28\n",
        "    id03,2024-08-30\n",
        "    id03,2024-09-01\n",
        "    id03,2024-09-02\n",
        "    id03,2024-09-03\n",
        "    id03,2024-09-05\n",
        "    id03,2024-09-06\n",
        "    id03,2024-09-07\n",
        "    id04,2024-09-03\n",
        "    id04,2024-09-04\n",
        "    id04,2024-09-05\n",
        "    id04,2024-09-06\n",
        "    id04,2024-09-07\n",
        "    id04,2024-09-08\n",
        "    id04,2024-09-09\n",
        "    id04,2024-10-08\n",
        "    id04,2024-10-09\n",
        "    id04,2024-10-10\n",
        "    id04,2024-10-11\n",
        "    id04,2024-10-12\n",
        "    id04,2024-10-13\n",
        "    id04,2024-10-14\n",
        "    id05,2024-10-19\n",
        "    id05,2024-10-23\n",
        "    id05,2024-10-24\n",
        "    id05,2024-10-25\n",
        "    id05,2024-10-26\n",
        "    id05,2024-10-27\n",
        "    id05,2024-10-28\n",
        "    id06,2024-07-25\n",
        "    id06,2024-07-26\n",
        "    id06,2024-07-27\n",
        "    id06,2024-07-28\n",
        "    id06,2024-07-29\n",
        "    id06,2024-07-30\n",
        "    id06,2024-07-31\n",
        "    id07,2024-07-07\n",
        "    id07,2024-07-08\n",
        "    id07,2024-07-09\n",
        "    id07,2024-07-10\n",
        "    id07,2024-07-11\n",
        "    id07,2024-07-12\n",
        "    id07,2024-07-13\n",
        "    id07,2024-07-30\n",
        "    id07,2024-08-01\n",
        "    id07,2024-08-02\n",
        "    id07,2024-08-03\n",
        "    id07,2024-08-04\n",
        "    id07,2024-08-05\n",
        "    id07,2024-08-06\n",
        "    id08,2024-08-28\n",
        "    id08,2024-08-29\n",
        "    id08,2024-08-30\n",
        "    id08,2024-08-31\n",
        "    id08,2024-09-01\n",
        "    id08,2024-09-02\n",
        "    id08,2024-09-04\n",
        "    id09,2024-08-02\n",
        "    id09,2024-08-22\n",
        "    id09,2024-08-23\n",
        "    id09,2024-08-24\n",
        "    id09,2024-08-25\n",
        "    id09,2024-08-27\n",
        "    id09,2024-08-28\n",
        "    id09,2024-08-29\n",
        "    id09,2024-08-30\n",
        "    id09,2024-08-31\n",
        "    id09,2024-09-01\n",
        "    id09,2024-09-02\n",
        "    id09,2024-09-03\n",
        "    id09,2024-09-04\n",
        "    id10,2024-08-28\n",
        "    id10,2024-08-30\n",
        "    id10,2024-08-31\n",
        "    id10,2024-09-01\n",
        "    id10,2024-09-02\n",
        "    id10,2024-09-03\n",
        "    id10,2024-09-06\n",
        "    \"\"\"\n",
        "\n",
        "    self.common_params = {\n",
        "      'n_estimators': 5000, # 5000\n",
        "      \"learning_rate\": 0.01, # 0.01\n",
        "      'lambda_l1': 5, # 5\n",
        "      'lambda_l2': 1, # 1\n",
        "      'bagging_fraction': 0.8, # 0.8\n",
        "      'feature_fraction': 1,\n",
        "      'n_jobs': -1,\n",
        "      'verbosity': -1\n",
        "    }\n",
        "\n",
        "    self.lgb_params = {\n",
        "      'n_estimators': 5000, # 5000\n",
        "      \"learning_rate\": 0.01, # 0.01\n",
        "      'lambda_l1': 5, # 5\n",
        "      'lambda_l2': 1, # 1\n",
        "      'bagging_fraction': 0.8, # 0.8\n",
        "      'feature_fraction': 1,\n",
        "      'n_jobs': -1,\n",
        "      'verbosity': -1\n",
        "    }\n",
        "\n",
        "    self.xgb_params = {\n",
        "        'n_estimators': 1000,\n",
        "        'learning_rate': 0.01,\n",
        "        'max_depth': 6,\n",
        "        'min_child_weight': 1, # 1\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'random_state': seed\n",
        "    }\n",
        "\n",
        "    self.xgb_params_S1 = {\n",
        "        'n_estimators': 1000,\n",
        "        'learning_rate': 0.01,\n",
        "        'max_depth': 6,\n",
        "        'min_child_weight': 5, # 1\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'random_state': seed\n",
        "    }\n",
        "\n",
        "    self.tabpfn_params = {\n",
        "        'device': 'cuda',  # GPU ÏÇ¨Ïö©\n",
        "    }\n",
        "\n",
        "\n",
        "  def read_originaldata(self):\n",
        "\n",
        "    # [1]dataset version1\n",
        "    train1 = pd.read_parquet(f'{PATH}/train_63775_vF.parquet')\n",
        "    test1 = pd.read_parquet(f'{PATH}/test_63775_vF.parquet')\n",
        "\n",
        "    # [2]dataset version2\n",
        "    train2 = pd.read_parquet(f'{PATH}/train_hjy_0603_v1.parquet')\n",
        "    test2 = pd.read_parquet(f'{PATH}/test_hjy_0603_v1.parquet')\n",
        "\n",
        "    # [1]+[2]\n",
        "    a1 = train1.columns.tolist()\n",
        "    a2 = train2.columns.tolist()\n",
        "    feats = ['subject_id','sleep_date','lifelog_date']+list(set(a2)-set(a1))\n",
        "    train2 = train2[feats].copy()\n",
        "    test2 = test2[feats].copy()\n",
        "    train = train1.merge(train2,on=['subject_id','sleep_date','lifelog_date'],how='left')\n",
        "    test = test1.merge(test2,on=['subject_id','sleep_date','lifelog_date'],how='left')\n",
        "\n",
        "    return train, test\n",
        "\n",
        "  def read_meanimputedata(self):\n",
        "\n",
        "    # [1]dataset version1\n",
        "    train1 = pd.read_parquet(f'{PATH}/train_63775_vF.parquet')\n",
        "    test1 = pd.read_parquet(f'{PATH}/test_63775_vF.parquet')\n",
        "\n",
        "    # [2]dataset version2\n",
        "    train2 = pd.read_parquet(f'{PATH}/train_hjy_0603_v1.parquet')\n",
        "    test2 = pd.read_parquet(f'{PATH}/test_hjy_0603_v1.parquet')\n",
        "\n",
        "    # [1]+[2]\n",
        "    a1 = train1.columns.tolist()\n",
        "    a2 = train2.columns.tolist()\n",
        "    feats = ['subject_id','sleep_date','lifelog_date']+list(set(a2)-set(a1))\n",
        "    train2 = train2[feats].copy()\n",
        "    test2 = test2[feats].copy()\n",
        "    train = train1.merge(train2,on=['subject_id','sleep_date','lifelog_date'],how='left')\n",
        "    test = test1.merge(test2,on=['subject_id','sleep_date','lifelog_date'],how='left')\n",
        "\n",
        "    # experiment dataset\n",
        "    feats = [\n",
        "        'sleep_time',        'wake_time',        'sleep_duration_min',  'avg_sleep_time', 'avg_wake_time', 'avg_sleep_duration', 'sleep_time_diff', 'wake_time_diff', 'sleep_duration_diff', 'sleep_time_ratio', 'wake_time_ratio', 'sleep_duration_ratio',\n",
        "        'sleep_time_lag1',   'wake_time_lag1',   'sleep_duration_lag1', 'sleep_time_diff_lag1', 'wake_time_diff_lag1', 'sleep_duration_diff_lag1', 'sleep_time_ratio_lag1', 'wake_time_ratio_lag1', 'sleep_duration_ratio_lag1',\n",
        "        'sleep_time_lag2',   'wake_time_lag2',   'sleep_duration_lag2', 'sleep_time_diff_lag2', 'wake_time_diff_lag2', 'sleep_duration_diff_lag2', 'sleep_time_ratio_lag2', 'wake_time_ratio_lag2', 'sleep_duration_ratio_lag2', 'sleep_time_mean2d', 'wake_time_mean2d', 'sleep_duration_min_mean2d', 'sleep_time_diff_mean2d', 'wake_time_diff_mean2d', 'sleep_duration_diff_mean2d', 'sleep_time_ratio_mean2d', 'wake_time_ratio_mean2d', 'sleep_duration_ratio_mean2d', 'sleep_time_std2d', 'wake_time_std2d', 'sleep_duration_min_std2d', 'sleep_time_diff_std2d', 'wake_time_diff_std2d', 'sleep_duration_diff_std2d', 'sleep_time_ratio_std2d', 'wake_time_ratio_std2d', 'sleep_duration_ratio_std2d',\n",
        "        'sleep_time_mean3d', 'wake_time_mean3d', 'sleep_duration_min_mean3d', 'sleep_time_diff_mean3d', 'wake_time_diff_mean3d', 'sleep_duration_diff_mean3d', 'sleep_time_ratio_mean3d', 'wake_time_ratio_mean3d', 'sleep_duration_ratio_mean3d', 'sleep_time_std3d', 'wake_time_std3d', 'sleep_duration_min_std3d', 'sleep_time_diff_std3d', 'wake_time_diff_std3d', 'sleep_duration_diff_std3d', 'sleep_time_ratio_std3d', 'wake_time_ratio_std3d', 'sleep_duration_ratio_std3d',\n",
        "        #'sleep_time_mean5d', 'wake_time_mean5d', 'sleep_duration_min_mean5d', 'sleep_time_diff_mean5d', 'wake_time_diff_mean5d', 'sleep_duration_diff_mean5d', 'sleep_time_ratio_mean5d', 'wake_time_ratio_mean5d', 'sleep_duration_ratio_mean5d', 'sleep_time_std5d', 'wake_time_std5d', 'sleep_duration_min_std5d', 'sleep_time_diff_std5d', 'wake_time_diff_std5d', 'sleep_duration_diff_std5d', 'sleep_time_ratio_std5d', 'wake_time_ratio_std5d', 'sleep_duration_ratio_std5d',\n",
        "        #'sleep_time_mean7d', 'wake_time_mean7d', 'sleep_duration_min_mean7d', 'sleep_time_diff_mean7d', 'wake_time_diff_mean7d', 'sleep_duration_diff_mean7d', 'sleep_time_ratio_mean7d', 'wake_time_ratio_mean7d', 'sleep_duration_ratio_mean7d', 'sleep_time_std7d', 'wake_time_std7d', 'sleep_duration_min_std7d', 'sleep_time_diff_std7d', 'wake_time_diff_std7d', 'sleep_duration_diff_std7d', 'sleep_time_ratio_std7d', 'wake_time_ratio_std7d', 'sleep_duration_ratio_std7d',\n",
        "        'weekday_avg_sleep', 'sleep_duration_weekday_avg_diff', 'sleep_duration_weekday_avg_div'\n",
        "    ]\n",
        "    for feat in feats:\n",
        "      a1_dict = train.groupby(['subject_id'])[feat].mean().to_dict()\n",
        "      train[feat] = train[feat].fillna(train['subject_id'].map(a1_dict))\n",
        "      test[feat] = test[feat].fillna(test['subject_id'].map(a1_dict))\n",
        "\n",
        "    return train, test\n",
        "\n",
        "  def read_knnimputedata(self):\n",
        "\n",
        "    # [1]dataset version1\n",
        "    train1 = pd.read_parquet(f'{PATH}/train_63775_vF.parquet')\n",
        "    test1 = pd.read_parquet(f'{PATH}/test_63775_vF.parquet')\n",
        "\n",
        "    # [2]dataset version2\n",
        "    train2 = pd.read_parquet(f'{PATH}/train_hjy_0603_v1.parquet')\n",
        "    test2 = pd.read_parquet(f'{PATH}/test_hjy_0603_v1.parquet')\n",
        "\n",
        "    # [1]+[2]\n",
        "    a1 = train1.columns.tolist()\n",
        "    a2 = train2.columns.tolist()\n",
        "    feats = ['subject_id','sleep_date','lifelog_date']+list(set(a2)-set(a1))\n",
        "    train2 = train2[feats].copy()\n",
        "    test2 = test2[feats].copy()\n",
        "    train = train1.merge(train2,on=['subject_id','sleep_date','lifelog_date'],how='left')\n",
        "    test = test1.merge(test2,on=['subject_id','sleep_date','lifelog_date'],how='left')\n",
        "\n",
        "    # experiment dataset\n",
        "    feats = [\n",
        "        'sleep_time',        'wake_time',        'sleep_duration_min',  'avg_sleep_time', 'avg_wake_time', 'avg_sleep_duration', 'sleep_time_diff', 'wake_time_diff', 'sleep_duration_diff', 'sleep_time_ratio', 'wake_time_ratio', 'sleep_duration_ratio',\n",
        "        'sleep_time_lag1',   'wake_time_lag1',   'sleep_duration_lag1', 'sleep_time_diff_lag1', 'wake_time_diff_lag1', 'sleep_duration_diff_lag1', 'sleep_time_ratio_lag1', 'wake_time_ratio_lag1', 'sleep_duration_ratio_lag1',\n",
        "        'sleep_time_lag2',   'wake_time_lag2',   'sleep_duration_lag2', 'sleep_time_diff_lag2', 'wake_time_diff_lag2', 'sleep_duration_diff_lag2', 'sleep_time_ratio_lag2', 'wake_time_ratio_lag2', 'sleep_duration_ratio_lag2', 'sleep_time_mean2d', 'wake_time_mean2d', 'sleep_duration_min_mean2d', 'sleep_time_diff_mean2d', 'wake_time_diff_mean2d', 'sleep_duration_diff_mean2d', 'sleep_time_ratio_mean2d', 'wake_time_ratio_mean2d', 'sleep_duration_ratio_mean2d', 'sleep_time_std2d', 'wake_time_std2d', 'sleep_duration_min_std2d', 'sleep_time_diff_std2d', 'wake_time_diff_std2d', 'sleep_duration_diff_std2d', 'sleep_time_ratio_std2d', 'wake_time_ratio_std2d', 'sleep_duration_ratio_std2d',\n",
        "        'sleep_time_mean3d', 'wake_time_mean3d', 'sleep_duration_min_mean3d', 'sleep_time_diff_mean3d', 'wake_time_diff_mean3d', 'sleep_duration_diff_mean3d', 'sleep_time_ratio_mean3d', 'wake_time_ratio_mean3d', 'sleep_duration_ratio_mean3d', 'sleep_time_std3d', 'wake_time_std3d', 'sleep_duration_min_std3d', 'sleep_time_diff_std3d', 'wake_time_diff_std3d', 'sleep_duration_diff_std3d', 'sleep_time_ratio_std3d', 'wake_time_ratio_std3d', 'sleep_duration_ratio_std3d',\n",
        "        #'sleep_time_mean5d', 'wake_time_mean5d', 'sleep_duration_min_mean5d', 'sleep_time_diff_mean5d', 'wake_time_diff_mean5d', 'sleep_duration_diff_mean5d', 'sleep_time_ratio_mean5d', 'wake_time_ratio_mean5d', 'sleep_duration_ratio_mean5d', 'sleep_time_std5d', 'wake_time_std5d', 'sleep_duration_min_std5d', 'sleep_time_diff_std5d', 'wake_time_diff_std5d', 'sleep_duration_diff_std5d', 'sleep_time_ratio_std5d', 'wake_time_ratio_std5d', 'sleep_duration_ratio_std5d',\n",
        "        #'sleep_time_mean7d', 'wake_time_mean7d', 'sleep_duration_min_mean7d', 'sleep_time_diff_mean7d', 'wake_time_diff_mean7d', 'sleep_duration_diff_mean7d', 'sleep_time_ratio_mean7d', 'wake_time_ratio_mean7d', 'sleep_duration_ratio_mean7d', 'sleep_time_std7d', 'wake_time_std7d', 'sleep_duration_min_std7d', 'sleep_time_diff_std7d', 'wake_time_diff_std7d', 'sleep_duration_diff_std7d', 'sleep_time_ratio_std7d', 'wake_time_ratio_std7d', 'sleep_duration_ratio_std7d',\n",
        "        'weekday_avg_sleep', 'sleep_duration_weekday_avg_diff', 'sleep_duration_weekday_avg_div'\n",
        "    ]\n",
        "    imputer = KNNImputer(n_neighbors=5)\n",
        "    imputer.fit(train[feats])\n",
        "    train[feats] = imputer.transform(train[feats])\n",
        "    test[feats] = imputer.transform(test[feats])\n",
        "\n",
        "    return train, test\n",
        "\n",
        "  def read_llmimputedata(self):\n",
        "\n",
        "    # [1]dataset version1\n",
        "    train1 = pd.read_parquet(f'{PATH}/train_63775_vF.parquet')\n",
        "    test1 = pd.read_parquet(f'{PATH}/test_63775_vF.parquet')\n",
        "\n",
        "    # [2]dataset version2\n",
        "    train2 = pd.read_parquet(f'{PATH}/train_hjy_0603_v1.parquet')\n",
        "    test2 = pd.read_parquet(f'{PATH}/test_hjy_0603_v1.parquet')\n",
        "\n",
        "    # [1]+[2]\n",
        "    a1 = train1.columns.tolist()\n",
        "    a2 = train2.columns.tolist()\n",
        "    feats = ['subject_id','sleep_date','lifelog_date']+list(set(a2)-set(a1))\n",
        "    train2 = train2[feats].copy()\n",
        "    test2 = test2[feats].copy()\n",
        "    train = train1.merge(train2,on=['subject_id','sleep_date','lifelog_date'],how='left')\n",
        "    test = test1.merge(test2,on=['subject_id','sleep_date','lifelog_date'],how='left')\n",
        "\n",
        "    # [3]QWEN3 8B ÌôúÏö©Ìïú Í≤∞Ï∏°Ï≤òÎ¶¨ (ÎåÄÏÉÅ: mScreenStatus)\n",
        "    mScreenStatus_llm = pd.read_excel(f'{PATH}/mScreenStatus_llmÍ≤∞Ï∏°Í∞íÏÉùÏÑ±ÌõÑÌååÏÉùÎ≥ÄÏàòÏÉùÏÑ±_20250609_v1.xlsx')\n",
        "    feats = [\n",
        "        'sleep_time',        'wake_time',        'sleep_duration_min',  'avg_sleep_time', 'avg_wake_time', 'avg_sleep_duration', 'sleep_time_diff', 'wake_time_diff', 'sleep_duration_diff', 'sleep_time_ratio', 'wake_time_ratio', 'sleep_duration_ratio',\n",
        "        'sleep_time_lag1',   'wake_time_lag1',   'sleep_duration_lag1', 'sleep_time_diff_lag1', 'wake_time_diff_lag1', 'sleep_duration_diff_lag1', 'sleep_time_ratio_lag1', 'wake_time_ratio_lag1', 'sleep_duration_ratio_lag1',\n",
        "        'sleep_time_lag2',   'wake_time_lag2',   'sleep_duration_lag2', 'sleep_time_diff_lag2', 'wake_time_diff_lag2', 'sleep_duration_diff_lag2', 'sleep_time_ratio_lag2', 'wake_time_ratio_lag2', 'sleep_duration_ratio_lag2', 'sleep_time_mean2d', 'wake_time_mean2d', 'sleep_duration_min_mean2d', 'sleep_time_diff_mean2d', 'wake_time_diff_mean2d', 'sleep_duration_diff_mean2d', 'sleep_time_ratio_mean2d', 'wake_time_ratio_mean2d', 'sleep_duration_ratio_mean2d', 'sleep_time_std2d', 'wake_time_std2d', 'sleep_duration_min_std2d', 'sleep_time_diff_std2d', 'wake_time_diff_std2d', 'sleep_duration_diff_std2d', 'sleep_time_ratio_std2d', 'wake_time_ratio_std2d', 'sleep_duration_ratio_std2d',\n",
        "        'sleep_time_mean3d', 'wake_time_mean3d', 'sleep_duration_min_mean3d', 'sleep_time_diff_mean3d', 'wake_time_diff_mean3d', 'sleep_duration_diff_mean3d', 'sleep_time_ratio_mean3d', 'wake_time_ratio_mean3d', 'sleep_duration_ratio_mean3d', 'sleep_time_std3d', 'wake_time_std3d', 'sleep_duration_min_std3d', 'sleep_time_diff_std3d', 'wake_time_diff_std3d', 'sleep_duration_diff_std3d', 'sleep_time_ratio_std3d', 'wake_time_ratio_std3d', 'sleep_duration_ratio_std3d',\n",
        "        #'sleep_time_mean5d', 'wake_time_mean5d', 'sleep_duration_min_mean5d', 'sleep_time_diff_mean5d', 'wake_time_diff_mean5d', 'sleep_duration_diff_mean5d', 'sleep_time_ratio_mean5d', 'wake_time_ratio_mean5d', 'sleep_duration_ratio_mean5d', 'sleep_time_std5d', 'wake_time_std5d', 'sleep_duration_min_std5d', 'sleep_time_diff_std5d', 'wake_time_diff_std5d', 'sleep_duration_diff_std5d', 'sleep_time_ratio_std5d', 'wake_time_ratio_std5d', 'sleep_duration_ratio_std5d',\n",
        "        #'sleep_time_mean7d', 'wake_time_mean7d', 'sleep_duration_min_mean7d', 'sleep_time_diff_mean7d', 'wake_time_diff_mean7d', 'sleep_duration_diff_mean7d', 'sleep_time_ratio_mean7d', 'wake_time_ratio_mean7d', 'sleep_duration_ratio_mean7d', 'sleep_time_std7d', 'wake_time_std7d', 'sleep_duration_min_std7d', 'sleep_time_diff_std7d', 'wake_time_diff_std7d', 'sleep_duration_diff_std7d', 'sleep_time_ratio_std7d', 'wake_time_ratio_std7d', 'sleep_duration_ratio_std7d',\n",
        "        'weekday_avg_sleep', 'sleep_duration_weekday_avg_diff', 'sleep_duration_weekday_avg_div'\n",
        "    ]\n",
        "    drop_features = [i for i in feats if i in train.columns]\n",
        "    train = train.drop(columns=drop_features)\n",
        "    train = train.merge(mScreenStatus_llm,on=['subject_id','lifelog_date'],how='left')\n",
        "    test = test.drop(columns=drop_features)\n",
        "    test = test.merge(mScreenStatus_llm,on=['subject_id','lifelog_date'],how='left')\n",
        "\n",
        "    return train, test\n",
        "\n",
        "  def preprocess(self, train, test):\n",
        "\n",
        "    drop_featurs = [\n",
        "     'Unnamed: 0'\n",
        "    ,'light_week_type_lag1'\n",
        "    ,'week_type'\n",
        "    ,'week_type_lag1'\n",
        "    ,'activehour_top_bssid'\n",
        "    ,'beforebed_top_bssid'\n",
        "    ]\n",
        "    drop_featurs = [i for i in drop_featurs if i in train.columns]\n",
        "    train = train.drop(columns=drop_featurs)\n",
        "    test = test.drop(columns=drop_featurs)\n",
        "\n",
        "    # weekend\n",
        "    train['weekend'] = np.where(train['weekday'].isin(['ÌÜ†ÏöîÏùº','Í∏àÏöîÏùº']),1,0)\n",
        "    test['weekend'] = np.where(test['weekday'].isin(['ÌÜ†ÏöîÏùº','Í∏àÏöîÏùº']),1,0)\n",
        "\n",
        "    # REPLACE MISSING VALUES (COMMON)\n",
        "    train[train.select_dtypes(include='number').columns] = train.select_dtypes(include='number').fillna(-1)\n",
        "    test[test.select_dtypes(include='number').columns] = test.select_dtypes(include='number').fillna(-1)\n",
        "\n",
        "    # drop_featurs\n",
        "    drop_featurs = ['light_month']\n",
        "    drop_featurs = [i for i in drop_featurs if i in train.columns]\n",
        "    train = train.drop(columns=drop_featurs)\n",
        "    test = test.drop(columns=drop_featurs)\n",
        "\n",
        "    return train, test\n",
        "\n",
        "  def run_basemodel(self, train, test, valid_ids, common_params, n_splits, random_state, S1_balance):\n",
        "\n",
        "      lgb_A = 0.4\n",
        "      xgb_B = 0.3\n",
        "      tab_C = 0.3 ###\n",
        "      print(f'# lgb_A:{lgb_A} xgb_B:{xgb_B} tab_C:{tab_C}')\n",
        "\n",
        "      lgb_params = common_params['Q1'].copy()\n",
        "      lgb_params['random_state'] = random_state\n",
        "\n",
        "      train_df = train.copy()\n",
        "      test_df = test.copy()\n",
        "\n",
        "      submission_final = test_df[['subject_id', 'sleep_date', 'lifelog_date']].copy()\n",
        "      submission_final['lifelog_date'] = pd.to_datetime(submission_final['lifelog_date']).dt.date\n",
        "\n",
        "      # TARGET\n",
        "      targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3']\n",
        "      targets_binary_name = ['Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà','Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú','Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§','ÏàòÎ©¥Ìö®Ïú®','ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ']\n",
        "      target_multiclass = 'S1'\n",
        "      all_targets = targets_binary + [target_multiclass]\n",
        "\n",
        "      # add_noise\n",
        "      def add_noise(series, noise_level, seed=3):\n",
        "          rng = np.random.default_rng(seed)\n",
        "          return series * (1 + noise_level * rng.standard_normal(len(series)))\n",
        "\n",
        "      noise_level = 0.015\n",
        "\n",
        "      # ----------------------------------------------------------------------------------------------------------\n",
        "\n",
        "      # predweekday ÏÉùÏÑ±: subject_idÎ≥ÑÎ°ú sleep_duration_minÏù¥ Í∏¥ ÏöîÏùº 2Í∞úÎ•º ÏÑ†Ï†ï\n",
        "      top2_days = (\n",
        "          train_df.groupby(['subject_id', 'weekday'])['sleep_duration_min']\n",
        "          .mean()\n",
        "          .reset_index()\n",
        "          .sort_values(['subject_id', 'sleep_duration_min'], ascending=[True, False])\n",
        "          .groupby('subject_id')\n",
        "          .head(1)\n",
        "      )\n",
        "\n",
        "      # subject_idÎ≥Ñ top2 weekday ÏßëÌï© ÎßåÎì§Í∏∞\n",
        "      top2_day_dict = top2_days.groupby('subject_id')['weekday'].apply(set).to_dict()\n",
        "\n",
        "      # train_df, test_dfÏóê predweekday Ïª¨Îüº Ï∂îÍ∞Ä\n",
        "      def mark_predweekday(row):\n",
        "          return int(row['weekday'] in top2_day_dict.get(row['subject_id'], set()))\n",
        "\n",
        "      train_df['predweekday'] = train_df.apply(mark_predweekday, axis=1)\n",
        "      test_df['predweekday'] = test_df.apply(mark_predweekday, axis=1)\n",
        "\n",
        "      # ----------------------------------------------------------------------------------------------------------\n",
        "\n",
        "      # [1]\n",
        "      train_df['new1'] = np.where(train_df['img0']>0,1,0)\n",
        "      train_df['new2'] = np.where(train_df['light_wake_time_diff']>0,1,0)\n",
        "      train_df['new3'] = np.where(train_df['light_sleep_time_ratio']>0,1,0)\n",
        "      train_df['new4'] = np.where(train_df['light_wake_time_ratio']>0,1,0)\n",
        "      train_df['new5'] = np.where(train_df['light_sleep_duration_ratio']>0,1,0)\n",
        "      train_df['new6'] = np.where(train_df['sleep_duration_vs_weekday_avg']>0,1,0)\n",
        "      train_df['new7'] = np.where(train_df['wake_time_ratio']>1,1,0)\n",
        "      train_df['new8'] = np.where(train_df['sleep_duration_ratio']>1,1,0)\n",
        "\n",
        "      # [2]\n",
        "      test_df['new1'] = np.where(test_df['img0']>0,1,0)\n",
        "      test_df['new2'] = np.where(test_df['light_wake_time_diff']>0,1,0)\n",
        "      test_df['new3'] = np.where(test_df['light_sleep_time_ratio']>0,1,0)\n",
        "      test_df['new4'] = np.where(test_df['light_wake_time_ratio']>0,1,0)\n",
        "      test_df['new5'] = np.where(test_df['light_sleep_duration_ratio']>0,1,0)\n",
        "      test_df['new6'] = np.where(test_df['sleep_duration_vs_weekday_avg']>0,1,0)\n",
        "      test_df['new7'] = np.where(test_df['wake_time_ratio']>1,1,0)\n",
        "      test_df['new8'] = np.where(test_df['sleep_duration_ratio']>1,1,0)\n",
        "\n",
        "      # ----------------------------------------------------------------------------------------------------------\n",
        "\n",
        "      # TARGET ENCODER\n",
        "      for new in ['new1','new2','new6','new7','new8','weekend']:\n",
        "\n",
        "        for tgt in all_targets:\n",
        "\n",
        "          encoder_feats = ['subject_id','month',new] # 'weekday', 'subject_id','month','weekend'\n",
        "\n",
        "          #### ENCODER1\n",
        "\n",
        "          subject_mean = train_df.groupby(encoder_feats)[tgt].mean().rename(f'{tgt}_{encoder_feats[2]}_te')\n",
        "          train_df = train_df.merge(subject_mean, on=encoder_feats, how='left')\n",
        "          test_df = test_df.merge(subject_mean, on=encoder_feats, how='left')\n",
        "          global_mean = train_df[tgt].mean()\n",
        "          test_df[f'{tgt}_{encoder_feats[2]}_te'] = test_df[f'{tgt}_{encoder_feats[2]}_te'].fillna(global_mean)\n",
        "\n",
        "          # ÎÖ∏Ïù¥Ï¶à Ï∂îÍ∞Ä\n",
        "          train_df[f'{tgt}_{encoder_feats[2]}_te'] = add_noise(train_df[f'{tgt}_{encoder_feats[2]}_te'], noise_level)\n",
        "          test_df[f'{tgt}_{encoder_feats[2]}_te'] = add_noise(test_df[f'{tgt}_{encoder_feats[2]}_te'], noise_level)\n",
        "\n",
        "          #### ENCODER2\n",
        "\n",
        "          # ÏÉàÎ°úÏö¥ Î≤îÏ£ºÌòï Ïó¥ ÏÉùÏÑ±\n",
        "          train_df['TMP'] = train_df[encoder_feats].applymap(str).apply(lambda x: ''.join(x) ,axis=1)\n",
        "          test_df['TMP'] = test_df[encoder_feats].applymap(str).apply(lambda x: ''.join(x) ,axis=1)\n",
        "\n",
        "          # ENCODER\n",
        "          encoder = TargetEncoder(cols=['TMP'], smoothing=300) # 40\n",
        "          encoder.fit(train_df[['TMP']], train_df[tgt])\n",
        "\n",
        "          # Ïù∏ÏΩîÎî© Í≤∞Í≥ºÎ•º ÏÉàÎ°úÏö¥ Ïó¥Ïóê Ï†ÄÏû•\n",
        "          train_df[f'{tgt}_{encoder_feats[2]}_te2'] = encoder.transform(train_df[['TMP']])\n",
        "          test_df[f'{tgt}_{encoder_feats[2]}_te2'] = encoder.transform(test_df[['TMP']])\n",
        "\n",
        "          # ADD NOISE\n",
        "          train_df[f'{tgt}_{encoder_feats[2]}_te2'] = add_noise(train_df[f'{tgt}_{encoder_feats[2]}_te2'], noise_level)\n",
        "          test_df[f'{tgt}_{encoder_feats[2]}_te2'] = add_noise(test_df[f'{tgt}_{encoder_feats[2]}_te2'], noise_level)\n",
        "\n",
        "          # DROP TMP COLUMNS\n",
        "          train_df = train_df.drop(columns=['TMP'])\n",
        "          test_df = test_df.drop(columns=['TMP'])\n",
        "\n",
        "\n",
        "      # ENCODER\n",
        "      PK = ['sleep_date', 'lifelog_date', 'subject_id']\n",
        "      encoder = LabelEncoder()\n",
        "      categorical_features = [i for i in train_df.select_dtypes(include=['object', 'category']).columns if i not in PK+['pk']]\n",
        "      for col in categorical_features:\n",
        "          # print(col)\n",
        "          train_df[col] = encoder.fit_transform(train_df[col])\n",
        "          test_df[col] = encoder.fit_transform(test_df[col])\n",
        "\n",
        "      # X\n",
        "      X = train_df.drop(columns=PK + all_targets)\n",
        "      test_X = test_df.drop(columns=PK + all_targets)\n",
        "\n",
        "      total_avg_f1s = []\n",
        "      val_f1 = []\n",
        "      binary_val_preds = {}\n",
        "      multiclass_val_preds = {}\n",
        "      binary_test_preds = {}\n",
        "      multiclass_test_preds = {}\n",
        "      test_preds = {}\n",
        "      xfeatures_dict = {}\n",
        "\n",
        "      # Find optimal weights\n",
        "      best_weights = []\n",
        "      best_scores = []\n",
        "\n",
        "      # ------\n",
        "      # binary\n",
        "      # ------\n",
        "\n",
        "      for col in targets_binary:\n",
        "\n",
        "          y = train_df[col]\n",
        "\n",
        "          valid_ids['pk'] = valid_ids['subject_id']+valid_ids['sleep_date']\n",
        "          valid_ids['pk'] = valid_ids['pk'].str.strip().tolist()\n",
        "          train_df['pk'] = train_df['subject_id']+train_df['sleep_date']\n",
        "\n",
        "          X_valid = train_df.loc[train_df['pk'].isin(valid_ids['pk']),X.columns.tolist()].reset_index(drop=True).copy()\n",
        "          X_train = train_df.loc[~train_df['pk'].isin(valid_ids['pk']),X.columns.tolist()].reset_index(drop=True).copy()\n",
        "          y_valid = train_df.loc[train_df['pk'].isin(valid_ids['pk']),y.name].reset_index(drop=True).copy()\n",
        "          y_train = train_df.loc[~train_df['pk'].isin(valid_ids['pk']),y.name].reset_index(drop=True).copy()\n",
        "\n",
        "          # ----------------------------------------------------------------------------------------------------------\n",
        "\n",
        "          feature_names = X.columns.tolist()\n",
        "\n",
        "          if col in ['Q3']:\n",
        "            # xfeatures1\n",
        "            model = XGBClassifier(**self.xgb_params)\n",
        "            model.fit(X_train, y_train)\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "            shap_values = explainer.shap_values(X_train)\n",
        "            shap_importance = np.abs(shap_values).mean(axis=0)\n",
        "            shap_df = pd.DataFrame({\n",
        "                'feature': X_train.columns,\n",
        "                'shap_importance': shap_importance\n",
        "            }).sort_values(by='shap_importance', ascending=False)\n",
        "            xfeatures1 = shap_df.head(15)['feature'].tolist()\n",
        "\n",
        "          else:\n",
        "            xfeatures1 = []\n",
        "\n",
        "          # xfeatures2\n",
        "          correlations = X.select_dtypes(include=['number']).corrwith(y)\n",
        "          sorted_correlations = correlations.abs().sort_values(ascending=False)\n",
        "          xfeatures2 = sorted_correlations[sorted_correlations>0.1].index.tolist()\n",
        "          xfeatures_dict[col] = [i for i in X.columns.tolist() if i in set(xfeatures1+xfeatures2)]\n",
        "\n",
        "          # ----------------------------------------------------------------------------------------------------------\n",
        "\n",
        "          X_valid = train_df.loc[train_df['pk'].isin(valid_ids['pk']),xfeatures_dict[col]].reset_index(drop=True).copy()\n",
        "          X_train = train_df.loc[~train_df['pk'].isin(valid_ids['pk']),xfeatures_dict[col]].reset_index(drop=True).copy()\n",
        "          y_valid = train_df.loc[train_df['pk'].isin(valid_ids['pk']),y.name].reset_index(drop=True).copy()\n",
        "          y_train = train_df.loc[~train_df['pk'].isin(valid_ids['pk']),y.name].reset_index(drop=True).copy()\n",
        "\n",
        "          # Train LightGBM\n",
        "          lgb_model = LGBMClassifier(**self.lgb_params)\n",
        "          lgb_model.fit(X_train, y_train)\n",
        "\n",
        "          # Train XGBoost\n",
        "          xgb_model = XGBClassifier(**self.xgb_params)\n",
        "          xgb_model.fit(X_train, y_train)\n",
        "\n",
        "          # Train TabPFN\n",
        "          tabpfn_model = TabPFNClassifier(**self.tabpfn_params)\n",
        "          tabpfn_model.fit(X_train, y_train)\n",
        "\n",
        "          tab_pred_valid = tabpfn_model.predict_proba(X_valid.values)[:, 1]\n",
        "          lgb_pred_valid = lgb_model.predict_proba(X_valid)[:, 1]\n",
        "          xgb_pred_valid = xgb_model.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "          pred_valid = (lgb_A * lgb_pred_valid + xgb_B * xgb_pred_valid + tab_C * tab_pred_valid > 0.5).astype(int)\n",
        "\n",
        "          f1 = f1_score(y_valid, pred_valid, average='macro')\n",
        "          val_f1.append(f1)\n",
        "\n",
        "          # Store predictions\n",
        "          binary_val_preds[col] = {\n",
        "              'lgb': lgb_pred_valid,\n",
        "              'xgb': xgb_pred_valid,\n",
        "              'tab': tab_pred_valid,\n",
        "              'true': y_valid\n",
        "          }\n",
        "\n",
        "      # ----------\n",
        "      # multiclass\n",
        "      # ----------\n",
        "\n",
        "      y = train_df[target_multiclass]\n",
        "      X_valid = train_df.loc[train_df['pk'].isin(valid_ids['pk']),X.columns.tolist()].reset_index(drop=True).copy()\n",
        "      X_train = train_df.loc[~train_df['pk'].isin(valid_ids['pk']),X.columns.tolist()].reset_index(drop=True).copy()\n",
        "      y_valid = train_df.loc[train_df['pk'].isin(valid_ids['pk']),y.name].reset_index(drop=True).copy()\n",
        "      y_train = train_df.loc[~train_df['pk'].isin(valid_ids['pk']),y.name].reset_index(drop=True).copy()\n",
        "\n",
        "      # ----------------------------------------------------------------------------------------------------------\n",
        "\n",
        "      feature_names = X.columns.tolist()\n",
        "      xfeatures1 = []\n",
        "      correlations = X.select_dtypes(include=['number']).corrwith(y)\n",
        "      sorted_correlations = correlations.abs().sort_values(ascending=False)\n",
        "      xfeatures2 = sorted_correlations[sorted_correlations>0.1].index.tolist()\n",
        "      xfeatures_dict['S1'] = [i for i in X.columns.tolist() if i in set(xfeatures1+xfeatures2)]\n",
        "\n",
        "      # ----------------------------------------------------------------------------------------------------------\n",
        "\n",
        "      y = train_df[target_multiclass]\n",
        "      X_valid = train_df.loc[train_df['pk'].isin(valid_ids['pk']),xfeatures_dict['S1']].reset_index(drop=True).copy()\n",
        "      X_train = train_df.loc[~train_df['pk'].isin(valid_ids['pk']),xfeatures_dict['S1']].reset_index(drop=True).copy()\n",
        "      y_valid = train_df.loc[train_df['pk'].isin(valid_ids['pk']),y.name].reset_index(drop=True).copy()\n",
        "      y_train = train_df.loc[~train_df['pk'].isin(valid_ids['pk']),y.name].reset_index(drop=True).copy()\n",
        "\n",
        "      # ÌÅ¥ÎûòÏä§ weight Í≥ÑÏÇ∞\n",
        "      classes = np.unique(y_train)\n",
        "      weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
        "      class_weights = dict(zip(classes, weights))\n",
        "\n",
        "      # Í∞Å ÏÉòÌîåÏóê ÎåÄÌï¥ weight Îß§Ìïë\n",
        "      w_train = pd.Series(y_train).map(class_weights)\n",
        "      w_train = compute_sample_weight(class_weight='balanced', y=y_train)\n",
        "\n",
        "      if S1_balance==True:\n",
        "        # Train LightGBM\n",
        "        lgb_model = LGBMClassifier(**self.lgb_params, objective='multiclass', num_class=3)\n",
        "        lgb_model.fit(X_train, y_train, sample_weight=w_train)\n",
        "\n",
        "        # Train XGBoost\n",
        "        xgb_model = XGBClassifier(**self.xgb_params_S1, objective='multi:softmax', num_class=3)\n",
        "        xgb_model.fit(X_train, y_train,sample_weight=w_train)\n",
        "      else:\n",
        "        # Train LightGBM\n",
        "        lgb_model = LGBMClassifier(**self.lgb_params, objective='multiclass', num_class=3)\n",
        "        lgb_model.fit(X_train, y_train)\n",
        "\n",
        "        # Train XGBoost\n",
        "        xgb_model = XGBClassifier(**self.xgb_params_S1, objective='multi:softmax', num_class=3)\n",
        "        xgb_model.fit(X_train, y_train)\n",
        "\n",
        "      # Train TabPFN\n",
        "      tabpfn_model = TabPFNClassifier(**self.tabpfn_params)\n",
        "      tabpfn_model.fit(X_train, y_train)\n",
        "\n",
        "      # Get predictions and ensemble\n",
        "      lgb_pred_valid = lgb_model.predict_proba(X_valid)\n",
        "      xgb_pred_valid = xgb_model.predict_proba(X_valid)\n",
        "      tab_pred_valid = tabpfn_model.predict_proba(X_valid.values)\n",
        "\n",
        "      pred_valid = np.argmax(lgb_A * lgb_pred_valid + xgb_B * xgb_pred_valid + tab_C * tab_pred_valid, axis=1)\n",
        "\n",
        "      f1 = f1_score(y_valid, pred_valid, average='macro')\n",
        "      val_f1.append(f1)\n",
        "\n",
        "      multiclass_val_preds = {\n",
        "          'lgb': lgb_pred_valid,\n",
        "          'xgb': xgb_pred_valid,\n",
        "          'tab': tab_pred_valid,\n",
        "          'true': y_valid\n",
        "      }\n",
        "\n",
        "      # Generate all possible weight combinations that sum to 1\n",
        "      from itertools import product\n",
        "\n",
        "      step = 0.1\n",
        "      candidates = np.arange(0, 1.1, step)\n",
        "\n",
        "      for lgb_A, xgb_B, tab_C in product(candidates, repeat=3):\n",
        "          total = lgb_A + xgb_B + tab_C\n",
        "          if np.isclose(total, 1.0):\n",
        "              weights = (lgb_A, xgb_B, tab_C)\n",
        "              val_scores = []\n",
        "\n",
        "              # Binary targets\n",
        "              for col in targets_binary:\n",
        "                  preds = binary_val_preds[col]\n",
        "                  blended = lgb_A * preds['lgb'] + xgb_B * preds['xgb'] + tab_C * preds['tab']\n",
        "                  val_scores.append(f1_score(preds['true'], (blended > 0.5).astype(int), average='macro'))\n",
        "\n",
        "              # Multiclass target\n",
        "              preds = multiclass_val_preds\n",
        "              blended = lgb_A * preds['lgb'] + xgb_B * preds['xgb'] + tab_C * preds['tab']\n",
        "              val_scores.append(f1_score(preds['true'], np.argmax(blended, axis=1), average='macro'))\n",
        "\n",
        "              best_weights.append(weights)\n",
        "              best_scores.append(np.mean(val_scores))\n",
        "\n",
        "      # ------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "      # Sort results and get top\n",
        "      sorted_indices = np.argsort(best_scores)[::-1]\n",
        "      top_weights = [best_weights[i] for i in sorted_indices]\n",
        "      top_scores = [best_scores[i] for i in sorted_indices]\n",
        "\n",
        "      # eval\n",
        "      avg_f1 = np.mean(val_f1)\n",
        "      total_avg_f1s.append(avg_f1)\n",
        "      detail = \" \".join([f\"{name}({tname}):{score:.4f}\" for name, tname, score in zip(targets_binary + [target_multiclass], targets_binary_name + ['S1'], val_f1)])\n",
        "      print(f\"# 6 Targets F1 avg: {avg_f1:.4f} / [Details] {detail}\")\n",
        "\n",
        "      # binary\n",
        "      binary_preds = {}\n",
        "      binary_preds_proba = {}\n",
        "      for col in targets_binary:\n",
        "\n",
        "          y = train_df[col]\n",
        "          is_multiclass = False\n",
        "\n",
        "          # Train LightGBM\n",
        "          lgb_model = LGBMClassifier(**self.lgb_params)\n",
        "          lgb_model.fit(X[xfeatures_dict[col]], y)\n",
        "\n",
        "          # Train XGBoost\n",
        "          xgb_model = XGBClassifier(**self.xgb_params)\n",
        "          xgb_model.fit(X[xfeatures_dict[col]], y)\n",
        "\n",
        "          # Train TabPFN\n",
        "          tabpfn_model = TabPFNClassifier(**self.tabpfn_params)\n",
        "          tabpfn_model.fit(X[xfeatures_dict[col]], y)\n",
        "\n",
        "          tab_pred = tabpfn_model.predict_proba(test_X[xfeatures_dict[col]])[:, 1]\n",
        "          lgb_pred = lgb_model.predict_proba(test_X[xfeatures_dict[col]])[:, 1]\n",
        "          xgb_pred = xgb_model.predict_proba(test_X[xfeatures_dict[col]])[:, 1]\n",
        "\n",
        "          binary_preds[col] = (lgb_A * lgb_pred + xgb_B * xgb_pred + tab_C * tab_pred > 0.5).astype(int)\n",
        "\n",
        "          # Store predictions\n",
        "          binary_test_preds[col] = {\n",
        "              'lgb': lgb_pred,\n",
        "              'xgb': xgb_pred,\n",
        "              'tab': tab_pred\n",
        "          }\n",
        "\n",
        "          # Feature importance (using LightGBM's importance)\n",
        "          fi_df = pd.DataFrame({'feature': X[xfeatures_dict[col]].columns, 'importance': lgb_model.feature_importances_})\n",
        "          fi_df = fi_df[~fi_df['feature'].str.contains('_te')]\n",
        "          top10 = fi_df.sort_values(by='importance', ascending=False).head(10)\n",
        "          feat_str = \", \".join([f\"{row['feature']}({int(row['importance'])})\" for _, row in top10.iterrows()])\n",
        "          # print(f\"[{col}] {feat_str}\")\n",
        "\n",
        "\n",
        "      # ----------\n",
        "      # multiclass\n",
        "      # ----------\n",
        "\n",
        "      y = train_df['S1']\n",
        "\n",
        "      # CLASS WEIGHTS\n",
        "      classes = np.unique(y)\n",
        "      weights = compute_class_weight(class_weight='balanced', classes=classes, y=y)\n",
        "      class_weights = dict(zip(classes, weights))\n",
        "\n",
        "      # Í∞Å ÏÉòÌîåÏóê ÎåÄÌï¥ weight Îß§Ìïë\n",
        "      w_train = pd.Series(y).map(class_weights)\n",
        "      w_train = compute_sample_weight(class_weight='balanced', y=y)\n",
        "\n",
        "      is_multiclass = True\n",
        "\n",
        "      if S1_balance==True:\n",
        "        # Train LightGBM\n",
        "        lgb_model = LGBMClassifier(**self.lgb_params, objective='multiclass', num_class=3)\n",
        "        lgb_model.fit(X[xfeatures_dict['S1']], y, sample_weight=w_train)\n",
        "\n",
        "        # Train XGBoost\n",
        "        xgb_model = XGBClassifier(**self.xgb_params_S1, objective='multi:softmax', num_class=3)\n",
        "        xgb_model.fit(X[xfeatures_dict['S1']], y,sample_weight=w_train)\n",
        "      else:\n",
        "        # Train LightGBM\n",
        "        lgb_model = LGBMClassifier(**self.lgb_params, objective='multiclass', num_class=3)\n",
        "        lgb_model.fit(X[xfeatures_dict['S1']], y)\n",
        "\n",
        "        # Train XGBoost\n",
        "        xgb_model = XGBClassifier(**self.xgb_params_S1, objective='multi:softmax', num_class=3)\n",
        "        xgb_model.fit(X[xfeatures_dict['S1']], y)\n",
        "\n",
        "      # Train TabPFN\n",
        "      tabpfn_model = TabPFNClassifier(**self.tabpfn_params)\n",
        "      tabpfn_model.fit(X[xfeatures_dict['S1']], y)\n",
        "\n",
        "      # Get predictions and ensemble\n",
        "      lgb_pred = lgb_model.predict_proba(test_X[xfeatures_dict['S1']])\n",
        "      xgb_pred = xgb_model.predict_proba(test_X[xfeatures_dict['S1']])\n",
        "      tab_pred = tabpfn_model.predict_proba(test_X[xfeatures_dict['S1']])\n",
        "\n",
        "      multiclass_test_preds = {\n",
        "          'lgb': lgb_pred,\n",
        "          'xgb': xgb_pred,\n",
        "          'tab': tab_pred\n",
        "      }\n",
        "\n",
        "      multiclass_pred = np.argmax(lgb_A * lgb_pred + xgb_B * xgb_pred + tab_C * tab_pred, axis=1)\n",
        "      multiclass_pred_proba = lgb_A * lgb_pred + xgb_B * xgb_pred + tab_C * tab_pred\n",
        "\n",
        "      # Feature importance\n",
        "      fi_df = pd.DataFrame({'feature': X[xfeatures_dict['S1']].columns, 'importance': lgb_model.feature_importances_})\n",
        "      fi_df = fi_df[~fi_df['feature'].str.contains('_te')]\n",
        "      top10 = fi_df.sort_values(by='importance', ascending=False).head(10)\n",
        "      feat_str = \", \".join([f\"{row['feature']}({int(row['importance'])})\" for _, row in top10.iterrows()])\n",
        "      # print(f\"[S1] {feat_str}\")\n",
        "\n",
        "      # SAVE PRED\n",
        "      submission_final['S1'] = multiclass_pred\n",
        "      for col in targets_binary:\n",
        "        submission_final[col] = binary_preds[col]\n",
        "      submission_final = submission_final[['subject_id', 'sleep_date', 'lifelog_date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]\n",
        "      fname = f\"submission_{np.mean(total_avg_f1s)}.csv\"\n",
        "      submission_final.to_csv(fname, index=False)\n",
        "\n",
        "      # Top 1 Weight Combinations\n",
        "      submission_final_dict = {}\n",
        "      print(\"\\nTop 1 Weight Combinations:\")\n",
        "      for i, (weights, score) in enumerate(zip(top_weights[:1], top_scores[:1])):\n",
        "\n",
        "          print(f\"Rank {i+1}: lgb_A={weights[0]:.1f}, xgb_B={weights[1]:.1f}, tab_C={weights[2]:.1f} - Score: {score:.4f}\")\n",
        "          lgb_A, xgb_B, tab_C = weights\n",
        "\n",
        "          # Binary predictions\n",
        "          for col in targets_binary:\n",
        "              preds = binary_test_preds[col]\n",
        "              ensemble_pred = (lgb_A * preds['lgb'] + xgb_B * preds['xgb'] + tab_C * preds['tab'] > 0.5).astype(int)\n",
        "              submission_final[col] = ensemble_pred\n",
        "\n",
        "          # Multiclass prediction\n",
        "          preds = multiclass_test_preds\n",
        "          ensemble_pred = np.argmax(lgb_A * preds['lgb'] + xgb_B * preds['xgb'] + tab_C * preds['tab'], axis=1)\n",
        "          submission_final['S1'] = ensemble_pred\n",
        "\n",
        "          # SAVE SUBMISSIONS\n",
        "          submission_final_dict[i] = submission_final.copy()\n",
        "          fname = f\"submission_top{i+1}_{score:.4f}.csv\"\n",
        "          submission_final_dict[i].to_csv(fname, index=False)\n",
        "          print(f\"Saved submission to {fname} \\n\")\n",
        "\n",
        "      # Î™®Îç∏Î≥Ñ ÏòàÏ∏°Í≤∞Í≥º ÎπÑÏú® ÎπÑÍµê\n",
        "      a11 = train_df[['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']].sum()\n",
        "      a13 = train_df[['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']].apply(len)\n",
        "      a12 = train_df[['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']].mean()\n",
        "      a21 = submission_final_dict[0][['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']].sum()\n",
        "      a23 = submission_final_dict[0][['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']].apply(len)\n",
        "      a22 = submission_final_dict[0][['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']].mean()\n",
        "      result = pd.concat([a11, a13, a12, a21, a23, a22], axis=1)\n",
        "      result.columns = ['ÌïôÏäµsum','ÌïôÏäµlen','ÌïôÏäµmean','ÌÖåÏä§Ìä∏sum','ÌÖåÏä§Ìä∏len','ÌÖåÏä§Ìä∏mean']\n",
        "      # print('\\n STEP3: ÏòàÏ∏°Í≤∞Í≥º ÎπÑÍµêÌëú')\n",
        "      # display(result)\n",
        "\n",
        "      # S1Î∂ÑÌè¨\n",
        "      a1 = train['S1'].value_counts(normalize=True)\n",
        "      a2 = submission_final['S1'].value_counts(normalize=True)\n",
        "      S1Î∂ÑÌè¨ = pd.concat([a1,a2],axis=1)\n",
        "      # display(S1Î∂ÑÌè¨)\n",
        "\n",
        "      oof_result = []\n",
        "\n",
        "      return submission_final_dict[0], oof_result"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9XKcbkGhQ4xi"
      },
      "id": "9XKcbkGhQ4xi",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_9LsP3MuRKBL"
      },
      "id": "_9LsP3MuRKBL",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T4IBlufQRKDi"
      },
      "id": "T4IBlufQRKDi",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LjudZWAvQ40M"
      },
      "id": "LjudZWAvQ40M",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå do experiments\n",
        "1. original\n",
        "2. mean impute\n",
        "3. KNN impute (K=5)\n",
        "4. LLM impute"
      ],
      "metadata": {
        "id": "2cCN81RDRbJj"
      },
      "id": "2cCN81RDRbJj"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gvl1e3pdTJxO"
      },
      "id": "gvl1e3pdTJxO",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CPKjj-mBTKC8"
      },
      "id": "CPKjj-mBTKC8",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_DudNi9wTKiV"
      },
      "id": "_DudNi9wTKiV",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. eval : original"
      ],
      "metadata": {
        "id": "8sNyAENLTOpD"
      },
      "id": "8sNyAENLTOpD"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "NQKvDq1oieSC",
      "metadata": {
        "id": "NQKvDq1oieSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "819c29f7-1fbb-46e3-81e8-d04fe3043421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# valid size:40\n",
            "# lgb_A:0.4 xgb_B:0.3 tab_C:0.3\n",
            "# 6 Targets F1 avg: 0.6841 / [Details] Q1(Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà):0.7163 Q2(Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú):0.7802 Q3(Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§):0.7151 S2(ÏàòÎ©¥Ìö®Ïú®):0.6698 S3(ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ):0.7475 S1(S1):0.4759\n",
            "\n",
            "Top 1 Weight Combinations:\n",
            "Rank 1: lgb_A=0.1, xgb_B=0.2, tab_C=0.7 - Score: 0.6998\n",
            "Saved submission to submission_top1_0.6998.csv \n",
            "\n",
            "# valid size:105\n",
            "# lgb_A:0.4 xgb_B:0.3 tab_C:0.3\n",
            "# 6 Targets F1 avg: 0.6956 / [Details] Q1(Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà):0.7043 Q2(Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú):0.8081 Q3(Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§):0.6907 S2(ÏàòÎ©¥Ìö®Ïú®):0.6870 S3(ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ):0.7041 S1(S1):0.5793\n",
            "\n",
            "Top 1 Weight Combinations:\n",
            "Rank 1: lgb_A=0.2, xgb_B=0.0, tab_C=0.8 - Score: 0.7060\n",
            "Saved submission to submission_top1_0.7060.csv \n",
            "\n",
            "CPU times: user 22min 44s, sys: 2.81 s, total: 22min 47s\n",
            "Wall time: 3min 20s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "\"\"\"\n",
        "# valid size:40\n",
        "# lgb_A:0.4 xgb_B:0.3 tab_C:0.3\n",
        "# 6 Targets F1 avg: 0.6841 / [Details] Q1(Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà):0.7163 Q2(Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú):0.7802 Q3(Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§):0.7151 S2(ÏàòÎ©¥Ìö®Ïú®):0.6698 S3(ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ):0.7475 S1(S1):0.4759\n",
        "\n",
        "Top 1 Weight Combinations:\n",
        "Rank 1: lgb_A=0.1, xgb_B=0.2, tab_C=0.7 - Score: 0.6998\n",
        "Saved submission to submission_top1_0.6998.csv\n",
        "\n",
        "# valid size:105\n",
        "# lgb_A:0.4 xgb_B:0.3 tab_C:0.3\n",
        "# 6 Targets F1 avg: 0.6956 / [Details] Q1(Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà):0.7043 Q2(Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú):0.8081 Q3(Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§):0.6907 S2(ÏàòÎ©¥Ìö®Ïú®):0.6870 S3(ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ):0.7041 S1(S1):0.5793\n",
        "\n",
        "Top 1 Weight Combinations:\n",
        "Rank 1: lgb_A=0.2, xgb_B=0.0, tab_C=0.8 - Score: 0.7060\n",
        "Saved submission to submission_top1_0.7060.csv\n",
        "\n",
        "CPU times: user 22min 44s, sys: 2.81 s, total: 22min 47s\n",
        "Wall time: 3min 20s\n",
        "\"\"\"\n",
        "\n",
        "ETRI = etri_lifelog_pipeline()\n",
        "valid_ids1 = pd.read_csv(StringIO(ETRI.valid_id1), sep=',')\n",
        "valid_ids2 = pd.read_csv(StringIO(ETRI.valid_id2), sep=',')\n",
        "\n",
        "for valid_id_num in ['1','2']:\n",
        "\n",
        "  # 2 VALID DATASETS\n",
        "  if valid_id_num == '1':\n",
        "    valid_ids = valid_ids1.copy()\n",
        "  elif valid_id_num == '2':\n",
        "    valid_ids = valid_ids2.copy()\n",
        "\n",
        "  # SET VALID DATASET\n",
        "  valid_ids.columns = valid_ids.columns.str.strip()\n",
        "  valid_ids['pk'] = valid_ids['subject_id']+valid_ids['sleep_date']\n",
        "  valid_ids['pk'] = valid_ids['pk'].str.strip().tolist()\n",
        "  print(f\"# valid size:{len(valid_ids)}\")\n",
        "\n",
        "  # READ DATA\n",
        "  train, test = ETRI.read_originaldata()\n",
        "\n",
        "  # PREPROCESS DATA\n",
        "  train, test = ETRI.preprocess(train, test)\n",
        "\n",
        "  # SET PARAMETERS\n",
        "  best_param_dict = {}\n",
        "  best_param_dict['Q3'] = ETRI.common_params\n",
        "  best_param_dict['S1'] = ETRI.common_params\n",
        "  best_param_dict['S2'] = ETRI.common_params\n",
        "  best_param_dict['S3'] = ETRI.common_params\n",
        "  best_param_dict['Q1'] = ETRI.common_params\n",
        "  best_param_dict['Q2'] = ETRI.common_params\n",
        "\n",
        "  # RUN MODEL\n",
        "  submission_final, oof_result = ETRI.run_basemodel(train, test, valid_ids, best_param_dict, n_splits=5, random_state=42, S1_balance=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EqrtIdJrIWfj"
      },
      "id": "EqrtIdJrIWfj",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "59ZDfVRV8wel"
      },
      "id": "59ZDfVRV8wel",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXql1QZlIWkp"
      },
      "id": "pXql1QZlIWkp",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8nAdGoNeIWol"
      },
      "id": "8nAdGoNeIWol",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. eval : mean impute"
      ],
      "metadata": {
        "id": "T-hyniESTqbr"
      },
      "id": "T-hyniESTqbr"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "\"\"\"\n",
        "# valid size:40\n",
        "# lgb_A:0.4 xgb_B:0.3 tab_C:0.3\n",
        "# 6 Targets F1 avg: 0.6684 / [Details] Q1(Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà):0.6931 Q2(Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú):0.8107 Q3(Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§):0.6800 S2(ÏàòÎ©¥Ìö®Ïú®):0.6190 S3(ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ):0.7234 S1(S1):0.4843\n",
        "\n",
        "Top 1 Weight Combinations:\n",
        "Rank 1: lgb_A=0.2, xgb_B=0.0, tab_C=0.8 - Score: 0.7152\n",
        "Saved submission to submission_top1_0.7152.csv\n",
        "\n",
        "# valid size:105\n",
        "# lgb_A:0.4 xgb_B:0.3 tab_C:0.3\n",
        "# 6 Targets F1 avg: 0.6912 / [Details] Q1(Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà):0.6945 Q2(Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú):0.7797 Q3(Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§):0.6701 S2(ÏàòÎ©¥Ìö®Ïú®):0.6955 S3(ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ):0.7165 S1(S1):0.5907\n",
        "\n",
        "Top 1 Weight Combinations:\n",
        "Rank 1: lgb_A=0.0, xgb_B=0.1, tab_C=0.9 - Score: 0.7028\n",
        "Saved submission to submission_top1_0.7028.csv\n",
        "\n",
        "CPU times: user 23min 21s, sys: 2.73 s, total: 23min 24s\n",
        "Wall time: 3min 25s\n",
        "\"\"\"\n",
        "\n",
        "ETRI = etri_lifelog_pipeline()\n",
        "valid_ids1 = pd.read_csv(StringIO(ETRI.valid_id1), sep=',')\n",
        "valid_ids2 = pd.read_csv(StringIO(ETRI.valid_id2), sep=',')\n",
        "\n",
        "for valid_id_num in ['1','2']:\n",
        "\n",
        "  # 2 VALID DATASETS\n",
        "  if valid_id_num == '1':\n",
        "    valid_ids = valid_ids1.copy()\n",
        "  elif valid_id_num == '2':\n",
        "    valid_ids = valid_ids2.copy()\n",
        "\n",
        "  # SET VALID DATASET\n",
        "  valid_ids.columns = valid_ids.columns.str.strip()\n",
        "  valid_ids['pk'] = valid_ids['subject_id']+valid_ids['sleep_date']\n",
        "  valid_ids['pk'] = valid_ids['pk'].str.strip().tolist()\n",
        "  print(f\"# valid size:{len(valid_ids)}\")\n",
        "\n",
        "  # READ DATA\n",
        "  train, test = ETRI.read_meanimputedata()\n",
        "\n",
        "  # PREPROCESS DATA\n",
        "  train, test = ETRI.preprocess(train, test)\n",
        "\n",
        "  # SET PARAMETERS\n",
        "  best_param_dict = {}\n",
        "  best_param_dict['Q3'] = ETRI.common_params\n",
        "  best_param_dict['S1'] = ETRI.common_params\n",
        "  best_param_dict['S2'] = ETRI.common_params\n",
        "  best_param_dict['S3'] = ETRI.common_params\n",
        "  best_param_dict['Q1'] = ETRI.common_params\n",
        "  best_param_dict['Q2'] = ETRI.common_params\n",
        "\n",
        "  # RUN MODEL\n",
        "  submission_final, oof_result = ETRI.run_basemodel(train, test, valid_ids, best_param_dict, n_splits=5, random_state=42, S1_balance=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-9Kc0uaIWqG",
        "outputId": "c1d7c269-9c5f-478b-95c5-542c15fb82e6"
      },
      "id": "S-9Kc0uaIWqG",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# valid size:40\n",
            "# lgb_A:0.4 xgb_B:0.3 tab_C:0.3\n",
            "# 6 Targets F1 avg: 0.6684 / [Details] Q1(Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà):0.6931 Q2(Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú):0.8107 Q3(Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§):0.6800 S2(ÏàòÎ©¥Ìö®Ïú®):0.6190 S3(ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ):0.7234 S1(S1):0.4843\n",
            "\n",
            "Top 1 Weight Combinations:\n",
            "Rank 1: lgb_A=0.2, xgb_B=0.0, tab_C=0.8 - Score: 0.7152\n",
            "Saved submission to submission_top1_0.7152.csv \n",
            "\n",
            "# valid size:105\n",
            "# lgb_A:0.4 xgb_B:0.3 tab_C:0.3\n",
            "# 6 Targets F1 avg: 0.6912 / [Details] Q1(Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà):0.6945 Q2(Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú):0.7797 Q3(Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§):0.6701 S2(ÏàòÎ©¥Ìö®Ïú®):0.6955 S3(ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ):0.7165 S1(S1):0.5907\n",
            "\n",
            "Top 1 Weight Combinations:\n",
            "Rank 1: lgb_A=0.0, xgb_B=0.1, tab_C=0.9 - Score: 0.7028\n",
            "Saved submission to submission_top1_0.7028.csv \n",
            "\n",
            "CPU times: user 23min 21s, sys: 2.73 s, total: 23min 24s\n",
            "Wall time: 3min 25s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A7UBRB2xyfAg"
      },
      "id": "A7UBRB2xyfAg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cqRqp0q_ranh"
      },
      "id": "cqRqp0q_ranh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "11xpQOaGraqj"
      },
      "id": "11xpQOaGraqj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IFih-wPpylLZ"
      },
      "id": "IFih-wPpylLZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D9aa9-5eRJdl",
      "metadata": {
        "id": "D9aa9-5eRJdl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. eval: KNN impute"
      ],
      "metadata": {
        "id": "_HL4-IzaeDhX"
      },
      "id": "_HL4-IzaeDhX"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "\"\"\"\n",
        "# valid size:40\n",
        "# lgb_A:0.4 xgb_B:0.3 tab_C:0.3\n",
        "# 6 Targets F1 avg: 0.6989 / [Details] Q1(Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà):0.6970 Q2(Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú):0.8400 Q3(Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§):0.7151 S2(ÏàòÎ©¥Ìö®Ïú®):0.6992 S3(ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ):0.7234 S1(S1):0.5185\n",
        "\n",
        "Top 1 Weight Combinations:\n",
        "Rank 1: lgb_A=0.2, xgb_B=0.1, tab_C=0.7 - Score: 0.7116\n",
        "Saved submission to submission_top1_0.7116.csv\n",
        "\n",
        "# valid size:105\n",
        "# lgb_A:0.4 xgb_B:0.3 tab_C:0.3\n",
        "# 6 Targets F1 avg: 0.6867 / [Details] Q1(Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà):0.7043 Q2(Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú):0.8074 Q3(Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§):0.6244 S2(ÏàòÎ©¥Ìö®Ïú®):0.6870 S3(ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ):0.7165 S1(S1):0.5805\n",
        "\n",
        "Top 1 Weight Combinations:\n",
        "Rank 1: lgb_A=0.4, xgb_B=0.0, tab_C=0.6 - Score: 0.7034\n",
        "Saved submission to submission_top1_0.7034.csv\n",
        "\n",
        "CPU times: user 21min 29s, sys: 2.66 s, total: 21min 32s\n",
        "Wall time: 3min 9s\n",
        "\"\"\"\n",
        "\n",
        "ETRI = etri_lifelog_pipeline()\n",
        "valid_ids1 = pd.read_csv(StringIO(ETRI.valid_id1), sep=',')\n",
        "valid_ids2 = pd.read_csv(StringIO(ETRI.valid_id2), sep=',')\n",
        "\n",
        "for valid_id_num in ['1','2']:\n",
        "\n",
        "  # 2 VALID DATASETS\n",
        "  if valid_id_num == '1':\n",
        "    valid_ids = valid_ids1.copy()\n",
        "  elif valid_id_num == '2':\n",
        "    valid_ids = valid_ids2.copy()\n",
        "\n",
        "  # SET VALID DATASET\n",
        "  valid_ids.columns = valid_ids.columns.str.strip()\n",
        "  valid_ids['pk'] = valid_ids['subject_id']+valid_ids['sleep_date']\n",
        "  valid_ids['pk'] = valid_ids['pk'].str.strip().tolist()\n",
        "  print(f\"# valid size:{len(valid_ids)}\")\n",
        "\n",
        "  # READ DATA\n",
        "  train, test = ETRI.read_knnimputedata()\n",
        "\n",
        "  # PREPROCESS DATA\n",
        "  train, test = ETRI.preprocess(train, test)\n",
        "\n",
        "  # SET PARAMETERS\n",
        "  best_param_dict = {}\n",
        "  best_param_dict['Q3'] = ETRI.common_params\n",
        "  best_param_dict['S1'] = ETRI.common_params\n",
        "  best_param_dict['S2'] = ETRI.common_params\n",
        "  best_param_dict['S3'] = ETRI.common_params\n",
        "  best_param_dict['Q1'] = ETRI.common_params\n",
        "  best_param_dict['Q2'] = ETRI.common_params\n",
        "\n",
        "  # RUN MODEL\n",
        "  submission_final, oof_result = ETRI.run_basemodel(train, test, valid_ids, best_param_dict, n_splits=5, random_state=42, S1_balance=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGv4BkUH6VoS",
        "outputId": "428e5970-3477-4fa5-9fcb-2dc4a5de1456"
      },
      "id": "xGv4BkUH6VoS",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# valid size:40\n",
            "# lgb_A:0.4 xgb_B:0.3 tab_C:0.3\n",
            "# 6 Targets F1 avg: 0.6964 / [Details] Q1(Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà):0.6748 Q2(Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú):0.7867 Q3(Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§):0.7484 S2(ÏàòÎ©¥Ìö®Ïú®):0.7234 S3(ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ):0.7234 S1(S1):0.5218\n",
            "\n",
            "Top 1 Weight Combinations:\n",
            "Rank 1: lgb_A=0.4, xgb_B=0.2, tab_C=0.4 - Score: 0.7164\n",
            "Saved submission to submission_top1_0.7164.csv \n",
            "\n",
            "# valid size:105\n",
            "# lgb_A:0.4 xgb_B:0.3 tab_C:0.3\n",
            "# 6 Targets F1 avg: 0.6988 / [Details] Q1(Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà):0.7141 Q2(Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú):0.8273 Q3(Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§):0.6702 S2(ÏàòÎ©¥Ìö®Ïú®):0.6955 S3(ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ):0.7165 S1(S1):0.5693\n",
            "\n",
            "Top 1 Weight Combinations:\n",
            "Rank 1: lgb_A=0.3, xgb_B=0.1, tab_C=0.6 - Score: 0.7186\n",
            "Saved submission to submission_top1_0.7186.csv \n",
            "\n",
            "CPU times: user 22min 25s, sys: 2.72 s, total: 22min 27s\n",
            "Wall time: 3min 16s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "OK-YAkk3RJja",
      "metadata": {
        "id": "OK-YAkk3RJja"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Z8h1v756dDv"
      },
      "id": "0Z8h1v756dDv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4bn5rIM06dGp"
      },
      "id": "4bn5rIM06dGp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-HVqynLYRJmV",
      "metadata": {
        "id": "-HVqynLYRJmV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. eval : LMM impute\n"
      ],
      "metadata": {
        "id": "yIrBjhlweI3a"
      },
      "id": "yIrBjhlweI3a"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "yQlSQjjjRJpl",
      "metadata": {
        "id": "yQlSQjjjRJpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b7c6e8-530c-4fb3-bd21-33e234bf2bfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# valid size:40\n",
            "# lgb_A:0.4 xgb_B:0.3 tab_C:0.3\n",
            "# 6 Targets F1 avg: 0.7218 / [Details] Q1(Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà):0.7206 Q2(Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú):0.8400 Q3(Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§):0.7484 S2(ÏàòÎ©¥Ìö®Ïú®):0.7749 S3(ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ):0.7234 S1(S1):0.5235\n",
            "\n",
            "Top 1 Weight Combinations:\n",
            "Rank 1: lgb_A=0.3, xgb_B=0.0, tab_C=0.7 - Score: 0.7238\n",
            "Saved submission to submission_top1_0.7238.csv \n",
            "\n",
            "# valid size:105\n",
            "# lgb_A:0.4 xgb_B:0.3 tab_C:0.3\n",
            "# 6 Targets F1 avg: 0.6902 / [Details] Q1(Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà):0.7141 Q2(Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú):0.7781 Q3(Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§):0.6785 S2(ÏàòÎ©¥Ìö®Ïú®):0.6654 S3(ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ):0.7041 S1(S1):0.6008\n",
            "\n",
            "Top 1 Weight Combinations:\n",
            "Rank 1: lgb_A=0.1, xgb_B=0.1, tab_C=0.8 - Score: 0.7081\n",
            "Saved submission to submission_top1_0.7081.csv \n",
            "\n",
            "CPU times: user 23min 37s, sys: 2.83 s, total: 23min 40s\n",
            "Wall time: 3min 27s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "\"\"\"\n",
        "# valid size:40\n",
        "# lgb_A:0.4 xgb_B:0.3 tab_C:0.3\n",
        "# 6 Targets F1 avg: 0.7218 / [Details] Q1(Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà):0.7206 Q2(Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú):0.8400 Q3(Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§):0.7484 S2(ÏàòÎ©¥Ìö®Ïú®):0.7749 S3(ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ):0.7234 S1(S1):0.5235\n",
        "\n",
        "Top 1 Weight Combinations:\n",
        "Rank 1: lgb_A=0.3, xgb_B=0.0, tab_C=0.7 - Score: 0.7238\n",
        "Saved submission to submission_top1_0.7238.csv\n",
        "\n",
        "# valid size:105\n",
        "# lgb_A:0.4 xgb_B:0.3 tab_C:0.3\n",
        "# 6 Targets F1 avg: 0.6902 / [Details] Q1(Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà):0.7141 Q2(Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú):0.7781 Q3(Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§):0.6785 S2(ÏàòÎ©¥Ìö®Ïú®):0.6654 S3(ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ):0.7041 S1(S1):0.6008\n",
        "\n",
        "Top 1 Weight Combinations:\n",
        "Rank 1: lgb_A=0.1, xgb_B=0.1, tab_C=0.8 - Score: 0.7081\n",
        "Saved submission to submission_top1_0.7081.csv\n",
        "\n",
        "CPU times: user 23min 37s, sys: 2.83 s, total: 23min 40s\n",
        "Wall time: 3min 27s\n",
        "\"\"\"\n",
        "\n",
        "ETRI = etri_lifelog_pipeline()\n",
        "valid_ids1 = pd.read_csv(StringIO(ETRI.valid_id1), sep=',')\n",
        "valid_ids2 = pd.read_csv(StringIO(ETRI.valid_id2), sep=',')\n",
        "\n",
        "for valid_id_num in ['1','2']:\n",
        "\n",
        "  # 2 VALID DATASETS\n",
        "  if valid_id_num == '1':\n",
        "    valid_ids = valid_ids1.copy()\n",
        "  elif valid_id_num == '2':\n",
        "    valid_ids = valid_ids2.copy()\n",
        "\n",
        "  # SET VALID DATASET\n",
        "  valid_ids.columns = valid_ids.columns.str.strip()\n",
        "  valid_ids['pk'] = valid_ids['subject_id']+valid_ids['sleep_date']\n",
        "  valid_ids['pk'] = valid_ids['pk'].str.strip().tolist()\n",
        "  print(f\"# valid size:{len(valid_ids)}\")\n",
        "\n",
        "  # READ DATA\n",
        "  train, test = ETRI.read_llmimputedata()\n",
        "\n",
        "  # PREPROCESS DATA\n",
        "  train, test = ETRI.preprocess(train, test)\n",
        "\n",
        "  # SET PARAMETERS\n",
        "  best_param_dict = {}\n",
        "  best_param_dict['Q3'] = ETRI.common_params\n",
        "  best_param_dict['S1'] = ETRI.common_params\n",
        "  best_param_dict['S2'] = ETRI.common_params\n",
        "  best_param_dict['S3'] = ETRI.common_params\n",
        "  best_param_dict['Q1'] = ETRI.common_params\n",
        "  best_param_dict['Q2'] = ETRI.common_params\n",
        "\n",
        "  # RUN MODEL\n",
        "  submission_final, oof_result = ETRI.run_basemodel(train, test, valid_ids, best_param_dict, n_splits=5, random_state=seed, S1_balance=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n18bU6WIeMgp"
      },
      "id": "n18bU6WIeMgp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ryv1ucVmeMms"
      },
      "id": "Ryv1ucVmeMms",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tl7LGIO-eMpU"
      },
      "id": "tl7LGIO-eMpU",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DCdoVDoyeM5y"
      },
      "id": "DCdoVDoyeM5y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1BaGFl5eM80"
      },
      "id": "K1BaGFl5eM80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t76nmZAfeM_g"
      },
      "id": "t76nmZAfeM_g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oEWeWGMXeNCw"
      },
      "id": "oEWeWGMXeNCw",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y5AgNtVqeNGO"
      },
      "id": "y5AgNtVqeNGO",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F_w_1K90eNKK"
      },
      "id": "F_w_1K90eNKK",
      "execution_count": 18,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 7210916,
          "sourceId": 12085434,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 4148.996629,
      "end_time": "2025-06-07T13:22:50.273930",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-06-07T12:13:41.277301",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}