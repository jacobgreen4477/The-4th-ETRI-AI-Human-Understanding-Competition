{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobgreen4477/The-4th-ETRI-AI-Human-Understanding-Competition/blob/main/etri_template_vF1_0_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e04b986",
      "metadata": {
        "id": "5e04b986"
      },
      "source": [
        "> title : Ï†ú 4Ìöå ETRI Ìú¥Î®ºÏù¥Ìï¥ Ïù∏Í≥µÏßÄÎä• ÎÖºÎ¨∏Í≤ΩÏßÑÎåÄÌöå <br>\n",
        "> author : hjy,byc <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21389cf1",
      "metadata": {
        "id": "21389cf1"
      },
      "source": [
        "### üì¶ ÎùºÏù¥Î∏åÎü¨Î¶¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "aca4f97e-8957-48ee-8927-9f27e8046cfc",
      "metadata": {
        "id": "aca4f97e-8957-48ee-8927-9f27e8046cfc"
      },
      "outputs": [],
      "source": [
        "! pip install haversine >/dev/null\n",
        "! pip install optuna  >/dev/null\n",
        "! pip install category_encoders >/dev/null\n",
        "! pip install tabpfn  >/dev/null\n",
        "! pip install catboost >/dev/null\n",
        "! pip install torchmetrics >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4f2a25a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "4f2a25a5",
        "outputId": "c5fedf7e-6a84-4131-9b4e-c7167dc97a3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbyc3230\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250611_061002-ycawcume</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/byc3230/etri_lifelog/runs/ycawcume' target=\"_blank\">giddy-sun-115</a></strong> to <a href='https://wandb.ai/byc3230/etri_lifelog' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/byc3230/etri_lifelog' target=\"_blank\">https://wandb.ai/byc3230/etri_lifelog</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/byc3230/etri_lifelog/runs/ycawcume' target=\"_blank\">https://wandb.ai/byc3230/etri_lifelog/runs/ycawcume</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Í∏∞Î≥∏ Î™®Îìà\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import ast\n",
        "import glob\n",
        "import random\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from math import radians, cos, sin, asin, sqrt\n",
        "from functools import reduce\n",
        "from datetime import datetime, timedelta, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Î®∏Ïã†Îü¨Îãù\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import f1_score, roc_auc_score, roc_curve\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
        "from category_encoders import TargetEncoder\n",
        "from lightgbm import LGBMClassifier, log_evaluation, early_stopping\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "import lightgbm as lgb\n",
        "from tabpfn import TabPFNClassifier\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Hugging Face\n",
        "from huggingface_hub import login\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    LlamaTokenizer,\n",
        "    LlamaForCausalLM,\n",
        "    LlamaForSequenceClassification\n",
        ")\n",
        "\n",
        "# PEFT (Parameter-Efficient Fine-Tuning)\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    TaskType\n",
        ")\n",
        "\n",
        "# Evaluation & Utilities\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "# Í∏∞ÌÉÄ\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import tqdm as auto_tqdm  # ÌïÑÏöî Ïãú Íµ¨Î∂Ñ\n",
        "from scipy.stats import entropy\n",
        "from haversine import haversine\n",
        "from io import StringIO\n",
        "import gc\n",
        "\n",
        "# wandb\n",
        "import wandb\n",
        "wandb.login(key=\"5fa8dfb2c5be3c888bfe0101437a8fa22fbdf0e0\")\n",
        "wandb.init(project=\"etri_lifelog\", entity=\"byc3230\")\n",
        "\n",
        "# ÏòµÏÖò\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "pd.set_option('display.max_columns', 999)\n",
        "pd.set_option('display.max_rows', 999)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.float_format', lambda x: '%0.4f' % x)\n",
        "\n",
        "# Í∏∞ÌÉÄ\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "de5b6541",
      "metadata": {
        "id": "de5b6541"
      },
      "outputs": [],
      "source": [
        "string = \"\"\"\n",
        "subject_id,sleep_date\n",
        "id01,2024-07-24\n",
        "id01,2024-08-26\n",
        "id01,2024-08-28\n",
        "id01,2024-08-29\n",
        "id02,2024-08-23\n",
        "id02,2024-09-24\n",
        "id02,2024-09-26\n",
        "id02,2024-09-27\n",
        "id03,2024-08-30\n",
        "id03,2024-09-01\n",
        "id03,2024-09-02\n",
        "id03,2024-09-06\n",
        "id04,2024-09-03\n",
        "id04,2024-10-10\n",
        "id04,2024-10-12\n",
        "id04,2024-10-13\n",
        "id05,2024-10-19\n",
        "id05,2024-10-23\n",
        "id05,2024-10-24\n",
        "id05,2024-10-27\n",
        "id06,2024-07-25\n",
        "id06,2024-07-26\n",
        "id06,2024-07-27\n",
        "id06,2024-07-30\n",
        "id07,2024-07-07\n",
        "id07,2024-08-02\n",
        "id07,2024-08-04\n",
        "id07,2024-08-05\n",
        "id08,2024-08-28\n",
        "id08,2024-08-29\n",
        "id08,2024-08-30\n",
        "id08,2024-09-02\n",
        "id09,2024-08-02\n",
        "id09,2024-08-31\n",
        "id09,2024-09-02\n",
        "id09,2024-09-03\n",
        "id10,2024-08-28\n",
        "id10,2024-08-30\n",
        "id10,2024-08-31\n",
        "id10,2024-09-03\n",
        "\"\"\"\n",
        "\n",
        "# DataFrame ÏÉùÏÑ±\n",
        "valid_ids = pd.read_csv(StringIO(string), sep=',')\n",
        "valid_ids['pk'] = valid_ids['subject_id']+valid_ids['sleep_date']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a6ab82b",
      "metadata": {
        "id": "0a6ab82b"
      },
      "source": [
        "### üì¶ Îç∞Ïù¥ÌÑ∞ ÏùΩÍ∏∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "39d79d1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39d79d1f",
        "outputId": "a2bbd6cc-cf96-49c9-bb9e-bbe43e0ede0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "# train  shape: (450, 237)\n",
            "# test   shape: (250, 237)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/MyDrive/data/ch2025_data_items/share/'\n",
        "\n",
        "train = pd.read_parquet(f'{path}train_63775.parquet')\n",
        "test = pd.read_parquet(f'{path}test_63775.parquet')\n",
        "\n",
        "print('# train  shape:',train.shape)\n",
        "print('# test   shape:',test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5396de95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5396de95",
        "outputId": "b1947fe2-a45d-4dc0-e6c3-bc826a274ff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# drop_features: []\n"
          ]
        }
      ],
      "source": [
        "# drop_features = ['afterwork_max_label','sleeptime_max_label','worktime_max_label']\n",
        "drop_features = ['top_bssid'] # ,'week_type','week_type_lag1'\n",
        "drop_features = [i for i in drop_features if i in train.columns.tolist()]\n",
        "print('# drop_features:',drop_features)\n",
        "train = train.drop(columns=drop_features)\n",
        "test = test.drop(columns=drop_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e31852ee",
      "metadata": {
        "id": "e31852ee"
      },
      "outputs": [],
      "source": [
        "# ---\n",
        "# Ï∂îÏ†ïÏàòÎ©¥Ìö®Ïú®\n",
        "# ---\n",
        "\n",
        "def calculate_sleep_duration_min(sleep_time, wake_time):\n",
        "    \"\"\"\n",
        "    Ï∑®Ïπ® ÏãúÍ∞Å(sleep_time)Í≥º Í∏∞ÏÉÅ ÏãúÍ∞Å(wake_time)ÏùÑ ÏûÖÎ†•Î∞õÏïÑ ÏàòÎ©¥ ÏãúÍ∞Ñ(Î∂Ñ) Î∞òÌôò\n",
        "    Îã®ÏúÑÎäî float ÏãúÍ∞Ñ (Ïòà: 23.5, 6.25)\n",
        "    \"\"\"\n",
        "    if pd.isna(sleep_time) or pd.isna(wake_time):\n",
        "        return None\n",
        "    if wake_time < sleep_time:\n",
        "        wake_time += 24  # ÏûêÏ†ï ÎÑòÍ∏¥ Í≤ΩÏö∞ Î≥¥Ï†ï\n",
        "    duration = (wake_time - sleep_time) * 60\n",
        "    return round(duration)\n",
        "\n",
        "train['Î∂àÎÅàÏãúÍ∞ÑÎ∂ÄÌÑ∞Í∏∞ÏÉÅÏãúÍ∞Ñ'] = train.apply(lambda x: calculate_sleep_duration_min(x['lights_off_time'],x['wake_time']),axis=1)\n",
        "test['Î∂àÎÅàÏãúÍ∞ÑÎ∂ÄÌÑ∞Í∏∞ÏÉÅÏãúÍ∞Ñ'] = test.apply(lambda x: calculate_sleep_duration_min(x['lights_off_time'],x['wake_time']),axis=1)\n",
        "\n",
        "train['Ï∂îÏ†ïÏàòÎ©¥Ìö®Ïú®'] = train['Î∂àÎÅàÏãúÍ∞ÑÎ∂ÄÌÑ∞Í∏∞ÏÉÅÏãúÍ∞Ñ']/train['sleep_duration_min']\n",
        "test['Ï∂îÏ†ïÏàòÎ©¥Ìö®Ïú®'] = test['Î∂àÎÅàÏãúÍ∞ÑÎ∂ÄÌÑ∞Í∏∞ÏÉÅÏãúÍ∞Ñ']/test['sleep_duration_min']\n",
        "\n",
        "# Ïù¥ÏÉÅÍ∞í Ï†úÍ±∞\n",
        "train['Ï∂îÏ†ïÏàòÎ©¥Ìö®Ïú®'] = np.where(train['Ï∂îÏ†ïÏàòÎ©¥Ìö®Ïú®']<-5,np.nan,train['Ï∂îÏ†ïÏàòÎ©¥Ìö®Ïú®'])\n",
        "test['Ï∂îÏ†ïÏàòÎ©¥Ìö®Ïú®'] = np.where(test['Ï∂îÏ†ïÏàòÎ©¥Ìö®Ïú®']<-5,np.nan,test['Ï∂îÏ†ïÏàòÎ©¥Ìö®Ïú®'])\n",
        "train['Ï∂îÏ†ïÏàòÎ©¥Ìö®Ïú®'] = np.where(train['Ï∂îÏ†ïÏàòÎ©¥Ìö®Ïú®']>5,np.nan,train['Ï∂îÏ†ïÏàòÎ©¥Ìö®Ïú®'])\n",
        "test['Ï∂îÏ†ïÏàòÎ©¥Ìö®Ïú®'] = np.where(test['Ï∂îÏ†ïÏàòÎ©¥Ìö®Ïú®']>55,np.nan,test['Ï∂îÏ†ïÏàòÎ©¥Ìö®Ïú®'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a50b8b13",
      "metadata": {
        "id": "a50b8b13"
      },
      "outputs": [],
      "source": [
        "# ÏöîÏùº Ïª¨Îüº Ï∂îÍ∞Ä (Ïòà: ÏõîÏöîÏùº, ÌôîÏöîÏùº, ...)\n",
        "train['lifelog_date'] = pd.to_datetime(train['lifelog_date'])\n",
        "test['lifelog_date'] = pd.to_datetime(test['lifelog_date'])\n",
        "\n",
        "# ÏöîÏùº\n",
        "weekday_map = {\n",
        "    0: 'ÏõîÏöîÏùº', 1: 'ÌôîÏöîÏùº', 2: 'ÏàòÏöîÏùº', 3: 'Î™©ÏöîÏùº',\n",
        "    4: 'Í∏àÏöîÏùº', 5: 'ÌÜ†ÏöîÏùº', 6: 'ÏùºÏöîÏùº'\n",
        "}\n",
        "train['weekday'] = train['lifelog_date'].dt.dayofweek.map(weekday_map)\n",
        "test['weekday'] = test['lifelog_date'].dt.dayofweek.map(weekday_map)\n",
        "\n",
        "# Ïõî\n",
        "train['month'] = train['lifelog_date'].dt.month\n",
        "test['month'] = test['lifelog_date'].dt.month\n",
        "\n",
        "# weekend\n",
        "train['weekend'] = np.where(train['weekday'].isin(['ÌÜ†ÏöîÏùº','ÏùºÏöîÏùº']),1,0)\n",
        "test['weekend'] = np.where(test['weekday'].isin(['ÌÜ†ÏöîÏùº','ÏùºÏöîÏùº']),1,0)\n",
        "\n",
        "# Í≥µÌú¥Ïùº\n",
        "Í≥µÌú¥Ïùº = [\n",
        "     '2024-08-15'\n",
        "    ,'2024-09-16'\n",
        "    ,'2024-09-17'\n",
        "    ,'2024-09-18'\n",
        "    ,'2024-10-03'\n",
        "    ,'2024-10-09'\n",
        "]\n",
        "train['Í≥µÌú¥Ïùº'] = np.where(train['lifelog_date'].isin(Í≥µÌú¥Ïùº),1,0)\n",
        "test['Í≥µÌú¥Ïùº'] = np.where(test['lifelog_date'].isin(Í≥µÌú¥Ïùº),1,0)\n",
        "\n",
        "# Ï£ºÎßê + Í≥µÌú¥Ïùº Î¨∂Ïñ¥Ï£ºÍ∏∞\n",
        "train['weekend_holilday'] = np.where( ((train['weekend']==0) & (train['Í≥µÌú¥Ïùº']==1)), 1, train['weekend'])\n",
        "test['weekend_holilday'] = np.where( ((test['weekend']==0) & (test['Í≥µÌú¥Ïùº']==1)), 1, test['weekend'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "24228ae9",
      "metadata": {
        "id": "24228ae9"
      },
      "outputs": [],
      "source": [
        "def add_prev_day_flag(df):\n",
        "    df = df.copy()\n",
        "    df['lifelog_date'] = pd.to_datetime(df['lifelog_date'])\n",
        "\n",
        "    # Í∞Å subject_idÎ≥ÑÎ°ú Ï†ÑÎÇ† ÎÇ†Ïßú ÎßåÎì§Í∏∞\n",
        "    df['prev_day'] = df['lifelog_date'] - pd.Timedelta(days=1)\n",
        "\n",
        "    # subject_id + ÎÇ†Ïßú Í∏∞Ï§ÄÏúºÎ°ú ÏõêÎ≥∏ ÌÇ§ Íµ¨ÏÑ±\n",
        "    key_set = set(zip(df['subject_id'], df['lifelog_date']))\n",
        "\n",
        "    # Ï†ÑÎÇ† Îç∞Ïù¥ÌÑ∞Í∞Ä Ï°¥Ïû¨ÌïòÎ©¥ 1, ÏóÜÏúºÎ©¥ 0\n",
        "    df['has_prev_day_data'] = df[['subject_id', 'prev_day']].apply(\n",
        "        lambda row: 1 if (row['subject_id'], row['prev_day']) in key_set else 0, axis=1\n",
        "    )\n",
        "\n",
        "    return df.drop(columns=['prev_day'])\n",
        "\n",
        "train = add_prev_day_flag(train)\n",
        "test = add_prev_day_flag(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "938b961c",
      "metadata": {
        "id": "938b961c"
      },
      "outputs": [],
      "source": [
        "# ---\n",
        "# Ï∂îÏ†ïÌú¥Í∞Ä\n",
        "# ---\n",
        "\n",
        "def rule_based_sum(x):\n",
        "    rules = (\n",
        "        # (x['sleep_duration_min'] > (x['avg_sleep_duration']+30))\n",
        "          (x['sleep_duration_min'] > (x['avg_sleep_duration']+60))\n",
        "        & (x['week_type'] == 'weekday')\n",
        "        # & (x['month'].isin([7,8]))\n",
        "    )\n",
        "    return rules\n",
        "\n",
        "train['vacation'] = train.groupby('subject_id').apply(rule_based_sum).reset_index(level=0, drop=True).astype(int)\n",
        "test['vacation'] = test.groupby('subject_id').apply(rule_based_sum).reset_index(level=0, drop=True).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cb9fd81b",
      "metadata": {
        "id": "cb9fd81b"
      },
      "outputs": [],
      "source": [
        "# Ïà´ÏûêÌòï Ïª¨ÎüºÎßå ÏÑ†ÌÉùÌï¥ÏÑú Í≤∞Ï∏°Í∞í -1Î°ú Ï±ÑÏö∞Í∏∞\n",
        "train[train.select_dtypes(include='number').columns] = train.select_dtypes(include='number').fillna(-1)\n",
        "test[test.select_dtypes(include='number').columns] = test.select_dtypes(include='number').fillna(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "qvMT9FPwoz-x",
      "metadata": {
        "id": "qvMT9FPwoz-x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "OUzhBfxNo3J0",
      "metadata": {
        "id": "OUzhBfxNo3J0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "gTVVBAHko3bs",
      "metadata": {
        "id": "gTVVBAHko3bs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "01bd961f",
      "metadata": {
        "id": "01bd961f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "S_syDjeiMnUN",
      "metadata": {
        "id": "S_syDjeiMnUN"
      },
      "source": [
        "### ============================"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HtiE52IWK_nd",
      "metadata": {
        "id": "HtiE52IWK_nd"
      },
      "source": [
        "### run_basemodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2kbYLCFJLpQ5",
      "metadata": {
        "id": "2kbYLCFJLpQ5"
      },
      "outputs": [],
      "source": [
        "def run_basemodel(train, test, valid_ids, common_params, n_splits, random_state=42):\n",
        "\n",
        "    lgb_A = 0.3\n",
        "    xgb_B = 0.3\n",
        "    tab_C = 0.3\n",
        "\n",
        "    train_df = train.copy()\n",
        "    test_df = test.copy()\n",
        "\n",
        "    submission_final = test_df[['subject_id', 'sleep_date', 'lifelog_date']].copy()\n",
        "    submission_final['lifelog_date'] = pd.to_datetime(submission_final['lifelog_date']).dt.date\n",
        "\n",
        "    # ÌÉÄÍ≤ü\n",
        "    targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3']\n",
        "    targets_binary_name = ['Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà','Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú','Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§','ÏàòÎ©¥Ìö®Ïú®','ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ']\n",
        "    target_multiclass = 'S1'\n",
        "    all_targets = targets_binary + [target_multiclass]\n",
        "\n",
        "    # ÎÖ∏Ïù¥Ï¶à ÏàòÏ§Ä ÏÑ§Ï†ï\n",
        "    def add_noise(series, noise_level, seed=3):\n",
        "        rng = np.random.default_rng(seed)\n",
        "        return series * (1 + noise_level * rng.standard_normal(len(series)))\n",
        "\n",
        "    noise_level = 0.015  # ÌïÑÏöîÏóê Îî∞Îùº Ï°∞Ï†ï\n",
        "\n",
        "    # ÌÉÄÍ≤üÏù∏ÏΩîÎî©\n",
        "    for tgt in all_targets:\n",
        "\n",
        "      encoder_feats = ['subject_id','month','weekend'] # 'weekday', 'subject_id','month','weekend'\n",
        "\n",
        "      #### ÌÉÄÍ≤üÏù∏ÏΩîÎî©1\n",
        "\n",
        "      subject_mean = train_df.groupby(encoder_feats)[tgt].mean().rename(f'{tgt}_te')\n",
        "      train_df = train_df.merge(subject_mean, on=encoder_feats, how='left')\n",
        "      test_df = test_df.merge(subject_mean, on=encoder_feats, how='left')\n",
        "      global_mean = train_df[tgt].mean()\n",
        "      test_df[f'{tgt}_te'] = test_df[f'{tgt}_te'].fillna(global_mean)\n",
        "\n",
        "      # ÎÖ∏Ïù¥Ï¶à Ï∂îÍ∞Ä\n",
        "      train_df[f'{tgt}_te'] = add_noise(train_df[f'{tgt}_te'], noise_level)\n",
        "      test_df[f'{tgt}_te'] = add_noise(test_df[f'{tgt}_te'], noise_level)\n",
        "\n",
        "      #### ÌÉÄÍ≤üÏù∏ÏΩîÎî©2\n",
        "\n",
        "      # ÏÉàÎ°úÏö¥ Î≤îÏ£ºÌòï Ïó¥ ÏÉùÏÑ±\n",
        "      train_df['TMP'] = train_df[encoder_feats].applymap(str).apply(lambda x: ''.join(x) ,axis=1)\n",
        "      test_df['TMP'] = test_df[encoder_feats].applymap(str).apply(lambda x: ''.join(x) ,axis=1)\n",
        "\n",
        "      # Ïù∏ÏΩîÎçî\n",
        "      encoder = TargetEncoder(cols=['TMP'], smoothing=300) # 40\n",
        "      encoder.fit(train_df[['TMP']], train_df[tgt])\n",
        "\n",
        "      # Ïù∏ÏΩîÎî© Í≤∞Í≥ºÎ•º ÏÉàÎ°úÏö¥ Ïó¥Ïóê Ï†ÄÏû•\n",
        "      train_df[f'{tgt}_te2'] = encoder.transform(train_df[['TMP']])\n",
        "      test_df[f'{tgt}_te2'] = encoder.transform(test_df[['TMP']])\n",
        "\n",
        "      # ÎÖ∏Ïù¥Ï¶à Ï∂îÍ∞Ä\n",
        "      train_df[f'{tgt}_te2'] = add_noise(train_df[f'{tgt}_te2'], noise_level)\n",
        "      test_df[f'{tgt}_te2'] = add_noise(test_df[f'{tgt}_te2'], noise_level)\n",
        "\n",
        "      # Î∂àÌïÑÏöîÌïú Î≥ÄÏàò Ï†úÍ±∞\n",
        "      train_df = train_df.drop(columns=['TMP'])\n",
        "      test_df = test_df.drop(columns=['TMP'])\n",
        "\n",
        "\n",
        "    # Ïù∏ÏΩîÎî©\n",
        "    PK = ['sleep_date', 'lifelog_date', 'subject_id']\n",
        "    encoder = LabelEncoder()\n",
        "    categorical_features = [i for i in train_df.select_dtypes(include=['object', 'category']).columns if i not in PK+['pk']]\n",
        "    for col in categorical_features:\n",
        "        print(col)\n",
        "        train_df[col] = encoder.fit_transform(train_df[col])\n",
        "        test_df[col] = encoder.fit_transform(test_df[col])\n",
        "\n",
        "    # X\n",
        "    X = train_df.drop(columns=PK + all_targets)\n",
        "    test_X = test_df.drop(columns=PK + all_targets)\n",
        "    print(f'# X shape: {X.shape}')\n",
        "    print(f'# test_X shape: {test_X.shape}')\n",
        "\n",
        "    print('\\n STEP1: Ïã§Ìóò Í≤∞Í≥º ÌôïÏù∏')\n",
        "    print(\"=============== Validation Results ==============\")\n",
        "    total_avg_f1s = []\n",
        "    val_f1 = []\n",
        "    binary_val_preds = {}\n",
        "    multiclass_val_preds = {}\n",
        "    binary_test_preds = {}\n",
        "    multiclass_test_preds = {}\n",
        "    test_preds = {}\n",
        "\n",
        "    # Find optimal weights\n",
        "    best_weights = []\n",
        "    best_scores = []\n",
        "\n",
        "    for col in targets_binary:\n",
        "        # binary\n",
        "        y = train_df[col]\n",
        "\n",
        "        valid_ids['pk'] = valid_ids['subject_id']+valid_ids['sleep_date']\n",
        "        train_df['pk'] = train_df['subject_id']+train_df['sleep_date']\n",
        "\n",
        "        X_valid = train_df.loc[train_df['pk'].isin(valid_ids['pk']),X.columns.tolist()].reset_index(drop=True).copy()\n",
        "        X_train = train_df.loc[~train_df['pk'].isin(valid_ids['pk']),X.columns.tolist()].reset_index(drop=True).copy()\n",
        "        y_valid = train_df.loc[train_df['pk'].isin(valid_ids['pk']),y.name].reset_index(drop=True).copy()\n",
        "        y_train = train_df.loc[~train_df['pk'].isin(valid_ids['pk']),y.name].reset_index(drop=True).copy()\n",
        "\n",
        "        # Get parameters for both models\n",
        "        lgb_params = common_params[col].copy()\n",
        "        lgb_params['random_state'] = random_state\n",
        "\n",
        "        xgb_params = {\n",
        "            'n_estimators': 1000,\n",
        "            'learning_rate': 0.01,\n",
        "            'max_depth': 6,\n",
        "            'min_child_weight': 1,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'random_state': random_state\n",
        "        }\n",
        "\n",
        "        # Train LightGBM\n",
        "        lgb_model = LGBMClassifier(**lgb_params)\n",
        "        lgb_model.fit(X_train, y_train)\n",
        "\n",
        "        # Train XGBoost\n",
        "        xgb_model = XGBClassifier(**xgb_params)\n",
        "        xgb_model.fit(X_train, y_train)\n",
        "\n",
        "        tabpfn_params = {\n",
        "            'device': 'cuda'\n",
        "        }\n",
        "\n",
        "        # Train TabPFN\n",
        "        tabpfn_model = TabPFNClassifier(**tabpfn_params)\n",
        "        tabpfn_model.fit(X_train, y_train)\n",
        "        tab_pred_valid = tabpfn_model.predict_proba(X_valid.values)[:, 1]\n",
        "\n",
        "        lgb_pred_valid = lgb_model.predict_proba(X_valid)[:, 1]\n",
        "        xgb_pred_valid = xgb_model.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "        pred_valid = (lgb_A * lgb_pred_valid + xgb_B * xgb_pred_valid + tab_C * tab_pred_valid > 0.5).astype(int)\n",
        "\n",
        "        f1 = f1_score(y_valid, pred_valid, average='macro')\n",
        "        val_f1.append(f1)\n",
        "\n",
        "        # Store predictions\n",
        "        binary_val_preds[col] = {\n",
        "            'lgb': lgb_pred_valid,\n",
        "            'xgb': xgb_pred_valid,\n",
        "            'tab': tab_pred_valid,\n",
        "            'true': y_valid\n",
        "        }\n",
        "\n",
        "    # multiclass\n",
        "    y = train_df[target_multiclass]\n",
        "    X_valid = train_df.loc[train_df['pk'].isin(valid_ids['pk']),X.columns.tolist()].reset_index(drop=True).copy()\n",
        "    X_train = train_df.loc[~train_df['pk'].isin(valid_ids['pk']),X.columns.tolist()].reset_index(drop=True).copy()\n",
        "    y_valid = train_df.loc[train_df['pk'].isin(valid_ids['pk']),y.name].reset_index(drop=True).copy()\n",
        "    y_train = train_df.loc[~train_df['pk'].isin(valid_ids['pk']),y.name].reset_index(drop=True).copy()\n",
        "\n",
        "    # Get parameters for both models\n",
        "    lgb_params = common_params['S1'].copy()\n",
        "    lgb_params['random_state'] = random_state\n",
        "\n",
        "    xgb_params = {\n",
        "        'n_estimators': 1000,\n",
        "        'learning_rate': 0.01,\n",
        "        'max_depth': 6,\n",
        "        'min_child_weight': 1,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'random_state': random_state\n",
        "    }\n",
        "\n",
        "    # ÌÅ¥ÎûòÏä§ weight Í≥ÑÏÇ∞\n",
        "    classes = np.unique(y_train)\n",
        "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
        "    class_weights = dict(zip(classes, weights))\n",
        "\n",
        "    # Í∞Å ÏÉòÌîåÏóê ÎåÄÌï¥ weight Îß§Ìïë\n",
        "    w_train = pd.Series(y_train).map(class_weights)\n",
        "    w_train = compute_sample_weight(class_weight='balanced', y=y_train)\n",
        "\n",
        "    # Train LightGBM\n",
        "    lgb_model = LGBMClassifier(**lgb_params, objective='multiclass', num_class=3)\n",
        "    lgb_model.fit(X_train, y_train, sample_weight=w_train)\n",
        "\n",
        "    # Train XGBoost\n",
        "    xgb_model = XGBClassifier(**xgb_params, objective='multi:softmax', num_class=3)\n",
        "    xgb_model.fit(X_train, y_train,sample_weight=w_train)\n",
        "\n",
        "    tabpfn_params = {\n",
        "        'device': 'cuda'\n",
        "    }\n",
        "\n",
        "    # Train TabPFN\n",
        "    tabpfn_model = TabPFNClassifier(**tabpfn_params)\n",
        "    tabpfn_model.fit(X_train, y_train)\n",
        "\n",
        "    # Get predictions and ensemble\n",
        "    lgb_pred_valid = lgb_model.predict_proba(X_valid)\n",
        "    xgb_pred_valid = xgb_model.predict_proba(X_valid)\n",
        "    tab_pred_valid = tabpfn_model.predict_proba(X_valid.values)\n",
        "\n",
        "    pred_valid = np.argmax(lgb_A * lgb_pred_valid + xgb_B * xgb_pred_valid + tab_C * tab_pred_valid, axis=1)\n",
        "\n",
        "    f1 = f1_score(y_valid, pred_valid, average='macro')\n",
        "    val_f1.append(f1)\n",
        "\n",
        "    multiclass_val_preds = {\n",
        "        'lgb': lgb_pred_valid,\n",
        "        'xgb': xgb_pred_valid,\n",
        "        'tab': tab_pred_valid,\n",
        "        'true': y_valid\n",
        "    }\n",
        "\n",
        "    # Generate all possible weight combinations that sum to 1\n",
        "    step = 0.1\n",
        "    for lgb_A in np.arange(0, 1.1, step):\n",
        "        for xgb_B in np.arange(0, 1.1 - lgb_A, step):\n",
        "            for tab_C in np.arange(0, 1.1 - lgb_A - xgb_B, step):\n",
        "                TOT = 1 - (lgb_A + xgb_B + tab_C)\n",
        "                if TOT >= 0:\n",
        "                    weights = (lgb_A, xgb_B, tab_C)\n",
        "                    val_scores = []\n",
        "\n",
        "                    # Binary targets\n",
        "                    for col in targets_binary:\n",
        "                        preds = binary_val_preds[col]\n",
        "                        ensemble_pred = (lgb_A * preds['lgb'] + xgb_B * preds['xgb'] + tab_C * preds['tab'] > 0.5).astype(int)\n",
        "                        f1 = f1_score(preds['true'], ensemble_pred, average='macro')\n",
        "                        val_scores.append(f1)\n",
        "                        # print(f\" Validation Score {col}:{f1:.4f}\")\n",
        "\n",
        "                    # Multiclass target\n",
        "                    preds = multiclass_val_preds\n",
        "                    ensemble_pred = np.argmax(lgb_A * preds['lgb'] + xgb_B * preds['xgb'] + tab_C * preds['tab'] , axis=1)\n",
        "                    f1 = f1_score(preds['true'], ensemble_pred, average='macro')\n",
        "                    # print(f\" Validation Score S1:{f1:.4f}\")\n",
        "                    val_scores.append(f1)\n",
        "\n",
        "                    avg_score = np.mean(val_scores)\n",
        "                    best_weights.append(weights)\n",
        "                    best_scores.append(avg_score)\n",
        "\n",
        "                    # print(f\"Average Validation Score: {avg_score:.4f}\")\n",
        "\n",
        "    # Sort results and get top 3\n",
        "    sorted_indices = np.argsort(best_scores)[::-1]\n",
        "    top_3_weights = [best_weights[i] for i in sorted_indices]\n",
        "    top_3_scores = [best_scores[i] for i in sorted_indices]\n",
        "\n",
        "    # print(\"\\nTop All Weight Combinations:\")\n",
        "    # for i, (weights, score) in enumerate(zip(top_3_weights, top_3_scores)):\n",
        "    #     print(f\"Rank {i+1}: lgb_A={weights[0]:.1f}, xgb_B={weights[1]:.1f}, tab_C={weights[2]:.1f} - Score: {score:.4f}\")\n",
        "\n",
        "    avg_f1 = np.mean(val_f1)\n",
        "    total_avg_f1s.append(avg_f1)\n",
        "    detail = \" \".join([f\"{name}({tname}):{score:.4f}\" for name, tname, score in zip(targets_binary + [target_multiclass], targets_binary_name + ['S1'], val_f1)])\n",
        "    print(f\" ÌèâÍ∑† F1: {avg_f1:.4f} / [ÏÉÅÏÑ∏] {detail}\")\n",
        "    print(f\"# Ï†ÑÏ≤¥ ÌèâÍ∑† F1: {np.mean(total_avg_f1s):.4f}\")\n",
        "    print(\"================================================\")\n",
        "\n",
        "    # modoling with 100% train & no valid\n",
        "    print('\\n STEP2: Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Î°ú Î™®Îç∏ Ïû¨ÌïôÏäµ')\n",
        "    print(\"====== modeling with 100% train & no valid =====\")\n",
        "\n",
        "    # binary\n",
        "    binary_preds = {}\n",
        "    binary_preds_proba = {}\n",
        "    for col in targets_binary:\n",
        "        # Get parameters for both models\n",
        "        lgb_params = common_params[col].copy()\n",
        "        lgb_params['random_state'] = random_state\n",
        "\n",
        "        xgb_params = {\n",
        "            'n_estimators': 1000,\n",
        "            'learning_rate': 0.01,\n",
        "            'max_depth': 6,\n",
        "            'min_child_weight': 1,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'random_state': random_state\n",
        "        }\n",
        "\n",
        "        y = train_df[col]\n",
        "\n",
        "        is_multiclass = False\n",
        "\n",
        "        # Train LightGBM\n",
        "        lgb_model = LGBMClassifier(**lgb_params)\n",
        "        lgb_model.fit(X, y)\n",
        "\n",
        "        # Train XGBoost\n",
        "        xgb_model = XGBClassifier(**xgb_params)\n",
        "        xgb_model.fit(X, y)\n",
        "\n",
        "        tabpfn_params = {\n",
        "            'device': 'cuda'\n",
        "        }\n",
        "\n",
        "        # Train TabPFN\n",
        "        tabpfn_model = TabPFNClassifier(**tabpfn_params)\n",
        "        tabpfn_model.fit(X, y)\n",
        "\n",
        "        tab_pred = tabpfn_model.predict_proba(test_X)[:, 1]\n",
        "        lgb_pred = lgb_model.predict_proba(test_X)[:, 1]\n",
        "        xgb_pred = xgb_model.predict_proba(test_X)[:, 1]\n",
        "\n",
        "        binary_preds[col] = (lgb_A * lgb_pred + xgb_B * xgb_pred + tab_C * tab_pred > 0.5).astype(int)\n",
        "\n",
        "        # Store predictions\n",
        "        binary_test_preds[col] = {\n",
        "            'lgb': lgb_pred,\n",
        "            'xgb': xgb_pred,\n",
        "            'tab': tab_pred\n",
        "        }\n",
        "\n",
        "        # Feature importance (using LightGBM's importance)\n",
        "        fi_df = pd.DataFrame({'feature': X.columns, 'importance': lgb_model.feature_importances_})\n",
        "        top10 = fi_df.sort_values(by='importance', ascending=False).head(10)\n",
        "        feat_str = \", \".join([f\"{row['feature']}({int(row['importance'])})\" for _, row in top10.iterrows()])\n",
        "        print(f\"[{col}] {feat_str}\")\n",
        "\n",
        "    # multiclass\n",
        "    y = train_df['S1']\n",
        "\n",
        "    # Get parameters for both models\n",
        "    lgb_params = common_params['S1'].copy()\n",
        "    lgb_params['random_state'] = random_state\n",
        "\n",
        "    xgb_params = {\n",
        "        'n_estimators': 1000,\n",
        "        'learning_rate': 0.01,\n",
        "        'max_depth': 6,\n",
        "        'min_child_weight': 1,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'random_state': random_state\n",
        "    }\n",
        "\n",
        "    # ÌÅ¥ÎûòÏä§ weight Í≥ÑÏÇ∞\n",
        "    classes = np.unique(y)\n",
        "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y)\n",
        "    class_weights = dict(zip(classes, weights))\n",
        "\n",
        "    # Í∞Å ÏÉòÌîåÏóê ÎåÄÌï¥ weight Îß§Ìïë\n",
        "    w_train = pd.Series(y).map(class_weights)\n",
        "    w_train = compute_sample_weight(class_weight='balanced', y=y)\n",
        "\n",
        "    is_multiclass = True\n",
        "\n",
        "    # Train LightGBM\n",
        "    lgb_model = LGBMClassifier(**lgb_params, objective='multiclass', num_class=3)\n",
        "    lgb_model.fit(X, y, sample_weight=w_train)\n",
        "\n",
        "    # Train XGBoost\n",
        "    xgb_model = XGBClassifier(**xgb_params, objective='multi:softmax', num_class=3)\n",
        "    xgb_model.fit(X, y, sample_weight=w_train)\n",
        "\n",
        "    tabpfn_params = {\n",
        "        'device': 'cuda'\n",
        "    }\n",
        "\n",
        "     # Train TabPFN\n",
        "    tabpfn_model = TabPFNClassifier(**tabpfn_params)\n",
        "    tabpfn_model.fit(X, y)\n",
        "\n",
        "    # Get predictions and ensemble\n",
        "    lgb_pred = lgb_model.predict_proba(test_X)\n",
        "    xgb_pred = xgb_model.predict_proba(test_X)\n",
        "    tab_pred = tabpfn_model.predict_proba(test_X)\n",
        "\n",
        "    multiclass_test_preds = {\n",
        "        'lgb': lgb_pred,\n",
        "        'xgb': xgb_pred,\n",
        "        'tab': tab_pred\n",
        "    }\n",
        "\n",
        "    multiclass_pred = np.argmax(lgb_A * lgb_pred + xgb_B * xgb_pred + tab_C * tab_pred, axis=1)\n",
        "    multiclass_pred_proba = lgb_A * lgb_pred + xgb_B * xgb_pred + tab_C * tab_pred\n",
        "\n",
        "    # Feature importance\n",
        "    fi_df = pd.DataFrame({'feature': X.columns, 'importance': lgb_model.feature_importances_})\n",
        "    top10 = fi_df.sort_values(by='importance', ascending=False).head(10)\n",
        "    feat_str = \", \".join([f\"{row['feature']}({int(row['importance'])})\" for _, row in top10.iterrows()])\n",
        "    print(f\"[S1] {feat_str}\")\n",
        "\n",
        "    # ÏòàÏ∏° Ï†ÄÏû•\n",
        "    submission_final['S1'] = multiclass_pred\n",
        "    for col in targets_binary:\n",
        "      submission_final[col] = binary_preds[col]\n",
        "    submission_final = submission_final[['subject_id', 'sleep_date', 'lifelog_date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]\n",
        "    fname = f\"submission_{np.mean(total_avg_f1s)}.csv\"\n",
        "    submission_final.to_csv(fname, index=False)\n",
        "    print(f\"# {fname} Ï†ÄÏû• ÏôÑÎ£å\")\n",
        "    print(f\"# submission shape:{submission_final.shape}\")\n",
        "    print(\"================================================\")\n",
        "\n",
        "    # Top 10 Weight Combinations\n",
        "    print(\"\\nTop 10 Weight Combinations:\")\n",
        "    for i, (weights, score) in enumerate(zip(top_3_weights[:10], top_3_scores[:10])):\n",
        "        print(f\"Rank {i+1}: lgb_A={weights[0]:.1f}, xgb_B={weights[1]:.1f}, tab_C={weights[2]:.1f} - Score: {score:.4f}\")\n",
        "\n",
        "        # Generate submission with these weights\n",
        "        lgb_A, xgb_B, tab_C = weights\n",
        "\n",
        "        # Binary predictions\n",
        "        for col in targets_binary:\n",
        "            preds = binary_test_preds[col]\n",
        "            ensemble_pred = (lgb_A * preds['lgb'] + xgb_B * preds['xgb'] + tab_C * preds['tab'] > 0.5).astype(int)\n",
        "            submission_final[col] = ensemble_pred\n",
        "\n",
        "        # Multiclass prediction\n",
        "        preds = multiclass_test_preds\n",
        "        ensemble_pred = np.argmax(lgb_A * preds['lgb'] + xgb_B * preds['xgb'] + tab_C * preds['tab'], axis=1)\n",
        "        submission_final['S1'] = ensemble_pred\n",
        "\n",
        "        fname = f\"submission_top{i+1}_{score:.4f}.csv\"\n",
        "        submission_final.to_csv(fname, index=False)\n",
        "        print(f\"Saved submission to {fname}\")\n",
        "\n",
        "    # Use the best weights for final submission\n",
        "    best_weights = top_3_weights[0]\n",
        "    lgb_A, xgb_B, tab_C = best_weights\n",
        "\n",
        "    # Î™®Îç∏Î≥Ñ ÏòàÏ∏°Í≤∞Í≥º ÎπÑÏú® ÎπÑÍµê\n",
        "    a11 = train_df[['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']].sum()\n",
        "    a13 = train_df[['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']].apply(len)\n",
        "    a12 = train_df[['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']].mean()\n",
        "    a21 = submission_final[['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']].sum()\n",
        "    a23 = submission_final[['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']].apply(len)\n",
        "    a22 = submission_final[['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']].mean()\n",
        "    result = pd.concat([a11, a13, a12, a21, a23, a22], axis=1)\n",
        "    result.columns = ['ÌïôÏäµsum','ÌïôÏäµlen','ÌïôÏäµmean','ÌÖåÏä§Ìä∏sum','ÌÖåÏä§Ìä∏len','ÌÖåÏä§Ìä∏mean']\n",
        "    print('\\n STEP3: ÏòàÏ∏°Í≤∞Í≥º ÎπÑÍµêÌëú')\n",
        "    display(result)\n",
        "    oof_result = []\n",
        "    return submission_final, oof_result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### seed"
      ],
      "metadata": {
        "id": "6Xslr-kVG1o3"
      },
      "id": "6Xslr-kVG1o3"
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(1)"
      ],
      "metadata": {
        "id": "Ly49bIaKG0-O"
      },
      "id": "Ly49bIaKG0-O",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ============================"
      ],
      "metadata": {
        "id": "lj8uKFxaKGbI"
      },
      "id": "lj8uKFxaKGbI"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7vvLSkwooZ0h",
      "metadata": {
        "id": "7vvLSkwooZ0h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pr98Y0D0KIPz"
      },
      "id": "Pr98Y0D0KIPz",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "U36LFTRioZ-g",
      "metadata": {
        "id": "U36LFTRioZ-g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "sg6Eiq-vOTr1",
      "metadata": {
        "id": "sg6Eiq-vOTr1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "HwoUMZO4LMb1",
      "metadata": {
        "id": "HwoUMZO4LMb1"
      },
      "source": [
        "### üì¶ Î™®Îç∏ ÌïôÏäµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bf3543b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bf3543b7",
        "outputId": "26211a08-3404-479e-f7f9-29520d37de26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "light_week_type_lag1\n",
            "weekday\n",
            "week_type\n",
            "week_type_lag1\n",
            "activehour_top_bssid\n",
            "beforebed_top_bssid\n",
            "# X shape: (450, 247)\n",
            "# test_X shape: (250, 247)\n",
            "\n",
            " STEP1: Ïã§Ìóò Í≤∞Í≥º ÌôïÏù∏\n",
            "=============== Validation Results ==============\n",
            " ÌèâÍ∑† F1: 0.6441 / [ÏÉÅÏÑ∏] Q1(Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà):0.7234 Q2(Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú):0.8157 Q3(Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§):0.6366 S2(ÏàòÎ©¥Ìö®Ïú®):0.5489 S3(ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ):0.7234 S1(S1):0.4163\n",
            "# Ï†ÑÏ≤¥ ÌèâÍ∑† F1: 0.6441\n",
            "================================================\n",
            "\n",
            " STEP2: Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Î°ú Î™®Îç∏ Ïû¨ÌïôÏäµ\n",
            "====== modeling with 100% train & no valid =====\n",
            "[Q1] beforebed_ÌÜµÌôî_time(1144), Q1_te2(477), wake_time_ratio(453), wake_time_diff(322), mlight_first_wakeup_minutes(310), Q1_te(290), lights_off_time(282), sleep_duration_ratio(214), active_hour_mean_speed(199), activehour_total_screen_time(181)\n",
            "[Q2] Q2_te(1990), Q2_te2(1827), activehour_total_screen_time(293), beforebed_unique_bssid_count(176), wake_time_lag1(163), light_rolling_wake_time_2d(161), activehour_screen_time_vs_avg_pct(144), beforebed_max_rssi(140), beforebed_top_bssid_count(139), active_hour_std_hr(134)\n",
            "[Q3] Q3_te2(2299), light_sleep_time_lag2(330), mlight_first_wakeup_minutes(288), rolling_sleep_time_3d(275), light_rolling_sleep_duration_3d(218), Q3_te(214), beforebed_scan_count(211), active_hour_distance_x(196), activehour_ÌÜµÌôî_time(181), walking_minutes(169)\n",
            "[S2] S2_te2(433), S2_te(422), light_sleep_time_lag1(198), work_hour_unknown_ratio(190), m_activity@240min@std@12h00m(176), beforebed_strong_signal_ratio(154), light_rolling_wake_time_2d(151), free_hour_rssi_mean(150), activehour_Ï†ÑÌôî_time(147), sleep_hour_mean_speed(136)\n",
            "[S3] S3_te(2612), S3_te2(336), beforebed_Î©îÏã†Ï†Ä_time(252), light_wake_time_diff(216), sleep_time_diff_lag1(209), light_sleep_time_lag2(199), m_activity_met@240min@sum@16h00m(156), free_hour_rssi_max(150), light_weekday_avg_sleep(137), Î∂àÎÅàÏãúÍ∞ÑÎ∂ÄÌÑ∞Í∏∞ÏÉÅÏãúÍ∞Ñ(126)\n",
            "[S1] S1_te(693), wake_time_diff(628), S1_te2(606), sleep_duration_ratio(495), m_activity_met@240min@sum@04h00m(413), beforebed_screen_time_vs_avg_pct(400), wake_time_ratio(382), rolling_wake_time_3d(340), m_activity_0@240min@std@20h00m(318), m_activity@240min@std@12h00m(316)\n",
            "# submission_0.6440575238969299.csv Ï†ÄÏû• ÏôÑÎ£å\n",
            "# submission shape:(250, 9)\n",
            "================================================\n",
            "\n",
            "Top 10 Weight Combinations:\n",
            "Rank 1: lgb_A=0.0, xgb_B=0.6, tab_C=0.1 - Score: 0.6696\n",
            "Saved submission to submission_top1_0.6696.csv\n",
            "Rank 2: lgb_A=0.0, xgb_B=0.5, tab_C=0.2 - Score: 0.6695\n",
            "Saved submission to submission_top2_0.6695.csv\n",
            "Rank 3: lgb_A=0.0, xgb_B=0.3, tab_C=0.4 - Score: 0.6681\n",
            "Saved submission to submission_top3_0.6681.csv\n",
            "Rank 4: lgb_A=0.0, xgb_B=0.3, tab_C=0.5 - Score: 0.6662\n",
            "Saved submission to submission_top4_0.6662.csv\n",
            "Rank 5: lgb_A=0.0, xgb_B=0.4, tab_C=0.4 - Score: 0.6656\n",
            "Saved submission to submission_top5_0.6656.csv\n",
            "Rank 6: lgb_A=0.1, xgb_B=0.2, tab_C=0.5 - Score: 0.6654\n",
            "Saved submission to submission_top6_0.6654.csv\n",
            "Rank 7: lgb_A=0.0, xgb_B=0.0, tab_C=0.7 - Score: 0.6647\n",
            "Saved submission to submission_top7_0.6647.csv\n",
            "Rank 8: lgb_A=0.1, xgb_B=0.3, tab_C=0.3 - Score: 0.6645\n",
            "Saved submission to submission_top8_0.6645.csv\n",
            "Rank 9: lgb_A=0.0, xgb_B=0.2, tab_C=0.6 - Score: 0.6625\n",
            "Saved submission to submission_top9_0.6625.csv\n",
            "Rank 10: lgb_A=0.1, xgb_B=0.3, tab_C=0.4 - Score: 0.6624\n",
            "Saved submission to submission_top10_0.6624.csv\n",
            "\n",
            " STEP3: ÏòàÏ∏°Í≤∞Í≥º ÎπÑÍµêÌëú\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    ÌïôÏäµsum  ÌïôÏäµlen  ÌïôÏäµmean  ÌÖåÏä§Ìä∏sum  ÌÖåÏä§Ìä∏len  ÌÖåÏä§Ìä∏mean\n",
              "Q1    223    450  0.4956     106     250   0.4240\n",
              "Q2    253    450  0.5622     126     250   0.5040\n",
              "Q3    270    450  0.6000     143     250   0.5720\n",
              "S1    390    450  0.8667     198     250   0.7920\n",
              "S2    293    450  0.6511     124     250   0.4960\n",
              "S3    298    450  0.6622     148     250   0.5920"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fb4941c7-db25-4ceb-8a47-4a180a00be99\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ÌïôÏäµsum</th>\n",
              "      <th>ÌïôÏäµlen</th>\n",
              "      <th>ÌïôÏäµmean</th>\n",
              "      <th>ÌÖåÏä§Ìä∏sum</th>\n",
              "      <th>ÌÖåÏä§Ìä∏len</th>\n",
              "      <th>ÌÖåÏä§Ìä∏mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Q1</th>\n",
              "      <td>223</td>\n",
              "      <td>450</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>106</td>\n",
              "      <td>250</td>\n",
              "      <td>0.4240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Q2</th>\n",
              "      <td>253</td>\n",
              "      <td>450</td>\n",
              "      <td>0.5622</td>\n",
              "      <td>126</td>\n",
              "      <td>250</td>\n",
              "      <td>0.5040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Q3</th>\n",
              "      <td>270</td>\n",
              "      <td>450</td>\n",
              "      <td>0.6000</td>\n",
              "      <td>143</td>\n",
              "      <td>250</td>\n",
              "      <td>0.5720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S1</th>\n",
              "      <td>390</td>\n",
              "      <td>450</td>\n",
              "      <td>0.8667</td>\n",
              "      <td>198</td>\n",
              "      <td>250</td>\n",
              "      <td>0.7920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S2</th>\n",
              "      <td>293</td>\n",
              "      <td>450</td>\n",
              "      <td>0.6511</td>\n",
              "      <td>124</td>\n",
              "      <td>250</td>\n",
              "      <td>0.4960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S3</th>\n",
              "      <td>298</td>\n",
              "      <td>450</td>\n",
              "      <td>0.6622</td>\n",
              "      <td>148</td>\n",
              "      <td>250</td>\n",
              "      <td>0.5920</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb4941c7-db25-4ceb-8a47-4a180a00be99')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fb4941c7-db25-4ceb-8a47-4a180a00be99 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fb4941c7-db25-4ceb-8a47-4a180a00be99');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ecd86408-9f51-4408-97c8-c49a6e7e7b3e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ecd86408-9f51-4408-97c8-c49a6e7e7b3e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ecd86408-9f51-4408-97c8-c49a6e7e7b3e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"get_ipython()\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"\\ud559\\uc2b5sum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57,\n        \"min\": 223,\n        \"max\": 390,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          223,\n          253,\n          298\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ud559\\uc2b5len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 450,\n        \"max\": 450,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          450\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ud559\\uc2b5mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1268919374350011,\n        \"min\": 0.4955555555555556,\n        \"max\": 0.8666666666666667,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.4955555555555556\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ud14c\\uc2a4\\ud2b8sum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 31,\n        \"min\": 106,\n        \"max\": 198,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          106\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ud14c\\uc2a4\\ud2b8len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 250,\n        \"max\": 250,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          250\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ud14c\\uc2a4\\ud2b8mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12700656151028838,\n        \"min\": 0.424,\n        \"max\": 0.792,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.424\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 20min 56s, sys: 2.44 s, total: 20min 58s\n",
            "Wall time: 3min 10s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Í≥µÌÜµ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞\n",
        "common_params = {\n",
        "  'n_estimators': 5000,\n",
        "  \"learning_rate\": 0.01,\n",
        "  # 'min_data_in_leaf':2,\n",
        "  # 'bagging_fraction':0.9,\n",
        "  # 'feature_fraction':0.6,\n",
        "  'lambda_l1': 5,\n",
        "  'lambda_l2': 1,\n",
        "  # 'max_depth': 4,\n",
        "  'n_jobs': -1,\n",
        "  'verbosity': -1\n",
        "}\n",
        "\n",
        "# Î™®Îç∏Î≥Ñ ÏÑ∏Î∂Ä ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞\n",
        "best_param_dict = {}\n",
        "best_param_dict['Q3'] = common_params\n",
        "best_param_dict['S1'] = common_params\n",
        "best_param_dict['S2'] = common_params\n",
        "best_param_dict['S3'] = common_params\n",
        "best_param_dict['Q1'] = common_params\n",
        "best_param_dict['Q2'] = common_params\n",
        "\n",
        "\"\"\"\n",
        "// submission_top1_0.6492.csv\n",
        "\n",
        "light_week_type_lag1\n",
        "weekday\n",
        "week_type\n",
        "week_type_lag1\n",
        "activehour_top_bssid\n",
        "beforebed_top_bssid\n",
        "# X shape: (450, 247)\n",
        "# test_X shape: (250, 247)\n",
        "\n",
        " STEP1: Ïã§Ìóò Í≤∞Í≥º ÌôïÏù∏\n",
        "=============== Validation Results ==============\n",
        "tabpfn-v2-classifier.ckpt:‚Äá100%\n",
        "‚Äá29.0M/29.0M‚Äá[00:00<00:00,‚Äá78.9MB/s]\n",
        "config.json:‚Äá100%\n",
        "‚Äá37.0/37.0‚Äá[00:00<00:00,‚Äá4.81kB/s]\n",
        " ÌèâÍ∑† F1: 0.6441 / [ÏÉÅÏÑ∏] Q1(Í∏∞ÏÉÅÏßÅÌõÑÏàòÎ©¥Ïßà):0.7234 Q2(Ï∑®Ïπ®Ï†ÑÏã†Ï≤¥Ï†ÅÌîºÎ°ú):0.8157 Q3(Ï∑®Ïπ®Ï†ÑÏä§Ìä∏Î†àÏä§):0.6366 S2(ÏàòÎ©¥Ìö®Ïú®):0.5489 S3(ÏàòÎ©¥Ïû†Îì§Í∏∞ÏãúÍ∞Ñ):0.7234 S1(S1):0.4163\n",
        "# Ï†ÑÏ≤¥ ÌèâÍ∑† F1: 0.6441\n",
        "================================================\n",
        "\n",
        " STEP2: Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Î°ú Î™®Îç∏ Ïû¨ÌïôÏäµ\n",
        "====== modeling with 100% train & no valid =====\n",
        "[Q1] beforebed_ÌÜµÌôî_time(1144), Q1_te2(477), wake_time_ratio(453), wake_time_diff(322), mlight_first_wakeup_minutes(310), Q1_te(290), lights_off_time(282), sleep_duration_ratio(214), active_hour_mean_speed(199), activehour_total_screen_time(181)\n",
        "[Q2] Q2_te(1990), Q2_te2(1827), activehour_total_screen_time(293), beforebed_unique_bssid_count(176), wake_time_lag1(163), light_rolling_wake_time_2d(161), activehour_screen_time_vs_avg_pct(144), beforebed_max_rssi(140), beforebed_top_bssid_count(139), active_hour_std_hr(134)\n",
        "[Q3] Q3_te2(2299), light_sleep_time_lag2(330), mlight_first_wakeup_minutes(288), rolling_sleep_time_3d(275), light_rolling_sleep_duration_3d(218), Q3_te(214), beforebed_scan_count(211), active_hour_distance_x(196), activehour_ÌÜµÌôî_time(181), walking_minutes(169)\n",
        "[S2] S2_te2(433), S2_te(422), light_sleep_time_lag1(198), work_hour_unknown_ratio(190), m_activity@240min@std@12h00m(176), beforebed_strong_signal_ratio(154), light_rolling_wake_time_2d(151), free_hour_rssi_mean(150), activehour_Ï†ÑÌôî_time(147), sleep_hour_mean_speed(136)\n",
        "[S3] S3_te(2612), S3_te2(336), beforebed_Î©îÏã†Ï†Ä_time(252), light_wake_time_diff(216), sleep_time_diff_lag1(209), light_sleep_time_lag2(199), m_activity_met@240min@sum@16h00m(156), free_hour_rssi_max(150), light_weekday_avg_sleep(137), Î∂àÎÅàÏãúÍ∞ÑÎ∂ÄÌÑ∞Í∏∞ÏÉÅÏãúÍ∞Ñ(126)\n",
        "[S1] S1_te(693), wake_time_diff(628), S1_te2(606), sleep_duration_ratio(495), m_activity_met@240min@sum@04h00m(413), beforebed_screen_time_vs_avg_pct(400), wake_time_ratio(382), rolling_wake_time_3d(340), m_activity_0@240min@std@20h00m(318), m_activity@240min@std@12h00m(316)\n",
        "# submission_0.6440575238969299.csv Ï†ÄÏû• ÏôÑÎ£å\n",
        "# submission shape:(250, 9)\n",
        "================================================\n",
        "\n",
        "Top 3 Weight Combinations:\n",
        "Rank 1: lgb_A=0.0, xgb_B=0.6, tab_C=0.1 - Score: 0.6696\n",
        "Saved submission to submission_top1_0.6696.csv\n",
        "Rank 2: lgb_A=0.0, xgb_B=0.5, tab_C=0.2 - Score: 0.6695\n",
        "Saved submission to submission_top2_0.6695.csv\n",
        "Rank 3: lgb_A=0.0, xgb_B=0.3, tab_C=0.4 - Score: 0.6681\n",
        "Saved submission to submission_top3_0.6681.csv\n",
        "Rank 4: lgb_A=0.0, xgb_B=0.3, tab_C=0.5 - Score: 0.6662\n",
        "Saved submission to submission_top4_0.6662.csv\n",
        "Rank 5: lgb_A=0.0, xgb_B=0.4, tab_C=0.4 - Score: 0.6656\n",
        "Saved submission to submission_top5_0.6656.csv\n",
        "Rank 6: lgb_A=0.1, xgb_B=0.2, tab_C=0.5 - Score: 0.6654\n",
        "Saved submission to submission_top6_0.6654.csv\n",
        "Rank 7: lgb_A=0.0, xgb_B=0.0, tab_C=0.7 - Score: 0.6647\n",
        "Saved submission to submission_top7_0.6647.csv\n",
        "Rank 8: lgb_A=0.1, xgb_B=0.3, tab_C=0.3 - Score: 0.6645\n",
        "Saved submission to submission_top8_0.6645.csv\n",
        "Rank 9: lgb_A=0.0, xgb_B=0.2, tab_C=0.6 - Score: 0.6625\n",
        "Saved submission to submission_top9_0.6625.csv\n",
        "Rank 10: lgb_A=0.1, xgb_B=0.3, tab_C=0.4 - Score: 0.6624\n",
        "Saved submission to submission_top10_0.6624.csv\n",
        "\n",
        " STEP3: ÏòàÏ∏°Í≤∞Í≥º ÎπÑÍµêÌëú\n",
        "ÌïôÏäµsum\tÌïôÏäµlen\tÌïôÏäµmean\tÌÖåÏä§Ìä∏sum\tÌÖåÏä§Ìä∏len\tÌÖåÏä§Ìä∏mean\n",
        "Q1\t223\t450\t0.4956\t106\t250\t0.4240\n",
        "Q2\t253\t450\t0.5622\t126\t250\t0.5040\n",
        "Q3\t270\t450\t0.6000\t143\t250\t0.5720\n",
        "S1\t390\t450\t0.8667\t198\t250\t0.7920\n",
        "S2\t293\t450\t0.6511\t124\t250\t0.4960\n",
        "S3\t298\t450\t0.6622\t148\t250\t0.5920\n",
        "\n",
        "CPU times: user 17min 32s, sys: 2.28 s, total: 17min 35s\n",
        "Wall time: 2min 40s\n",
        "\"\"\"\n",
        "\n",
        "submission_final, oof_result = run_basemodel(train, test, valid_ids, best_param_dict, n_splits=5, random_state=41)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hRrvmB5aHb4G"
      },
      "id": "hRrvmB5aHb4G",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "s5N7SlvaLgh-",
      "metadata": {
        "id": "s5N7SlvaLgh-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "EHSoZgGiLgsd",
      "metadata": {
        "id": "EHSoZgGiLgsd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "nsZ0kEKjLerl",
      "metadata": {
        "id": "nsZ0kEKjLerl"
      },
      "source": [
        "### üì¶ Ïù¥Ï†ÑÏ†úÏ∂úÍ≥º ÎπÑÍµê"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8d5e619e",
      "metadata": {
        "id": "8d5e619e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1b08600-368f-4c2d-d152-f8f05874c6bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 files with smallest differences:\n",
            "01. submission_0.6440575238969299.csv: 103 differences\n",
            "02. submission_top5_0.6656.csv: 137 differences\n",
            "03. submission_top10_0.6624.csv: 146 differences\n",
            "04. submission_top4_0.6662.csv: 159 differences\n",
            "05. submission_top6_0.6654.csv: 168 differences\n",
            "06. submission_top9_0.6625.csv: 182 differences\n",
            "07. submission_top1_0.6696.csv: 188 differences\n",
            "08. submission_top2_0.6695.csv: 213 differences\n",
            "09. submission_top8_0.6645.csv: 263 differences\n",
            "10. submission_top3_0.6681.csv: 273 differences\n",
            "11. submission_top7_0.6647.csv: 410 differences\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Reference file\n",
        "reference_file = '/content/drive/MyDrive/data/ch2025_data_items/share/submissions/submission_top1_0.6492.csv'\n",
        "ref_df = pd.read_csv(reference_file)\n",
        "\n",
        "# Get all CSV files in data directory\n",
        "data_dir = Path('./')\n",
        "csv_files = list(data_dir.glob('*.csv'))\n",
        "\n",
        "# Store differences for each file\n",
        "differences = []\n",
        "\n",
        "for csv_file in csv_files:\n",
        "    if csv_file.name == os.path.basename(reference_file):\n",
        "        continue\n",
        "\n",
        "    # Read current file\n",
        "    current_df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Calculate differences in specified columns\n",
        "    diff_count = 0\n",
        "    for col in ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']:\n",
        "        diff_count += (ref_df[col] != current_df[col]).sum()\n",
        "\n",
        "    differences.append((csv_file.name, diff_count))\n",
        "    # print(f\"File: {csv_file.name}, Differences: {diff_count}\")\n",
        "\n",
        "# Sort by difference count and get top 20\n",
        "differences.sort(key=lambda x: x[1])\n",
        "print(\"\\nTop 10 files with smallest differences:\")\n",
        "for i, (file_name, diff_count) in enumerate(differences[:20], 1):\n",
        "    print(f\"{str(i).zfill(2)}. {file_name}: {diff_count} differences\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d2b24160",
      "metadata": {
        "id": "d2b24160"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "29b4a176",
      "metadata": {
        "id": "29b4a176"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "79079453",
      "metadata": {
        "id": "79079453"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ee90ed8f",
      "metadata": {
        "id": "ee90ed8f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "694fc8ae",
      "metadata": {
        "id": "694fc8ae"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8744d38a",
      "metadata": {
        "id": "8744d38a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e496fd76",
      "metadata": {
        "id": "e496fd76"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 7210916,
          "sourceId": 12085434,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 4148.996629,
      "end_time": "2025-06-07T13:22:50.273930",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-06-07T12:13:41.277301",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
