{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobgreen4477/The-4th-ETRI-AI-Human-Understanding-Competition/blob/main/etri_template_vF1_0_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e04b986",
      "metadata": {
        "id": "5e04b986"
      },
      "source": [
        "> title : ì œ 4íšŒ ETRI íœ´ë¨¼ì´í•´ ì¸ê³µì§€ëŠ¥ ë…¼ë¬¸ê²½ì§„ëŒ€íšŒ <br>\n",
        "> author : hjy,byc <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21389cf1",
      "metadata": {
        "id": "21389cf1"
      },
      "source": [
        "### ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "aca4f97e-8957-48ee-8927-9f27e8046cfc",
      "metadata": {
        "id": "aca4f97e-8957-48ee-8927-9f27e8046cfc"
      },
      "outputs": [],
      "source": [
        "! pip install haversine >/dev/null\n",
        "! pip install optuna  >/dev/null\n",
        "! pip install category_encoders >/dev/null\n",
        "! pip install tabpfn  >/dev/null\n",
        "! pip install catboost >/dev/null\n",
        "! pip install torchmetrics >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4f2a25a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "4f2a25a5",
        "outputId": "c5fedf7e-6a84-4131-9b4e-c7167dc97a3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbyc3230\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250611_061002-ycawcume</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/byc3230/etri_lifelog/runs/ycawcume' target=\"_blank\">giddy-sun-115</a></strong> to <a href='https://wandb.ai/byc3230/etri_lifelog' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/byc3230/etri_lifelog' target=\"_blank\">https://wandb.ai/byc3230/etri_lifelog</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/byc3230/etri_lifelog/runs/ycawcume' target=\"_blank\">https://wandb.ai/byc3230/etri_lifelog/runs/ycawcume</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ê¸°ë³¸ ëª¨ë“ˆ\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import ast\n",
        "import glob\n",
        "import random\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from math import radians, cos, sin, asin, sqrt\n",
        "from functools import reduce\n",
        "from datetime import datetime, timedelta, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ë¨¸ì‹ ëŸ¬ë‹\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import f1_score, roc_auc_score, roc_curve\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
        "from category_encoders import TargetEncoder\n",
        "from lightgbm import LGBMClassifier, log_evaluation, early_stopping\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "import lightgbm as lgb\n",
        "from tabpfn import TabPFNClassifier\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Hugging Face\n",
        "from huggingface_hub import login\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    LlamaTokenizer,\n",
        "    LlamaForCausalLM,\n",
        "    LlamaForSequenceClassification\n",
        ")\n",
        "\n",
        "# PEFT (Parameter-Efficient Fine-Tuning)\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    TaskType\n",
        ")\n",
        "\n",
        "# Evaluation & Utilities\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "# ê¸°íƒ€\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import tqdm as auto_tqdm  # í•„ìš” ì‹œ êµ¬ë¶„\n",
        "from scipy.stats import entropy\n",
        "from haversine import haversine\n",
        "from io import StringIO\n",
        "import gc\n",
        "\n",
        "# wandb\n",
        "import wandb\n",
        "wandb.login(key=\"5fa8dfb2c5be3c888bfe0101437a8fa22fbdf0e0\")\n",
        "wandb.init(project=\"etri_lifelog\", entity=\"byc3230\")\n",
        "\n",
        "# ì˜µì…˜\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "pd.set_option('display.max_columns', 999)\n",
        "pd.set_option('display.max_rows', 999)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.float_format', lambda x: '%0.4f' % x)\n",
        "\n",
        "# ê¸°íƒ€\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "de5b6541",
      "metadata": {
        "id": "de5b6541"
      },
      "outputs": [],
      "source": [
        "string = \"\"\"\n",
        "subject_id,sleep_date\n",
        "id01,2024-07-24\n",
        "id01,2024-08-26\n",
        "id01,2024-08-28\n",
        "id01,2024-08-29\n",
        "id02,2024-08-23\n",
        "id02,2024-09-24\n",
        "id02,2024-09-26\n",
        "id02,2024-09-27\n",
        "id03,2024-08-30\n",
        "id03,2024-09-01\n",
        "id03,2024-09-02\n",
        "id03,2024-09-06\n",
        "id04,2024-09-03\n",
        "id04,2024-10-10\n",
        "id04,2024-10-12\n",
        "id04,2024-10-13\n",
        "id05,2024-10-19\n",
        "id05,2024-10-23\n",
        "id05,2024-10-24\n",
        "id05,2024-10-27\n",
        "id06,2024-07-25\n",
        "id06,2024-07-26\n",
        "id06,2024-07-27\n",
        "id06,2024-07-30\n",
        "id07,2024-07-07\n",
        "id07,2024-08-02\n",
        "id07,2024-08-04\n",
        "id07,2024-08-05\n",
        "id08,2024-08-28\n",
        "id08,2024-08-29\n",
        "id08,2024-08-30\n",
        "id08,2024-09-02\n",
        "id09,2024-08-02\n",
        "id09,2024-08-31\n",
        "id09,2024-09-02\n",
        "id09,2024-09-03\n",
        "id10,2024-08-28\n",
        "id10,2024-08-30\n",
        "id10,2024-08-31\n",
        "id10,2024-09-03\n",
        "\"\"\"\n",
        "\n",
        "# DataFrame ìƒì„±\n",
        "valid_ids = pd.read_csv(StringIO(string), sep=',')\n",
        "valid_ids['pk'] = valid_ids['subject_id']+valid_ids['sleep_date']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a6ab82b",
      "metadata": {
        "id": "0a6ab82b"
      },
      "source": [
        "### ğŸ“¦ ë°ì´í„° ì½ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "39d79d1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39d79d1f",
        "outputId": "a2bbd6cc-cf96-49c9-bb9e-bbe43e0ede0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "# train  shape: (450, 237)\n",
            "# test   shape: (250, 237)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/MyDrive/data/ch2025_data_items/share/'\n",
        "\n",
        "train = pd.read_parquet(f'{path}train_63775.parquet')\n",
        "test = pd.read_parquet(f'{path}test_63775.parquet')\n",
        "\n",
        "print('# train  shape:',train.shape)\n",
        "print('# test   shape:',test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5396de95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5396de95",
        "outputId": "b1947fe2-a45d-4dc0-e6c3-bc826a274ff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# drop_features: []\n"
          ]
        }
      ],
      "source": [
        "# drop_features = ['afterwork_max_label','sleeptime_max_label','worktime_max_label']\n",
        "drop_features = ['top_bssid'] # ,'week_type','week_type_lag1'\n",
        "drop_features = [i for i in drop_features if i in train.columns.tolist()]\n",
        "print('# drop_features:',drop_features)\n",
        "train = train.drop(columns=drop_features)\n",
        "test = test.drop(columns=drop_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e31852ee",
      "metadata": {
        "id": "e31852ee"
      },
      "outputs": [],
      "source": [
        "# ---\n",
        "# ì¶”ì •ìˆ˜ë©´íš¨ìœ¨\n",
        "# ---\n",
        "\n",
        "def calculate_sleep_duration_min(sleep_time, wake_time):\n",
        "    \"\"\"\n",
        "    ì·¨ì¹¨ ì‹œê°(sleep_time)ê³¼ ê¸°ìƒ ì‹œê°(wake_time)ì„ ì…ë ¥ë°›ì•„ ìˆ˜ë©´ ì‹œê°„(ë¶„) ë°˜í™˜\n",
        "    ë‹¨ìœ„ëŠ” float ì‹œê°„ (ì˜ˆ: 23.5, 6.25)\n",
        "    \"\"\"\n",
        "    if pd.isna(sleep_time) or pd.isna(wake_time):\n",
        "        return None\n",
        "    if wake_time < sleep_time:\n",
        "        wake_time += 24  # ìì • ë„˜ê¸´ ê²½ìš° ë³´ì •\n",
        "    duration = (wake_time - sleep_time) * 60\n",
        "    return round(duration)\n",
        "\n",
        "train['ë¶ˆëˆì‹œê°„ë¶€í„°ê¸°ìƒì‹œê°„'] = train.apply(lambda x: calculate_sleep_duration_min(x['lights_off_time'],x['wake_time']),axis=1)\n",
        "test['ë¶ˆëˆì‹œê°„ë¶€í„°ê¸°ìƒì‹œê°„'] = test.apply(lambda x: calculate_sleep_duration_min(x['lights_off_time'],x['wake_time']),axis=1)\n",
        "\n",
        "train['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'] = train['ë¶ˆëˆì‹œê°„ë¶€í„°ê¸°ìƒì‹œê°„']/train['sleep_duration_min']\n",
        "test['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'] = test['ë¶ˆëˆì‹œê°„ë¶€í„°ê¸°ìƒì‹œê°„']/test['sleep_duration_min']\n",
        "\n",
        "# ì´ìƒê°’ ì œê±°\n",
        "train['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'] = np.where(train['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨']<-5,np.nan,train['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'])\n",
        "test['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'] = np.where(test['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨']<-5,np.nan,test['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'])\n",
        "train['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'] = np.where(train['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨']>5,np.nan,train['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'])\n",
        "test['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'] = np.where(test['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨']>55,np.nan,test['ì¶”ì •ìˆ˜ë©´íš¨ìœ¨'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a50b8b13",
      "metadata": {
        "id": "a50b8b13"
      },
      "outputs": [],
      "source": [
        "# ìš”ì¼ ì»¬ëŸ¼ ì¶”ê°€ (ì˜ˆ: ì›”ìš”ì¼, í™”ìš”ì¼, ...)\n",
        "train['lifelog_date'] = pd.to_datetime(train['lifelog_date'])\n",
        "test['lifelog_date'] = pd.to_datetime(test['lifelog_date'])\n",
        "\n",
        "# ìš”ì¼\n",
        "weekday_map = {\n",
        "    0: 'ì›”ìš”ì¼', 1: 'í™”ìš”ì¼', 2: 'ìˆ˜ìš”ì¼', 3: 'ëª©ìš”ì¼',\n",
        "    4: 'ê¸ˆìš”ì¼', 5: 'í† ìš”ì¼', 6: 'ì¼ìš”ì¼'\n",
        "}\n",
        "train['weekday'] = train['lifelog_date'].dt.dayofweek.map(weekday_map)\n",
        "test['weekday'] = test['lifelog_date'].dt.dayofweek.map(weekday_map)\n",
        "\n",
        "# ì›”\n",
        "train['month'] = train['lifelog_date'].dt.month\n",
        "test['month'] = test['lifelog_date'].dt.month\n",
        "\n",
        "# weekend\n",
        "train['weekend'] = np.where(train['weekday'].isin(['í† ìš”ì¼','ì¼ìš”ì¼']),1,0)\n",
        "test['weekend'] = np.where(test['weekday'].isin(['í† ìš”ì¼','ì¼ìš”ì¼']),1,0)\n",
        "\n",
        "# ê³µíœ´ì¼\n",
        "ê³µíœ´ì¼ = [\n",
        "     '2024-08-15'\n",
        "    ,'2024-09-16'\n",
        "    ,'2024-09-17'\n",
        "    ,'2024-09-18'\n",
        "    ,'2024-10-03'\n",
        "    ,'2024-10-09'\n",
        "]\n",
        "train['ê³µíœ´ì¼'] = np.where(train['lifelog_date'].isin(ê³µíœ´ì¼),1,0)\n",
        "test['ê³µíœ´ì¼'] = np.where(test['lifelog_date'].isin(ê³µíœ´ì¼),1,0)\n",
        "\n",
        "# ì£¼ë§ + ê³µíœ´ì¼ ë¬¶ì–´ì£¼ê¸°\n",
        "train['weekend_holilday'] = np.where( ((train['weekend']==0) & (train['ê³µíœ´ì¼']==1)), 1, train['weekend'])\n",
        "test['weekend_holilday'] = np.where( ((test['weekend']==0) & (test['ê³µíœ´ì¼']==1)), 1, test['weekend'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "24228ae9",
      "metadata": {
        "id": "24228ae9"
      },
      "outputs": [],
      "source": [
        "def add_prev_day_flag(df):\n",
        "    df = df.copy()\n",
        "    df['lifelog_date'] = pd.to_datetime(df['lifelog_date'])\n",
        "\n",
        "    # ê° subject_idë³„ë¡œ ì „ë‚  ë‚ ì§œ ë§Œë“¤ê¸°\n",
        "    df['prev_day'] = df['lifelog_date'] - pd.Timedelta(days=1)\n",
        "\n",
        "    # subject_id + ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì›ë³¸ í‚¤ êµ¬ì„±\n",
        "    key_set = set(zip(df['subject_id'], df['lifelog_date']))\n",
        "\n",
        "    # ì „ë‚  ë°ì´í„°ê°€ ì¡´ì¬í•˜ë©´ 1, ì—†ìœ¼ë©´ 0\n",
        "    df['has_prev_day_data'] = df[['subject_id', 'prev_day']].apply(\n",
        "        lambda row: 1 if (row['subject_id'], row['prev_day']) in key_set else 0, axis=1\n",
        "    )\n",
        "\n",
        "    return df.drop(columns=['prev_day'])\n",
        "\n",
        "train = add_prev_day_flag(train)\n",
        "test = add_prev_day_flag(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "938b961c",
      "metadata": {
        "id": "938b961c"
      },
      "outputs": [],
      "source": [
        "# ---\n",
        "# ì¶”ì •íœ´ê°€\n",
        "# ---\n",
        "\n",
        "def rule_based_sum(x):\n",
        "    rules = (\n",
        "        # (x['sleep_duration_min'] > (x['avg_sleep_duration']+30))\n",
        "          (x['sleep_duration_min'] > (x['avg_sleep_duration']+60))\n",
        "        & (x['week_type'] == 'weekday')\n",
        "        # & (x['month'].isin([7,8]))\n",
        "    )\n",
        "    return rules\n",
        "\n",
        "train['vacation'] = train.groupby('subject_id').apply(rule_based_sum).reset_index(level=0, drop=True).astype(int)\n",
        "test['vacation'] = test.groupby('subject_id').apply(rule_based_sum).reset_index(level=0, drop=True).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cb9fd81b",
      "metadata": {
        "id": "cb9fd81b"
      },
      "outputs": [],
      "source": [
        "# ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒí•´ì„œ ê²°ì¸¡ê°’ -1ë¡œ ì±„ìš°ê¸°\n",
        "train[train.select_dtypes(include='number').columns] = train.select_dtypes(include='number').fillna(-1)\n",
        "test[test.select_dtypes(include='number').columns] = test.select_dtypes(include='number').fillna(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "qvMT9FPwoz-x",
      "metadata": {
        "id": "qvMT9FPwoz-x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "OUzhBfxNo3J0",
      "metadata": {
        "id": "OUzhBfxNo3J0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "gTVVBAHko3bs",
      "metadata": {
        "id": "gTVVBAHko3bs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "01bd961f",
      "metadata": {
        "id": "01bd961f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "S_syDjeiMnUN",
      "metadata": {
        "id": "S_syDjeiMnUN"
      },
      "source": [
        "### ============================"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HtiE52IWK_nd",
      "metadata": {
        "id": "HtiE52IWK_nd"
      },
      "source": [
        "### run_basemodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2kbYLCFJLpQ5",
      "metadata": {
        "id": "2kbYLCFJLpQ5"
      },
      "outputs": [],
      "source": [
        "def run_basemodel(train, test, valid_ids, common_params, n_splits, random_state=42):\n",
        "\n",
        "    lgb_A = 0.3\n",
        "    xgb_B = 0.3\n",
        "    tab_C = 0.3\n",
        "\n",
        "    train_df = train.copy()\n",
        "    test_df = test.copy()\n",
        "\n",
        "    submission_final = test_df[['subject_id', 'sleep_date', 'lifelog_date']].copy()\n",
        "    submission_final['lifelog_date'] = pd.to_datetime(submission_final['lifelog_date']).dt.date\n",
        "\n",
        "    # íƒ€ê²Ÿ\n",
        "    targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3']\n",
        "    targets_binary_name = ['ê¸°ìƒì§í›„ìˆ˜ë©´ì§ˆ','ì·¨ì¹¨ì „ì‹ ì²´ì í”¼ë¡œ','ì·¨ì¹¨ì „ìŠ¤íŠ¸ë ˆìŠ¤','ìˆ˜ë©´íš¨ìœ¨','ìˆ˜ë©´ì ë“¤ê¸°ì‹œê°„']\n",
        "    target_multiclass = 'S1'\n",
        "    all_targets = targets_binary + [target_multiclass]\n",
        "\n",
        "    # ë…¸ì´ì¦ˆ ìˆ˜ì¤€ ì„¤ì •\n",
        "    def add_noise(series, noise_level, seed=3):\n",
        "        rng = np.random.default_rng(seed)\n",
        "        return series * (1 + noise_level * rng.standard_normal(len(series)))\n",
        "\n",
        "    noise_level = 0.015  # í•„ìš”ì— ë”°ë¼ ì¡°ì •\n",
        "\n",
        "    # íƒ€ê²Ÿì¸ì½”ë”©\n",
        "    for tgt in all_targets:\n",
        "\n",
        "      encoder_feats = ['subject_id','month','weekend'] # 'weekday', 'subject_id','month','weekend'\n",
        "\n",
        "      #### íƒ€ê²Ÿì¸ì½”ë”©1\n",
        "\n",
        "      subject_mean = train_df.groupby(encoder_feats)[tgt].mean().rename(f'{tgt}_te')\n",
        "      train_df = train_df.merge(subject_mean, on=encoder_feats, how='left')\n",
        "      test_df = test_df.merge(subject_mean, on=encoder_feats, how='left')\n",
        "      global_mean = train_df[tgt].mean()\n",
        "      test_df[f'{tgt}_te'] = test_df[f'{tgt}_te'].fillna(global_mean)\n",
        "\n",
        "      # ë…¸ì´ì¦ˆ ì¶”ê°€\n",
        "      train_df[f'{tgt}_te'] = add_noise(train_df[f'{tgt}_te'], noise_level)\n",
        "      test_df[f'{tgt}_te'] = add_noise(test_df[f'{tgt}_te'], noise_level)\n",
        "\n",
        "      #### íƒ€ê²Ÿì¸ì½”ë”©2\n",
        "\n",
        "      # ìƒˆë¡œìš´ ë²”ì£¼í˜• ì—´ ìƒì„±\n",
        "      train_df['TMP'] = train_df[encoder_feats].applymap(str).apply(lambda x: ''.join(x) ,axis=1)\n",
        "      test_df['TMP'] = test_df[encoder_feats].applymap(str).apply(lambda x: ''.join(x) ,axis=1)\n",
        "\n",
        "      # ì¸ì½”ë”\n",
        "      encoder = TargetEncoder(cols=['TMP'], smoothing=300) # 40\n",
        "      encoder.fit(train_df[['TMP']], train_df[tgt])\n",
        "\n",
        "      # ì¸ì½”ë”© ê²°ê³¼ë¥¼ ìƒˆë¡œìš´ ì—´ì— ì €ì¥\n",
        "      train_df[f'{tgt}_te2'] = encoder.transform(train_df[['TMP']])\n",
        "      test_df[f'{tgt}_te2'] = encoder.transform(test_df[['TMP']])\n",
        "\n",
        "      # ë…¸ì´ì¦ˆ ì¶”ê°€\n",
        "      train_df[f'{tgt}_te2'] = add_noise(train_df[f'{tgt}_te2'], noise_level)\n",
        "      test_df[f'{tgt}_te2'] = add_noise(test_df[f'{tgt}_te2'], noise_level)\n",
        "\n",
        "      # ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ ì œê±°\n",
        "      train_df = train_df.drop(columns=['TMP'])\n",
        "      test_df = test_df.drop(columns=['TMP'])\n",
        "\n",
        "\n",
        "    # ì¸ì½”ë”©\n",
        "    PK = ['sleep_date', 'lifelog_date', 'subject_id']\n",
        "    encoder = LabelEncoder()\n",
        "    categorical_features = [i for i in train_df.select_dtypes(include=['object', 'category']).columns if i not in PK+['pk']]\n",
        "    for col in categorical_features:\n",
        "        print(col)\n",
        "        train_df[col] = encoder.fit_transform(train_df[col])\n",
        "        test_df[col] = encoder.fit_transform(test_df[col])\n",
        "\n",
        "    # X\n",
        "    X = train_df.drop(columns=PK + all_targets)\n",
        "    test_X = test_df.drop(columns=PK + all_targets)\n",
        "    print(f'# X shape: {X.shape}')\n",
        "    print(f'# test_X shape: {test_X.shape}')\n",
        "\n",
        "    print('\\n STEP1: ì‹¤í—˜ ê²°ê³¼ í™•ì¸')\n",
        "    print(\"=============== Validation Results ==============\")\n",
        "    total_avg_f1s = []\n",
        "    val_f1 = []\n",
        "    binary_val_preds = {}\n",
        "    multiclass_val_preds = {}\n",
        "    binary_test_preds = {}\n",
        "    multiclass_test_preds = {}\n",
        "    test_preds = {}\n",
        "\n",
        "    # Find optimal weights\n",
        "    best_weights = []\n",
        "    best_scores = []\n",
        "\n",
        "    for col in targets_binary:\n",
        "        # binary\n",
        "        y = train_df[col]\n",
        "\n",
        "        valid_ids['pk'] = valid_ids['subject_id']+valid_ids['sleep_date']\n",
        "        train_df['pk'] = train_df['subject_id']+train_df['sleep_date']\n",
        "\n",
        "        X_valid = train_df.loc[train_df['pk'].isin(valid_ids['pk']),X.columns.tolist()].reset_index(drop=True).copy()\n",
        "        X_train = train_df.loc[~train_df['pk'].isin(valid_ids['pk']),X.columns.tolist()].reset_index(drop=True).copy()\n",
        "        y_valid = train_df.loc[train_df['pk'].isin(valid_ids['pk']),y.name].reset_index(drop=True).copy()\n",
        "        y_train = train_df.loc[~train_df['pk'].isin(valid_ids['pk']),y.name].reset_index(drop=True).copy()\n",
        "\n",
        "        # Get parameters for both models\n",
        "        lgb_params = common_params[col].copy()\n",
        "        lgb_params['random_state'] = random_state\n",
        "\n",
        "        xgb_params = {\n",
        "            'n_estimators': 1000,\n",
        "            'learning_rate': 0.01,\n",
        "            'max_depth': 6,\n",
        "            'min_child_weight': 1,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'random_state': random_state\n",
        "        }\n",
        "\n",
        "        # Train LightGBM\n",
        "        lgb_model = LGBMClassifier(**lgb_params)\n",
        "        lgb_model.fit(X_train, y_train)\n",
        "\n",
        "        # Train XGBoost\n",
        "        xgb_model = XGBClassifier(**xgb_params)\n",
        "        xgb_model.fit(X_train, y_train)\n",
        "\n",
        "        tabpfn_params = {\n",
        "            'device': 'cuda'\n",
        "        }\n",
        "\n",
        "        # Train TabPFN\n",
        "        tabpfn_model = TabPFNClassifier(**tabpfn_params)\n",
        "        tabpfn_model.fit(X_train, y_train)\n",
        "        tab_pred_valid = tabpfn_model.predict_proba(X_valid.values)[:, 1]\n",
        "\n",
        "        lgb_pred_valid = lgb_model.predict_proba(X_valid)[:, 1]\n",
        "        xgb_pred_valid = xgb_model.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "        pred_valid = (lgb_A * lgb_pred_valid + xgb_B * xgb_pred_valid + tab_C * tab_pred_valid > 0.5).astype(int)\n",
        "\n",
        "        f1 = f1_score(y_valid, pred_valid, average='macro')\n",
        "        val_f1.append(f1)\n",
        "\n",
        "        # Store predictions\n",
        "        binary_val_preds[col] = {\n",
        "            'lgb': lgb_pred_valid,\n",
        "            'xgb': xgb_pred_valid,\n",
        "            'tab': tab_pred_valid,\n",
        "            'true': y_valid\n",
        "        }\n",
        "\n",
        "    # multiclass\n",
        "    y = train_df[target_multiclass]\n",
        "    X_valid = train_df.loc[train_df['pk'].isin(valid_ids['pk']),X.columns.tolist()].reset_index(drop=True).copy()\n",
        "    X_train = train_df.loc[~train_df['pk'].isin(valid_ids['pk']),X.columns.tolist()].reset_index(drop=True).copy()\n",
        "    y_valid = train_df.loc[train_df['pk'].isin(valid_ids['pk']),y.name].reset_index(drop=True).copy()\n",
        "    y_train = train_df.loc[~train_df['pk'].isin(valid_ids['pk']),y.name].reset_index(drop=True).copy()\n",
        "\n",
        "    # Get parameters for both models\n",
        "    lgb_params = common_params['S1'].copy()\n",
        "    lgb_params['random_state'] = random_state\n",
        "\n",
        "    xgb_params = {\n",
        "        'n_estimators': 1000,\n",
        "        'learning_rate': 0.01,\n",
        "        'max_depth': 6,\n",
        "        'min_child_weight': 1,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'random_state': random_state\n",
        "    }\n",
        "\n",
        "    # í´ë˜ìŠ¤ weight ê³„ì‚°\n",
        "    classes = np.unique(y_train)\n",
        "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
        "    class_weights = dict(zip(classes, weights))\n",
        "\n",
        "    # ê° ìƒ˜í”Œì— ëŒ€í•´ weight ë§¤í•‘\n",
        "    w_train = pd.Series(y_train).map(class_weights)\n",
        "    w_train = compute_sample_weight(class_weight='balanced', y=y_train)\n",
        "\n",
        "    # Train LightGBM\n",
        "    lgb_model = LGBMClassifier(**lgb_params, objective='multiclass', num_class=3)\n",
        "    lgb_model.fit(X_train, y_train, sample_weight=w_train)\n",
        "\n",
        "    # Train XGBoost\n",
        "    xgb_model = XGBClassifier(**xgb_params, objective='multi:softmax', num_class=3)\n",
        "    xgb_model.fit(X_train, y_train,sample_weight=w_train)\n",
        "\n",
        "    tabpfn_params = {\n",
        "        'device': 'cuda'\n",
        "    }\n",
        "\n",
        "    # Train TabPFN\n",
        "    tabpfn_model = TabPFNClassifier(**tabpfn_params)\n",
        "    tabpfn_model.fit(X_train, y_train)\n",
        "\n",
        "    # Get predictions and ensemble\n",
        "    lgb_pred_valid = lgb_model.predict_proba(X_valid)\n",
        "    xgb_pred_valid = xgb_model.predict_proba(X_valid)\n",
        "    tab_pred_valid = tabpfn_model.predict_proba(X_valid.values)\n",
        "\n",
        "    pred_valid = np.argmax(lgb_A * lgb_pred_valid + xgb_B * xgb_pred_valid + tab_C * tab_pred_valid, axis=1)\n",
        "\n",
        "    f1 = f1_score(y_valid, pred_valid, average='macro')\n",
        "    val_f1.append(f1)\n",
        "\n",
        "    multiclass_val_preds = {\n",
        "        'lgb': lgb_pred_valid,\n",
        "        'xgb': xgb_pred_valid,\n",
        "        'tab': tab_pred_valid,\n",
        "        'true': y_valid\n",
        "    }\n",
        "\n",
        "    # Generate all possible weight combinations that sum to 1\n",
        "    step = 0.1\n",
        "    for lgb_A in np.arange(0, 1.1, step):\n",
        "        for xgb_B in np.arange(0, 1.1 - lgb_A, step):\n",
        "            for tab_C in np.arange(0, 1.1 - lgb_A - xgb_B, step):\n",
        "                TOT = 1 - (lgb_A + xgb_B + tab_C)\n",
        "                if TOT >= 0:\n",
        "                    weights = (lgb_A, xgb_B, tab_C)\n",
        "                    val_scores = []\n",
        "\n",
        "                    # Binary targets\n",
        "                    for col in targets_binary:\n",
        "                        preds = binary_val_preds[col]\n",
        "                        ensemble_pred = (lgb_A * preds['lgb'] + xgb_B * preds['xgb'] + tab_C * preds['tab'] > 0.5).astype(int)\n",
        "                        f1 = f1_score(preds['true'], ensemble_pred, average='macro')\n",
        "                        val_scores.append(f1)\n",
        "                        # print(f\" Validation Score {col}:{f1:.4f}\")\n",
        "\n",
        "                    # Multiclass target\n",
        "                    preds = multiclass_val_preds\n",
        "                    ensemble_pred = np.argmax(lgb_A * preds['lgb'] + xgb_B * preds['xgb'] + tab_C * preds['tab'] , axis=1)\n",
        "                    f1 = f1_score(preds['true'], ensemble_pred, average='macro')\n",
        "                    # print(f\" Validation Score S1:{f1:.4f}\")\n",
        "                    val_scores.append(f1)\n",
        "\n",
        "                    avg_score = np.mean(val_scores)\n",
        "                    best_weights.append(weights)\n",
        "                    best_scores.append(avg_score)\n",
        "\n",
        "                    # print(f\"Average Validation Score: {avg_score:.4f}\")\n",
        "\n",
        "    # Sort results and get top 3\n",
        "    sorted_indices = np.argsort(best_scores)[::-1]\n",
        "    top_3_weights = [best_weights[i] for i in sorted_indices]\n",
        "    top_3_scores = [best_scores[i] for i in sorted_indices]\n",
        "\n",
        "    # print(\"\\nTop All Weight Combinations:\")\n",
        "    # for i, (weights, score) in enumerate(zip(top_3_weights, top_3_scores)):\n",
        "    #     print(f\"Rank {i+1}: lgb_A={weights[0]:.1f}, xgb_B={weights[1]:.1f}, tab_C={weights[2]:.1f} - Score: {score:.4f}\")\n",
        "\n",
        "    avg_f1 = np.mean(val_f1)\n",
        "    total_avg_f1s.append(avg_f1)\n",
        "    detail = \" \".join([f\"{name}({tname}):{score:.4f}\" for name, tname, score in zip(targets_binary + [target_multiclass], targets_binary_name + ['S1'], val_f1)])\n",
        "    print(f\" í‰ê·  F1: {avg_f1:.4f} / [ìƒì„¸] {detail}\")\n",
        "    print(f\"# ì „ì²´ í‰ê·  F1: {np.mean(total_avg_f1s):.4f}\")\n",
        "    print(\"================================================\")\n",
        "\n",
        "    # modoling with 100% train & no valid\n",
        "    print('\\n STEP2: ì „ì²´ ë°ì´í„°ë¡œ ëª¨ë¸ ì¬í•™ìŠµ')\n",
        "    print(\"====== modeling with 100% train & no valid =====\")\n",
        "\n",
        "    # binary\n",
        "    binary_preds = {}\n",
        "    binary_preds_proba = {}\n",
        "    for col in targets_binary:\n",
        "        # Get parameters for both models\n",
        "        lgb_params = common_params[col].copy()\n",
        "        lgb_params['random_state'] = random_state\n",
        "\n",
        "        xgb_params = {\n",
        "            'n_estimators': 1000,\n",
        "            'learning_rate': 0.01,\n",
        "            'max_depth': 6,\n",
        "            'min_child_weight': 1,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'random_state': random_state\n",
        "        }\n",
        "\n",
        "        y = train_df[col]\n",
        "\n",
        "        is_multiclass = False\n",
        "\n",
        "        # Train LightGBM\n",
        "        lgb_model = LGBMClassifier(**lgb_params)\n",
        "        lgb_model.fit(X, y)\n",
        "\n",
        "        # Train XGBoost\n",
        "        xgb_model = XGBClassifier(**xgb_params)\n",
        "        xgb_model.fit(X, y)\n",
        "\n",
        "        tabpfn_params = {\n",
        "            'device': 'cuda'\n",
        "        }\n",
        "\n",
        "        # Train TabPFN\n",
        "        tabpfn_model = TabPFNClassifier(**tabpfn_params)\n",
        "        tabpfn_model.fit(X, y)\n",
        "\n",
        "        tab_pred = tabpfn_model.predict_proba(test_X)[:, 1]\n",
        "        lgb_pred = lgb_model.predict_proba(test_X)[:, 1]\n",
        "        xgb_pred = xgb_model.predict_proba(test_X)[:, 1]\n",
        "\n",
        "        binary_preds[col] = (lgb_A * lgb_pred + xgb_B * xgb_pred + tab_C * tab_pred > 0.5).astype(int)\n",
        "\n",
        "        # Store predictions\n",
        "        binary_test_preds[col] = {\n",
        "            'lgb': lgb_pred,\n",
        "            'xgb': xgb_pred,\n",
        "            'tab': tab_pred\n",
        "        }\n",
        "\n",
        "        # Feature importance (using LightGBM's importance)\n",
        "        fi_df = pd.DataFrame({'feature': X.columns, 'importance': lgb_model.feature_importances_})\n",
        "        top10 = fi_df.sort_values(by='importance', ascending=False).head(10)\n",
        "        feat_str = \", \".join([f\"{row['feature']}({int(row['importance'])})\" for _, row in top10.iterrows()])\n",
        "        print(f\"[{col}] {feat_str}\")\n",
        "\n",
        "    # multiclass\n",
        "    y = train_df['S1']\n",
        "\n",
        "    # Get parameters for both models\n",
        "    lgb_params = common_params['S1'].copy()\n",
        "    lgb_params['random_state'] = random_state\n",
        "\n",
        "    xgb_params = {\n",
        "        'n_estimators': 1000,\n",
        "        'learning_rate': 0.01,\n",
        "        'max_depth': 6,\n",
        "        'min_child_weight': 1,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'random_state': random_state\n",
        "    }\n",
        "\n",
        "    # í´ë˜ìŠ¤ weight ê³„ì‚°\n",
        "    classes = np.unique(y)\n",
        "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y)\n",
        "    class_weights = dict(zip(classes, weights))\n",
        "\n",
        "    # ê° ìƒ˜í”Œì— ëŒ€í•´ weight ë§¤í•‘\n",
        "    w_train = pd.Series(y).map(class_weights)\n",
        "    w_train = compute_sample_weight(class_weight='balanced', y=y)\n",
        "\n",
        "    is_multiclass = True\n",
        "\n",
        "    # Train LightGBM\n",
        "    lgb_model = LGBMClassifier(**lgb_params, objective='multiclass', num_class=3)\n",
        "    lgb_model.fit(X, y, sample_weight=w_train)\n",
        "\n",
        "    # Train XGBoost\n",
        "    xgb_model = XGBClassifier(**xgb_params, objective='multi:softmax', num_class=3)\n",
        "    xgb_model.fit(X, y, sample_weight=w_train)\n",
        "\n",
        "    tabpfn_params = {\n",
        "        'device': 'cuda'\n",
        "    }\n",
        "\n",
        "     # Train TabPFN\n",
        "    tabpfn_model = TabPFNClassifier(**tabpfn_params)\n",
        "    tabpfn_model.fit(X, y)\n",
        "\n",
        "    # Get predictions and ensemble\n",
        "    lgb_pred = lgb_model.predict_proba(test_X)\n",
        "    xgb_pred = xgb_model.predict_proba(test_X)\n",
        "    tab_pred = tabpfn_model.predict_proba(test_X)\n",
        "\n",
        "    multiclass_test_preds = {\n",
        "        'lgb': lgb_pred,\n",
        "        'xgb': xgb_pred,\n",
        "        'tab': tab_pred\n",
        "    }\n",
        "\n",
        "    multiclass_pred = np.argmax(lgb_A * lgb_pred + xgb_B * xgb_pred + tab_C * tab_pred, axis=1)\n",
        "    multiclass_pred_proba = lgb_A * lgb_pred + xgb_B * xgb_pred + tab_C * tab_pred\n",
        "\n",
        "    # Feature importance\n",
        "    fi_df = pd.DataFrame({'feature': X.columns, 'importance': lgb_model.feature_importances_})\n",
        "    top10 = fi_df.sort_values(by='importance', ascending=False).head(10)\n",
        "    feat_str = \", \".join([f\"{row['feature']}({int(row['importance'])})\" for _, row in top10.iterrows()])\n",
        "    print(f\"[S1] {feat_str}\")\n",
        "\n",
        "    # ì˜ˆì¸¡ ì €ì¥\n",
        "    submission_final['S1'] = multiclass_pred\n",
        "    for col in targets_binary:\n",
        "      submission_final[col] = binary_preds[col]\n",
        "    submission_final = submission_final[['subject_id', 'sleep_date', 'lifelog_date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]\n",
        "    fname = f\"submission_{np.mean(total_avg_f1s)}.csv\"\n",
        "    submission_final.to_csv(fname, index=False)\n",
        "    print(f\"# {fname} ì €ì¥ ì™„ë£Œ\")\n",
        "    print(f\"# submission shape:{submission_final.shape}\")\n",
        "    print(\"================================================\")\n",
        "\n",
        "    # Top 10 Weight Combinations\n",
        "    print(\"\\nTop 10 Weight Combinations:\")\n",
        "    for i, (weights, score) in enumerate(zip(top_3_weights[:10], top_3_scores[:10])):\n",
        "        print(f\"Rank {i+1}: lgb_A={weights[0]:.1f}, xgb_B={weights[1]:.1f}, tab_C={weights[2]:.1f} - Score: {score:.4f}\")\n",
        "\n",
        "        # Generate submission with these weights\n",
        "        lgb_A, xgb_B, tab_C = weights\n",
        "\n",
        "        # Binary predictions\n",
        "        for col in targets_binary:\n",
        "            preds = binary_test_preds[col]\n",
        "            ensemble_pred = (lgb_A * preds['lgb'] + xgb_B * preds['xgb'] + tab_C * preds['tab'] > 0.5).astype(int)\n",
        "            submission_final[col] = ensemble_pred\n",
        "\n",
        "        # Multiclass prediction\n",
        "        preds = multiclass_test_preds\n",
        "        ensemble_pred = np.argmax(lgb_A * preds['lgb'] + xgb_B * preds['xgb'] + tab_C * preds['tab'], axis=1)\n",
        "        submission_final['S1'] = ensemble_pred\n",
        "\n",
        "        fname = f\"submission_top{i+1}_{score:.4f}.csv\"\n",
        "        submission_final.to_csv(fname, index=False)\n",
        "        print(f\"Saved submission to {fname}\")\n",
        "\n",
        "    # Use the best weights for final submission\n",
        "    best_weights = top_3_weights[0]\n",
        "    lgb_A, xgb_B, tab_C = best_weights\n",
        "\n",
        "    # ëª¨ë¸ë³„ ì˜ˆì¸¡ê²°ê³¼ ë¹„ìœ¨ ë¹„êµ\n",
        "    a11 = train_df[['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']].sum()\n",
        "    a13 = train_df[['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']].apply(len)\n",
        "    a12 = train_df[['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']].mean()\n",
        "    a21 = submission_final[['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']].sum()\n",
        "    a23 = submission_final[['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']].apply(len)\n",
        "    a22 = submission_final[['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']].mean()\n",
        "    result = pd.concat([a11, a13, a12, a21, a23, a22], axis=1)\n",
        "    result.columns = ['í•™ìŠµsum','í•™ìŠµlen','í•™ìŠµmean','í…ŒìŠ¤íŠ¸sum','í…ŒìŠ¤íŠ¸len','í…ŒìŠ¤íŠ¸mean']\n",
        "    print('\\n STEP3: ì˜ˆì¸¡ê²°ê³¼ ë¹„êµí‘œ')\n",
        "    display(result)\n",
        "    oof_result = []\n",
        "    return submission_final, oof_result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### seed"
      ],
      "metadata": {
        "id": "6Xslr-kVG1o3"
      },
      "id": "6Xslr-kVG1o3"
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(1)"
      ],
      "metadata": {
        "id": "Ly49bIaKG0-O"
      },
      "id": "Ly49bIaKG0-O",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ============================"
      ],
      "metadata": {
        "id": "lj8uKFxaKGbI"
      },
      "id": "lj8uKFxaKGbI"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7vvLSkwooZ0h",
      "metadata": {
        "id": "7vvLSkwooZ0h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pr98Y0D0KIPz"
      },
      "id": "Pr98Y0D0KIPz",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "U36LFTRioZ-g",
      "metadata": {
        "id": "U36LFTRioZ-g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "sg6Eiq-vOTr1",
      "metadata": {
        "id": "sg6Eiq-vOTr1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "HwoUMZO4LMb1",
      "metadata": {
        "id": "HwoUMZO4LMb1"
      },
      "source": [
        "### ğŸ“¦ ëª¨ë¸ í•™ìŠµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bf3543b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bf3543b7",
        "outputId": "26211a08-3404-479e-f7f9-29520d37de26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "light_week_type_lag1\n",
            "weekday\n",
            "week_type\n",
            "week_type_lag1\n",
            "activehour_top_bssid\n",
            "beforebed_top_bssid\n",
            "# X shape: (450, 247)\n",
            "# test_X shape: (250, 247)\n",
            "\n",
            " STEP1: ì‹¤í—˜ ê²°ê³¼ í™•ì¸\n",
            "=============== Validation Results ==============\n",
            " í‰ê·  F1: 0.6441 / [ìƒì„¸] Q1(ê¸°ìƒì§í›„ìˆ˜ë©´ì§ˆ):0.7234 Q2(ì·¨ì¹¨ì „ì‹ ì²´ì í”¼ë¡œ):0.8157 Q3(ì·¨ì¹¨ì „ìŠ¤íŠ¸ë ˆìŠ¤):0.6366 S2(ìˆ˜ë©´íš¨ìœ¨):0.5489 S3(ìˆ˜ë©´ì ë“¤ê¸°ì‹œê°„):0.7234 S1(S1):0.4163\n",
            "# ì „ì²´ í‰ê·  F1: 0.6441\n",
            "================================================\n",
            "\n",
            " STEP2: ì „ì²´ ë°ì´í„°ë¡œ ëª¨ë¸ ì¬í•™ìŠµ\n",
            "====== modeling with 100% train & no valid =====\n",
            "[Q1] beforebed_í†µí™”_time(1144), Q1_te2(477), wake_time_ratio(453), wake_time_diff(322), mlight_first_wakeup_minutes(310), Q1_te(290), lights_off_time(282), sleep_duration_ratio(214), active_hour_mean_speed(199), activehour_total_screen_time(181)\n",
            "[Q2] Q2_te(1990), Q2_te2(1827), activehour_total_screen_time(293), beforebed_unique_bssid_count(176), wake_time_lag1(163), light_rolling_wake_time_2d(161), activehour_screen_time_vs_avg_pct(144), beforebed_max_rssi(140), beforebed_top_bssid_count(139), active_hour_std_hr(134)\n",
            "[Q3] Q3_te2(2299), light_sleep_time_lag2(330), mlight_first_wakeup_minutes(288), rolling_sleep_time_3d(275), light_rolling_sleep_duration_3d(218), Q3_te(214), beforebed_scan_count(211), active_hour_distance_x(196), activehour_í†µí™”_time(181), walking_minutes(169)\n",
            "[S2] S2_te2(433), S2_te(422), light_sleep_time_lag1(198), work_hour_unknown_ratio(190), m_activity@240min@std@12h00m(176), beforebed_strong_signal_ratio(154), light_rolling_wake_time_2d(151), free_hour_rssi_mean(150), activehour_ì „í™”_time(147), sleep_hour_mean_speed(136)\n",
            "[S3] S3_te(2612), S3_te2(336), beforebed_ë©”ì‹ ì €_time(252), light_wake_time_diff(216), sleep_time_diff_lag1(209), light_sleep_time_lag2(199), m_activity_met@240min@sum@16h00m(156), free_hour_rssi_max(150), light_weekday_avg_sleep(137), ë¶ˆëˆì‹œê°„ë¶€í„°ê¸°ìƒì‹œê°„(126)\n",
            "[S1] S1_te(693), wake_time_diff(628), S1_te2(606), sleep_duration_ratio(495), m_activity_met@240min@sum@04h00m(413), beforebed_screen_time_vs_avg_pct(400), wake_time_ratio(382), rolling_wake_time_3d(340), m_activity_0@240min@std@20h00m(318), m_activity@240min@std@12h00m(316)\n",
            "# submission_0.6440575238969299.csv ì €ì¥ ì™„ë£Œ\n",
            "# submission shape:(250, 9)\n",
            "================================================\n",
            "\n",
            "Top 10 Weight Combinations:\n",
            "Rank 1: lgb_A=0.0, xgb_B=0.6, tab_C=0.1 - Score: 0.6696\n",
            "Saved submission to submission_top1_0.6696.csv\n",
            "Rank 2: lgb_A=0.0, xgb_B=0.5, tab_C=0.2 - Score: 0.6695\n",
            "Saved submission to submission_top2_0.6695.csv\n",
            "Rank 3: lgb_A=0.0, xgb_B=0.3, tab_C=0.4 - Score: 0.6681\n",
            "Saved submission to submission_top3_0.6681.csv\n",
            "Rank 4: lgb_A=0.0, xgb_B=0.3, tab_C=0.5 - Score: 0.6662\n",
            "Saved submission to submission_top4_0.6662.csv\n",
            "Rank 5: lgb_A=0.0, xgb_B=0.4, tab_C=0.4 - Score: 0.6656\n",
            "Saved submission to submission_top5_0.6656.csv\n",
            "Rank 6: lgb_A=0.1, xgb_B=0.2, tab_C=0.5 - Score: 0.6654\n",
            "Saved submission to submission_top6_0.6654.csv\n",
            "Rank 7: lgb_A=0.0, xgb_B=0.0, tab_C=0.7 - Score: 0.6647\n",
            "Saved submission to submission_top7_0.6647.csv\n",
            "Rank 8: lgb_A=0.1, xgb_B=0.3, tab_C=0.3 - Score: 0.6645\n",
            "Saved submission to submission_top8_0.6645.csv\n",
            "Rank 9: lgb_A=0.0, xgb_B=0.2, tab_C=0.6 - Score: 0.6625\n",
            "Saved submission to submission_top9_0.6625.csv\n",
            "Rank 10: lgb_A=0.1, xgb_B=0.3, tab_C=0.4 - Score: 0.6624\n",
            "Saved submission to submission_top10_0.6624.csv\n",
            "\n",
            " STEP3: ì˜ˆì¸¡ê²°ê³¼ ë¹„êµí‘œ\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    í•™ìŠµsum  í•™ìŠµlen  í•™ìŠµmean  í…ŒìŠ¤íŠ¸sum  í…ŒìŠ¤íŠ¸len  í…ŒìŠ¤íŠ¸mean\n",
              "Q1    223    450  0.4956     106     250   0.4240\n",
              "Q2    253    450  0.5622     126     250   0.5040\n",
              "Q3    270    450  0.6000     143     250   0.5720\n",
              "S1    390    450  0.8667     198     250   0.7920\n",
              "S2    293    450  0.6511     124     250   0.4960\n",
              "S3    298    450  0.6622     148     250   0.5920"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fb4941c7-db25-4ceb-8a47-4a180a00be99\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>í•™ìŠµsum</th>\n",
              "      <th>í•™ìŠµlen</th>\n",
              "      <th>í•™ìŠµmean</th>\n",
              "      <th>í…ŒìŠ¤íŠ¸sum</th>\n",
              "      <th>í…ŒìŠ¤íŠ¸len</th>\n",
              "      <th>í…ŒìŠ¤íŠ¸mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Q1</th>\n",
              "      <td>223</td>\n",
              "      <td>450</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>106</td>\n",
              "      <td>250</td>\n",
              "      <td>0.4240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Q2</th>\n",
              "      <td>253</td>\n",
              "      <td>450</td>\n",
              "      <td>0.5622</td>\n",
              "      <td>126</td>\n",
              "      <td>250</td>\n",
              "      <td>0.5040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Q3</th>\n",
              "      <td>270</td>\n",
              "      <td>450</td>\n",
              "      <td>0.6000</td>\n",
              "      <td>143</td>\n",
              "      <td>250</td>\n",
              "      <td>0.5720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S1</th>\n",
              "      <td>390</td>\n",
              "      <td>450</td>\n",
              "      <td>0.8667</td>\n",
              "      <td>198</td>\n",
              "      <td>250</td>\n",
              "      <td>0.7920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S2</th>\n",
              "      <td>293</td>\n",
              "      <td>450</td>\n",
              "      <td>0.6511</td>\n",
              "      <td>124</td>\n",
              "      <td>250</td>\n",
              "      <td>0.4960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S3</th>\n",
              "      <td>298</td>\n",
              "      <td>450</td>\n",
              "      <td>0.6622</td>\n",
              "      <td>148</td>\n",
              "      <td>250</td>\n",
              "      <td>0.5920</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb4941c7-db25-4ceb-8a47-4a180a00be99')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fb4941c7-db25-4ceb-8a47-4a180a00be99 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fb4941c7-db25-4ceb-8a47-4a180a00be99');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ecd86408-9f51-4408-97c8-c49a6e7e7b3e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ecd86408-9f51-4408-97c8-c49a6e7e7b3e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ecd86408-9f51-4408-97c8-c49a6e7e7b3e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"get_ipython()\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"\\ud559\\uc2b5sum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57,\n        \"min\": 223,\n        \"max\": 390,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          223,\n          253,\n          298\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ud559\\uc2b5len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 450,\n        \"max\": 450,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          450\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ud559\\uc2b5mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1268919374350011,\n        \"min\": 0.4955555555555556,\n        \"max\": 0.8666666666666667,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.4955555555555556\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ud14c\\uc2a4\\ud2b8sum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 31,\n        \"min\": 106,\n        \"max\": 198,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          106\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ud14c\\uc2a4\\ud2b8len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 250,\n        \"max\": 250,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          250\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\ud14c\\uc2a4\\ud2b8mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12700656151028838,\n        \"min\": 0.424,\n        \"max\": 0.792,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.424\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 20min 56s, sys: 2.44 s, total: 20min 58s\n",
            "Wall time: 3min 10s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# ê³µí†µ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "common_params = {\n",
        "  'n_estimators': 5000,\n",
        "  \"learning_rate\": 0.01,\n",
        "  # 'min_data_in_leaf':2,\n",
        "  # 'bagging_fraction':0.9,\n",
        "  # 'feature_fraction':0.6,\n",
        "  'lambda_l1': 5,\n",
        "  'lambda_l2': 1,\n",
        "  # 'max_depth': 4,\n",
        "  'n_jobs': -1,\n",
        "  'verbosity': -1\n",
        "}\n",
        "\n",
        "# ëª¨ë¸ë³„ ì„¸ë¶€ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "best_param_dict = {}\n",
        "best_param_dict['Q3'] = common_params\n",
        "best_param_dict['S1'] = common_params\n",
        "best_param_dict['S2'] = common_params\n",
        "best_param_dict['S3'] = common_params\n",
        "best_param_dict['Q1'] = common_params\n",
        "best_param_dict['Q2'] = common_params\n",
        "\n",
        "\"\"\"\n",
        "// submission_top1_0.6492.csv\n",
        "\n",
        "light_week_type_lag1\n",
        "weekday\n",
        "week_type\n",
        "week_type_lag1\n",
        "activehour_top_bssid\n",
        "beforebed_top_bssid\n",
        "# X shape: (450, 247)\n",
        "# test_X shape: (250, 247)\n",
        "\n",
        " STEP1: ì‹¤í—˜ ê²°ê³¼ í™•ì¸\n",
        "=============== Validation Results ==============\n",
        "tabpfn-v2-classifier.ckpt:â€‡100%\n",
        "â€‡29.0M/29.0Mâ€‡[00:00<00:00,â€‡78.9MB/s]\n",
        "config.json:â€‡100%\n",
        "â€‡37.0/37.0â€‡[00:00<00:00,â€‡4.81kB/s]\n",
        " í‰ê·  F1: 0.6441 / [ìƒì„¸] Q1(ê¸°ìƒì§í›„ìˆ˜ë©´ì§ˆ):0.7234 Q2(ì·¨ì¹¨ì „ì‹ ì²´ì í”¼ë¡œ):0.8157 Q3(ì·¨ì¹¨ì „ìŠ¤íŠ¸ë ˆìŠ¤):0.6366 S2(ìˆ˜ë©´íš¨ìœ¨):0.5489 S3(ìˆ˜ë©´ì ë“¤ê¸°ì‹œê°„):0.7234 S1(S1):0.4163\n",
        "# ì „ì²´ í‰ê·  F1: 0.6441\n",
        "================================================\n",
        "\n",
        " STEP2: ì „ì²´ ë°ì´í„°ë¡œ ëª¨ë¸ ì¬í•™ìŠµ\n",
        "====== modeling with 100% train & no valid =====\n",
        "[Q1] beforebed_í†µí™”_time(1144), Q1_te2(477), wake_time_ratio(453), wake_time_diff(322), mlight_first_wakeup_minutes(310), Q1_te(290), lights_off_time(282), sleep_duration_ratio(214), active_hour_mean_speed(199), activehour_total_screen_time(181)\n",
        "[Q2] Q2_te(1990), Q2_te2(1827), activehour_total_screen_time(293), beforebed_unique_bssid_count(176), wake_time_lag1(163), light_rolling_wake_time_2d(161), activehour_screen_time_vs_avg_pct(144), beforebed_max_rssi(140), beforebed_top_bssid_count(139), active_hour_std_hr(134)\n",
        "[Q3] Q3_te2(2299), light_sleep_time_lag2(330), mlight_first_wakeup_minutes(288), rolling_sleep_time_3d(275), light_rolling_sleep_duration_3d(218), Q3_te(214), beforebed_scan_count(211), active_hour_distance_x(196), activehour_í†µí™”_time(181), walking_minutes(169)\n",
        "[S2] S2_te2(433), S2_te(422), light_sleep_time_lag1(198), work_hour_unknown_ratio(190), m_activity@240min@std@12h00m(176), beforebed_strong_signal_ratio(154), light_rolling_wake_time_2d(151), free_hour_rssi_mean(150), activehour_ì „í™”_time(147), sleep_hour_mean_speed(136)\n",
        "[S3] S3_te(2612), S3_te2(336), beforebed_ë©”ì‹ ì €_time(252), light_wake_time_diff(216), sleep_time_diff_lag1(209), light_sleep_time_lag2(199), m_activity_met@240min@sum@16h00m(156), free_hour_rssi_max(150), light_weekday_avg_sleep(137), ë¶ˆëˆì‹œê°„ë¶€í„°ê¸°ìƒì‹œê°„(126)\n",
        "[S1] S1_te(693), wake_time_diff(628), S1_te2(606), sleep_duration_ratio(495), m_activity_met@240min@sum@04h00m(413), beforebed_screen_time_vs_avg_pct(400), wake_time_ratio(382), rolling_wake_time_3d(340), m_activity_0@240min@std@20h00m(318), m_activity@240min@std@12h00m(316)\n",
        "# submission_0.6440575238969299.csv ì €ì¥ ì™„ë£Œ\n",
        "# submission shape:(250, 9)\n",
        "================================================\n",
        "\n",
        "Top 3 Weight Combinations:\n",
        "Rank 1: lgb_A=0.0, xgb_B=0.6, tab_C=0.1 - Score: 0.6696\n",
        "Saved submission to submission_top1_0.6696.csv\n",
        "Rank 2: lgb_A=0.0, xgb_B=0.5, tab_C=0.2 - Score: 0.6695\n",
        "Saved submission to submission_top2_0.6695.csv\n",
        "Rank 3: lgb_A=0.0, xgb_B=0.3, tab_C=0.4 - Score: 0.6681\n",
        "Saved submission to submission_top3_0.6681.csv\n",
        "Rank 4: lgb_A=0.0, xgb_B=0.3, tab_C=0.5 - Score: 0.6662\n",
        "Saved submission to submission_top4_0.6662.csv\n",
        "Rank 5: lgb_A=0.0, xgb_B=0.4, tab_C=0.4 - Score: 0.6656\n",
        "Saved submission to submission_top5_0.6656.csv\n",
        "Rank 6: lgb_A=0.1, xgb_B=0.2, tab_C=0.5 - Score: 0.6654\n",
        "Saved submission to submission_top6_0.6654.csv\n",
        "Rank 7: lgb_A=0.0, xgb_B=0.0, tab_C=0.7 - Score: 0.6647\n",
        "Saved submission to submission_top7_0.6647.csv\n",
        "Rank 8: lgb_A=0.1, xgb_B=0.3, tab_C=0.3 - Score: 0.6645\n",
        "Saved submission to submission_top8_0.6645.csv\n",
        "Rank 9: lgb_A=0.0, xgb_B=0.2, tab_C=0.6 - Score: 0.6625\n",
        "Saved submission to submission_top9_0.6625.csv\n",
        "Rank 10: lgb_A=0.1, xgb_B=0.3, tab_C=0.4 - Score: 0.6624\n",
        "Saved submission to submission_top10_0.6624.csv\n",
        "\n",
        " STEP3: ì˜ˆì¸¡ê²°ê³¼ ë¹„êµí‘œ\n",
        "í•™ìŠµsum\tí•™ìŠµlen\tí•™ìŠµmean\tí…ŒìŠ¤íŠ¸sum\tí…ŒìŠ¤íŠ¸len\tí…ŒìŠ¤íŠ¸mean\n",
        "Q1\t223\t450\t0.4956\t106\t250\t0.4240\n",
        "Q2\t253\t450\t0.5622\t126\t250\t0.5040\n",
        "Q3\t270\t450\t0.6000\t143\t250\t0.5720\n",
        "S1\t390\t450\t0.8667\t198\t250\t0.7920\n",
        "S2\t293\t450\t0.6511\t124\t250\t0.4960\n",
        "S3\t298\t450\t0.6622\t148\t250\t0.5920\n",
        "\n",
        "CPU times: user 17min 32s, sys: 2.28 s, total: 17min 35s\n",
        "Wall time: 2min 40s\n",
        "\"\"\"\n",
        "\n",
        "submission_final, oof_result = run_basemodel(train, test, valid_ids, best_param_dict, n_splits=5, random_state=41)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hRrvmB5aHb4G"
      },
      "id": "hRrvmB5aHb4G",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "s5N7SlvaLgh-",
      "metadata": {
        "id": "s5N7SlvaLgh-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "EHSoZgGiLgsd",
      "metadata": {
        "id": "EHSoZgGiLgsd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "nsZ0kEKjLerl",
      "metadata": {
        "id": "nsZ0kEKjLerl"
      },
      "source": [
        "### ğŸ“¦ ì´ì „ì œì¶œê³¼ ë¹„êµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8d5e619e",
      "metadata": {
        "id": "8d5e619e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1b08600-368f-4c2d-d152-f8f05874c6bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 files with smallest differences:\n",
            "01. submission_0.6440575238969299.csv: 103 differences\n",
            "02. submission_top5_0.6656.csv: 137 differences\n",
            "03. submission_top10_0.6624.csv: 146 differences\n",
            "04. submission_top4_0.6662.csv: 159 differences\n",
            "05. submission_top6_0.6654.csv: 168 differences\n",
            "06. submission_top9_0.6625.csv: 182 differences\n",
            "07. submission_top1_0.6696.csv: 188 differences\n",
            "08. submission_top2_0.6695.csv: 213 differences\n",
            "09. submission_top8_0.6645.csv: 263 differences\n",
            "10. submission_top3_0.6681.csv: 273 differences\n",
            "11. submission_top7_0.6647.csv: 410 differences\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Reference file\n",
        "reference_file = '/content/drive/MyDrive/data/ch2025_data_items/share/submissions/submission_top1_0.6492.csv'\n",
        "ref_df = pd.read_csv(reference_file)\n",
        "\n",
        "# Get all CSV files in data directory\n",
        "data_dir = Path('./')\n",
        "csv_files = list(data_dir.glob('*.csv'))\n",
        "\n",
        "# Store differences for each file\n",
        "differences = []\n",
        "\n",
        "for csv_file in csv_files:\n",
        "    if csv_file.name == os.path.basename(reference_file):\n",
        "        continue\n",
        "\n",
        "    # Read current file\n",
        "    current_df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Calculate differences in specified columns\n",
        "    diff_count = 0\n",
        "    for col in ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']:\n",
        "        diff_count += (ref_df[col] != current_df[col]).sum()\n",
        "\n",
        "    differences.append((csv_file.name, diff_count))\n",
        "    # print(f\"File: {csv_file.name}, Differences: {diff_count}\")\n",
        "\n",
        "# Sort by difference count and get top 20\n",
        "differences.sort(key=lambda x: x[1])\n",
        "print(\"\\nTop 10 files with smallest differences:\")\n",
        "for i, (file_name, diff_count) in enumerate(differences[:20], 1):\n",
        "    print(f\"{str(i).zfill(2)}. {file_name}: {diff_count} differences\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d2b24160",
      "metadata": {
        "id": "d2b24160"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "29b4a176",
      "metadata": {
        "id": "29b4a176"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "79079453",
      "metadata": {
        "id": "79079453"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ee90ed8f",
      "metadata": {
        "id": "ee90ed8f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "694fc8ae",
      "metadata": {
        "id": "694fc8ae"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8744d38a",
      "metadata": {
        "id": "8744d38a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e496fd76",
      "metadata": {
        "id": "e496fd76"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 7210916,
          "sourceId": 12085434,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 4148.996629,
      "end_time": "2025-06-07T13:22:50.273930",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-06-07T12:13:41.277301",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
