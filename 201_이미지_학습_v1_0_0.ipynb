{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobgreen4477/The-4th-ETRI-AI-Human-Understanding-Competition/blob/main/201_%EC%9D%B4%EB%AF%B8%EC%A7%80_%ED%95%99%EC%8A%B5_v1_0_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTgURBTcpY0Q"
      },
      "source": [
        "> title : 제 4회 ETRI 휴먼이해 인공지능 논문경진대회 <br>\n",
        "> author : hjy <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QhncbejZIfV"
      },
      "source": [
        "In our study, we used smartphones, smartwatches, sleep sensors, and self-recording apps to collect daily life logs and sleep health records of study participants in 2024.The data collection procedures and methods followed a similar approach to those used in previous studies. Here, we pu﻿blicly provide the following 12 data items, which comprise a total of 700 days' worth of lifelog data, strictly for non-commercial and academic research purposes only.\n",
        "- mACStatus: Indicates whether the smartphone is currently being charged.\n",
        "- mActivity: Value calculated by the Google Activity Recognition API.\n",
        "- mAmbience: Ambient sound identification labels and their respective probabilities.\n",
        "- mBle: Bluetooth devices around individual subject.\n",
        "- mGps: Multiple GPS coordinates measured within a single minute using the smartphone.\n",
        "- mLight: Ambient light measured by the smartphone.\n",
        "- mScreenStatus: Indicates whether the smartphone screen is in use.\n",
        "- mUsageStats: Indicates which apps were used on the smartphone and for how long.\n",
        "- mWifi: Wifi devices around individual subject.\n",
        "- wHr: Heart rate readings recorded by the smartwatch.\n",
        "- wLight: Ambient light measured by the smartwatch.\n",
        "- wPedo: Step data recorded by the smartwatch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkY5S7k0ZLFG"
      },
      "source": [
        "For the purpose of training a learning model to predict sleep health, fatigue, and stress, the following six metrics were derived from sleep sensor data and self-reported survey records. Each metric consists of values categorized into either two levels (0, 1) or three levels (0, 1, 2), depending on the specific metric. The detailed classification criteria for each metric's levels will be provided in a separate document.These\n",
        "metrics assign a value of 0 for sleep records that do not meet the recommended guidelines.For instance, the first questionnaire metric (Q1) is assigned a value of 1 on days when an\n",
        "individual’s self-reported sleep quality exceeds their average over the experimental period, and 0 when it\n",
        "falls below that average. Similarly, the second and third metrics (Q2 and Q3) are assigned a value of 0\n",
        "on days when the participant’s fatigue and stress levels, respectively, exceed their average, and a value of\n",
        "1 when these levels are below average.\n",
        "\n",
        "- Q1: Overall sleep quality as perceived by a subject immediately after waking up.\n",
        "- Q2: Physical fatigue of a subject just before sleep.\n",
        "- Q3: Stress level experienced by a subject just before sleep.\n",
        "- S1: Adherence to sleep guidelines for total sleep time (TST).\n",
        "- S2: Adherence to sleep guidelines for sleep efficiency (SE).\n",
        "- S3: Adherence to sleep guidelines for sleep onset latency (SOL, or SL).\n",
        "\n",
        "수면 건강, 피로, 스트레스 예측을 위한 학습 모델을 훈련시키기 위해, 수면 센서 데이터와 자기 보고식 설문 기록을 기반으로 다음의 6가지 지표를 도출했습니다.\n",
        "각 지표는 해당 항목에 따라 두 수준(0, 1) 또는 세 수준(0, 1, 2)으로 구분된 값을 가집니다.\n",
        "각 지표의 세부 분류 기준은 별도의 문서에서 제공될 예정입니다.\n",
        "\n",
        "- Q1: 기상 직후 본인이 인지한 전반적인 수면의 질\n",
        " - 0: 개인 평균 이하\n",
        " - 1: 개인 평균 이상\n",
        "- Q2: 취침 직전 본인이 느낀 신체적 피로 수준\n",
        " - 0: 높은 피로 수준\n",
        " - 1: 낮은 피로 수준\n",
        "- Q3: 취침 직전 본인이 느낀 스트레스 수준\n",
        " - 0: 높은 스트레스 수준\n",
        " - 1: 낮은 스트레스 수준\n",
        "- S1: 총 수면 시간(TST) 가이드라인을 준수했는지 3LEVELS\n",
        " - 0: 가이드라인 미준수\n",
        " - 1: 가이드라인 부분적 준수\n",
        " - 2: 가이드라인 완전 준수\n",
        "- S2: 수면 효율(SE) 가이드라인을 준수했는지 여부\n",
        "- (SE: 잠자리에 누워 있었던 전체 시간 대비, 실제로 잠든 시간의 비율)\n",
        " - 0: 가이드라인 미준수\n",
        " - 1: 가이드라인 준수\n",
        "- S3: 수면 잠들기 지연 시간(SOL 또는 SL) 가이드라인을 준수했는지 여부\n",
        "- (SOL: 잠자리에 누운 순간부터 실제로 잠드는 데까지 걸린 시간)\n",
        " - 0: 가이드라인 미준수\n",
        " - 1: 가이드라인 준수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDVNXLQtLU6X"
      },
      "source": [
        "### 📦 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN6iwVhQpR_a",
        "outputId": "f6b475cb-bbfe-480e-d46a-8eccecbb3ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting haversine\n",
            "  Downloading haversine-2.9.0-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading haversine-2.9.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Installing collected packages: haversine\n",
            "Successfully installed haversine-2.9.0\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.15.2 colorlog-6.9.0 optuna-4.3.0\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.8.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.15.3)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (0.14.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Downloading category_encoders-2.8.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.8.1\n"
          ]
        }
      ],
      "source": [
        "! pip install haversine\n",
        "! pip install optuna\n",
        "! pip install category_encoders\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "from tqdm.auto import tqdm\n",
        "from collections import Counter\n",
        "from scipy.stats import entropy\n",
        "from haversine import haversine  # 설치 필요: pip install haversine\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFvEVmxWsRH4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import ast\n",
        "from tqdm import tqdm  # ← 추가\n",
        "from math import radians, cos, sin, asin, sqrt\n",
        "from datetime import time\n",
        "from datetime import timedelta\n",
        "from functools import reduce\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "import glob\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, f1_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, cross_val_score\n",
        "import lightgbm as lgb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from category_encoders import TargetEncoder\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# seed 고정\n",
        "SD = 42\n",
        "random.seed(SD)\n",
        "np.random.seed(SD)\n",
        "os.environ['PYTHONHASHSEED'] = str(SD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iVOoFq7pSCM",
        "outputId": "40e24aa9-84fb-48a0-9265-66b756598585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIbC9LQkv6T4"
      },
      "outputs": [],
      "source": [
        "# pandas 옵션\n",
        "pd.set_option('display.max_columns', 999)\n",
        "pd.set_option('display.max_rows', 999)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.float_format', lambda x: '%0.4f' % x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38uzdH-UYh_3"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_ZQfubWYiCk"
      },
      "outputs": [],
      "source": [
        "def correct_lifelog_date_for_midnight(df, timestamp_col='timestamp', lifelog_col='lifelog_date'):\n",
        "    df = df.copy()\n",
        "    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
        "    df[lifelog_col] = pd.to_datetime(df[lifelog_col])\n",
        "\n",
        "    # 조건: timestamp의 시(hour)가 0~5시인 경우만 하루 차감\n",
        "    mask = (df[timestamp_col].dt.hour >= 0) & (df[timestamp_col].dt.hour < 6)\n",
        "    df.loc[mask, lifelog_col] = df.loc[mask, lifelog_col] - pd.Timedelta(days=1)\n",
        "\n",
        "    # lifelog_date를 문자열로 바꾸는 경우\n",
        "    df[lifelog_col] = df[lifelog_col].dt.date.astype(str)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ikO0GN_KxyQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EvDe55ejzKR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQgtvPb3jzQv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEHsA6naKx0G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BodxdJFiv_DJ"
      },
      "source": [
        "### 📦 데이터 읽기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw0cx3wwpSE2"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/data/ch2025_data_items/'\n",
        "\n",
        "# 1\n",
        "mACStatus = pd.read_parquet(path+'ch2025_mACStatus.parquet')\n",
        "mActivity = pd.read_parquet(path+'ch2025_mActivity.parquet')\n",
        "mAmbience = pd.read_parquet(path+'ch2025_mAmbience.parquet')\n",
        "mBle = pd.read_parquet(path+'ch2025_mBle.parquet')\n",
        "mGps = pd.read_parquet(path+'ch2025_mGps.parquet')\n",
        "mLight = pd.read_parquet(path+'ch2025_mLight.parquet')\n",
        "mScreenStatus = pd.read_parquet(path+'ch2025_mScreenStatus.parquet')\n",
        "mUsageStats = pd.read_parquet(path+'ch2025_mUsageStats.parquet')\n",
        "mWifi = pd.read_parquet(path+'ch2025_mWifi.parquet')\n",
        "wHr = pd.read_parquet(path+'ch2025_wHr.parquet')\n",
        "wLight = pd.read_parquet(path+'ch2025_wLight.parquet')\n",
        "wPedo = pd.read_parquet(path+'ch2025_wPedo.parquet')\n",
        "\n",
        "# 2\n",
        "train = pd.read_csv('/content/drive/MyDrive/data/ch2025_metrics_train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/data/ch2025_submission_sample.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tk45v0V5xiay"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "mACStatus['lifelog_date'] = mACStatus['timestamp'].astype(str).str[:10]\n",
        "mActivity['lifelog_date'] = mActivity['timestamp'].astype(str).str[:10]\n",
        "\n",
        "# def extract_labels_and_probs(row):\n",
        "#     items = row['m_ambience']\n",
        "#     labels = [item[0] for item in items]\n",
        "#     probs = [item[1] for item in items]\n",
        "#     return pd.Series({'labels': labels, 'prob': probs})\n",
        "\n",
        "# mAmbience[['labels', 'prob']]  = mAmbience.apply(extract_labels_and_probs, axis=1)\n",
        "# mAmbience['lifelog_date'] = mAmbience['timestamp'].astype(str).str[:10]\n",
        "# mAmbience = mAmbience.drop(columns=['m_ambience'])\n",
        "\n",
        "# def extract_mble_info(row):\n",
        "#     m_data = row['m_ble']\n",
        "#     address = [item['address'] for item in m_data]\n",
        "#     device_class = [item['device_class'] for item in m_data]\n",
        "#     rssi = [item['rssi'] for item in m_data]\n",
        "#     return pd.Series({'address': address, 'device_class': device_class, 'rssi': rssi})\n",
        "\n",
        "# mBle[['address','device_class','rssi']] = mBle.apply(extract_mble_info, axis=1)\n",
        "# mBle['lifelog_date'] = mBle['timestamp'].astype(str).str[:10]\n",
        "\n",
        "# def extract_gps_info(row):\n",
        "#     m_data = row['m_gps']\n",
        "#     altitude = [item['altitude'] for item in m_data]\n",
        "#     latitude = [item['latitude'] for item in m_data]\n",
        "#     longitude = [item['longitude'] for item in m_data]\n",
        "#     speed = [item['speed'] for item in m_data]\n",
        "#     return pd.Series({'altitude': altitude, 'latitude': latitude, 'longitude': longitude, 'speed': speed})\n",
        "\n",
        "# mGps[['altitude','latitude','longitude','speed']] = mGps.apply(extract_gps_info, axis=1)\n",
        "# mGps['lifelog_date'] = mGps['timestamp'].astype(str).str[:10]\n",
        "# mGps = mGps.drop(columns=['m_gps'])\n",
        "\n",
        "mLight['lifelog_date'] = mLight['timestamp'].astype(str).str[:10]\n",
        "mScreenStatus['lifelog_date'] = mScreenStatus['timestamp'].astype(str).str[:10]\n",
        "\n",
        "# def extract_mUsageStats_info(row):\n",
        "#     m_data = row['m_usage_stats']\n",
        "#     app_name = [item['app_name'] for item in m_data]\n",
        "#     total_time = [item['total_time'] for item in m_data]\n",
        "#     return pd.Series({'app_name': app_name, 'total_time': total_time})\n",
        "\n",
        "# mUsageStats[['app_name', 'total_time']] = mUsageStats.apply(extract_mUsageStats_info, axis=1)\n",
        "# mUsageStats['lifelog_date'] = mUsageStats['timestamp'].astype(str).str[:10]\n",
        "\n",
        "# def extract_wifi_info(row):\n",
        "#     wifi_data = row['m_wifi']\n",
        "#     bssids = [item['bssid'] for item in wifi_data]\n",
        "#     rssis = [item['rssi'] for item in wifi_data]\n",
        "#     return pd.Series({'bssid': bssids, 'rssi': rssis})\n",
        "\n",
        "# mWifi[['bssid', 'rssi']] = mWifi.apply(extract_wifi_info, axis=1)\n",
        "# mWifi['lifelog_date'] = mWifi['timestamp'].astype(str).str[:10]\n",
        "\n",
        "wHr['lifelog_date'] = wHr['timestamp'].astype(str).str[:10]\n",
        "wLight['lifelog_date'] = wLight['timestamp'].astype(str).str[:10]\n",
        "wPedo['lifelog_date'] = wPedo['timestamp'].astype(str).str[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp0et2XoiBZ7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE-06HN6gtqy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBZKGmxSRBQW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAyS_RTHjjp3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp40u0R88PY_"
      },
      "source": [
        "### 📌 이미지 생성\n",
        "- spleeptime만 추출 (00시부터 06시까지)\n",
        "- 참고 : https://github.com/seongjiko/Pixleep/tree/master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-qgvZd-8cir"
      },
      "outputs": [],
      "source": [
        "def filter_by_group_size(df, group_cols=['subject_id', 'lifelog_date']):\n",
        "    # 그룹별 건수 계산\n",
        "    group_counts = df.groupby(group_cols).size().reset_index(name='count')\n",
        "    # 평균 건수 계산\n",
        "    mean_count = group_counts['count'].mean()\n",
        "    # 평균 초과 그룹만 추출\n",
        "    valid_groups = group_counts[group_counts['count'] > mean_count*0.5][group_cols]\n",
        "    # 원본과 inner join으로 필터링\n",
        "    return df.merge(valid_groups, on=group_cols, how='inner')\n",
        "\n",
        "def make_timestamps_unique(df, timestamp_col='timestamp'):\n",
        "    # 'timestamp' 컬럼을 기준으로 정렬\n",
        "    df = df.sort_values(by=[timestamp_col])\n",
        "    # 각 'timestamp'가 중복된 횟수를 세어 나노초 단위로 증가시킴\n",
        "    df[timestamp_col] = df[timestamp_col] + pd.to_timedelta(df.groupby(timestamp_col).cumcount(), unit='ns')\n",
        "    return df\n",
        "\n",
        "def average_list_columns(df, list_columns, pk_cols=['subject_id', 'lifelog_date']):\n",
        "\n",
        "    for col in list_columns:\n",
        "\n",
        "        def safe_mean(x):\n",
        "            if isinstance(x, list):\n",
        "                return np.mean(x) if len(x) > 0 else np.nan\n",
        "            elif isinstance(x, (int, float, np.integer, np.floating, type(None))):\n",
        "                return x\n",
        "            elif isinstance(x, (np.ndarray, pd.Series)):\n",
        "                return np.mean(x)\n",
        "            elif pd.api.types.is_scalar(x) and pd.isna(x):\n",
        "                return np.nan\n",
        "            else:\n",
        "                return np.nan\n",
        "\n",
        "        df[col] = df[col].apply(safe_mean)\n",
        "\n",
        "    return df\n",
        "\n",
        "def center_list_values(df, list_columns):\n",
        "    for col in list_columns:\n",
        "        def center(x):\n",
        "            if isinstance(x, list) and len(x) > 0:\n",
        "                mean = np.mean(x)\n",
        "                return [np.round(v - mean,3) for v in x]\n",
        "            return x  # NaN이나 비리스트는 그대로 유지\n",
        "        df[col] = df[col].apply(center)\n",
        "    return df\n",
        "\n",
        "def sleeptime_cutter(data): # 잠자는 시간 데이터가 더 중요한지 실험(🔥🔥🔥)\n",
        "\n",
        "    data_filtered = data.copy()\n",
        "    data_filtered['timestamp'] = pd.to_datetime(data_filtered['timestamp'])\n",
        "    data_filtered['lifelog_date'] = pd.to_datetime(data_filtered['lifelog_date'])\n",
        "\n",
        "    # spleeptime만 추출 (00시부터 06시까지)\n",
        "    data_filtered = data_filtered[(data_filtered['timestamp'].dt.hour >= 0) & (data_filtered['timestamp'].dt.hour < 6)]\n",
        "\n",
        "    # 하루 차감\n",
        "    data_filtered['timestamp'] = data_filtered['timestamp'] - pd.Timedelta(days=1)\n",
        "    data_filtered['lifelog_date'] = data_filtered['lifelog_date'] - pd.Timedelta(days=1)\n",
        "    # print('>> D-1 하루 차감! (lifelog_date 실제 일자는 D+1 새벽(0~6시) 데이터임)')\n",
        "\n",
        "    # lifelog_date를 다시 문자열로\n",
        "    data_filtered['lifelog_date'] = data_filtered['lifelog_date'].dt.date.astype(str)\n",
        "\n",
        "    return data_filtered\n",
        "\n",
        "def merge_data_for_group(user, date):\n",
        "\n",
        "    # 데이터 로드\n",
        "    # acc_group = mGps.copy()\n",
        "    activity_group = mActivity.copy()\n",
        "    hr_group = wHr.copy()\n",
        "    wPedo_group = wPedo[['subject_id','timestamp','lifelog_date','step']].copy()\n",
        "    mLight_group = mLight[['subject_id','timestamp','lifelog_date','m_light']].copy()\n",
        "    wLight_group = wLight[['subject_id','timestamp','lifelog_date','w_light']].copy()\n",
        "\n",
        "    # 건수가 없는 일자 이상치로 판단하고 제외\n",
        "    activity_group = filter_by_group_size(activity_group)\n",
        "    hr_group = filter_by_group_size(hr_group)\n",
        "    wPedo_group = filter_by_group_size(wPedo_group)\n",
        "    mLight_group = filter_by_group_size(mLight_group)\n",
        "    wLight_group = filter_by_group_size(wLight_group)\n",
        "\n",
        "    # sleeptime만 남기고 나머지 삭제 (🔥🔥🔥)\n",
        "    # activity_group = sleeptime_cutter(activity_group)\n",
        "    # hr_group = sleeptime_cutter(hr_group)\n",
        "    # wPedo_group = sleeptime_cutter(wPedo_group)\n",
        "    # mLight_group = sleeptime_cutter(mLight_group)\n",
        "    # wLight_group = sleeptime_cutter(wLight_group)\n",
        "\n",
        "    # 필터\n",
        "    activity_group = activity_group.loc[(activity_group['subject_id']==user) & (activity_group['lifelog_date']==date),:]\n",
        "    hr_group = hr_group.loc[(hr_group['subject_id']==user) & (hr_group['lifelog_date']==date),:]\n",
        "    wPedo_group = wPedo_group.loc[(wPedo_group['subject_id']==user) & (wPedo_group['lifelog_date']==date),:]\n",
        "    mLight_group = mLight_group.loc[(mLight_group['subject_id']==user) & (mLight_group['lifelog_date']==date),:]\n",
        "    wLight_group = wLight_group.loc[(wLight_group['subject_id']==user) & (wLight_group['lifelog_date']==date),:]\n",
        "\n",
        "    # 리스트 평균값으로 변환\n",
        "    # acc_group = average_list_columns(acc_group, ['altitude', 'latitude', 'longitude','speed'])\n",
        "    hr_group = average_list_columns(hr_group, ['heart_rate'])\n",
        "\n",
        "    # 'timestamp'를 고유하게 만듦\n",
        "    # acc_group = make_timestamps_unique(acc_group)\n",
        "    activity_group = make_timestamps_unique(activity_group)\n",
        "    hr_group = make_timestamps_unique(hr_group)\n",
        "    wPedo_group = make_timestamps_unique(wPedo_group)\n",
        "    mLight_group = make_timestamps_unique(mLight_group)\n",
        "    wLight_group = make_timestamps_unique(wLight_group)\n",
        "\n",
        "    # 'timestamp'를 인덱스로 설정하고 'subject_id'와 'date' 컬럼 제거\n",
        "    # mAcc_data = acc_group.set_index('timestamp').drop(columns=['subject_id', 'lifelog_date']).resample('S').nearest()\n",
        "    activity_data = activity_group.set_index('timestamp').drop(columns=['subject_id', 'lifelog_date']).resample('S').nearest()\n",
        "    e4Hr_data = hr_group.set_index('timestamp').drop(columns=['subject_id', 'lifelog_date']).resample('S').nearest()\n",
        "    wPedo_data = wPedo_group.set_index('timestamp').drop(columns=['subject_id', 'lifelog_date']).resample('S').nearest()\n",
        "    mLight_data = mLight_group.set_index('timestamp').drop(columns=['subject_id', 'lifelog_date']).resample('S').nearest()\n",
        "    wLight_data = wLight_group.set_index('timestamp').drop(columns=['subject_id', 'lifelog_date']).resample('S').nearest()\n",
        "\n",
        "    # 하루 86400초의 타임스탬프 생성\n",
        "    start_time = datetime.strptime(date, '%Y-%m-%d')\n",
        "    end_time = start_time + timedelta(days=1)\n",
        "    all_timestamps = pd.date_range(start=start_time, end=end_time, freq='S', inclusive='left')\n",
        "    merged_data = pd.DataFrame(index=all_timestamps)\n",
        "    merged_data.index.name = 'timestamp'\n",
        "\n",
        "    # 데이터 병합\n",
        "    # if not mAcc_data.empty:\n",
        "    #     merged_data = merged_data.join(mAcc_data, how='left')\n",
        "    if not e4Hr_data.empty:\n",
        "        merged_data = merged_data.join(e4Hr_data, how='left')\n",
        "    if not activity_data.empty:\n",
        "        merged_data = merged_data.join(activity_data, how='left')\n",
        "    if not wPedo_data.empty:\n",
        "        merged_data = merged_data.join(wPedo_data, how='left')\n",
        "    if not mLight_data.empty:\n",
        "        merged_data = merged_data.join(mLight_data, how='left')\n",
        "    if not wLight_data.empty:\n",
        "        merged_data = merged_data.join(wLight_data, how='left')\n",
        "\n",
        "    # 필요한 컬럼만 유지하고 NaN 값으로 채우기\n",
        "    # merged_data = merged_data.reindex(columns=['altitude', 'latitude', 'longitude', 'speed', 'heart_rate', 'm_activity', 'step'])\n",
        "    merged_data = merged_data.reindex(columns=['heart_rate', 'm_activity', 'step', 'm_light', 'w_light'])\n",
        "\n",
        "    # 선형 보간 적용\n",
        "    merged_data = merged_data.interpolate(method='time')\n",
        "\n",
        "    ### Activity 데이터의 그룹화 적용\n",
        "    # group0 : 0 (IN_VEHICLE), 1 (ON_BICYCLE), 2 (ON_FOOT), 7 (WALKING), 8 (RUNNING), 5 (TILTING)\n",
        "    # group1 : 3 (STILL)\n",
        "    # group2 : 4 (UNKNOWN)\n",
        "    activity_mapping = {\n",
        "        0: 1,\n",
        "        1: 1,\n",
        "        2: 1,\n",
        "        7: 1,\n",
        "        8: 2,\n",
        "        5: 1,\n",
        "        3: 0,\n",
        "        4: 0\n",
        "    }\n",
        "    merged_data['m_activity'] = merged_data['m_activity'].map(activity_mapping)\n",
        "\n",
        "    # subject_id와 date를 추가\n",
        "    merged_data['subject_id'] = user\n",
        "    merged_data['lifelog_date'] = date\n",
        "\n",
        "    return merged_data\n",
        "\n",
        "def plot_time_series(data, user, date, channel_name):\n",
        "\n",
        "    # x축을 00:00:00부터 23:59:59까지 고정\n",
        "    total_seconds = 86400\n",
        "    time_range = pd.date_range(start=datetime.strptime(date, '%Y-%m-%d'), periods=total_seconds, freq='S')\n",
        "\n",
        "    # 데이터를 시간 단위로 정렬\n",
        "    data = data.reindex(time_range)\n",
        "\n",
        "    # 시계열 이미지 생성\n",
        "    fig, axes = plt.subplots(5, 1, figsize=(5, 5), sharex=True, facecolor='black')\n",
        "    fig.patch.set_facecolor('black')\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.set_facecolor('black')\n",
        "        ax.spines['top'].set_visible(False)           # Hide the top spine\n",
        "        ax.spines['right'].set_visible(False)         # Hide the right spine\n",
        "        ax.spines['left'].set_visible(False)          # Hide the left spine\n",
        "        ax.spines['bottom'].set_visible(False)        # Hide the bottom spine\n",
        "\n",
        "    # 설정한 시간 범위에 맞게 x축 설정\n",
        "    for ax in axes:\n",
        "        ax.set_xlim([time_range[0], time_range[-1]])\n",
        "\n",
        "    # plot\n",
        "    if 'heart_rate' in data.columns:\n",
        "        axes[0].plot(data.index, data['heart_rate'], color='white')\n",
        "    if 'm_activity' in data.columns:\n",
        "        axes[1].plot(data.index, data['m_activity'], color='white')\n",
        "    if 'step' in data.columns:\n",
        "        axes[2].plot(data.index, data['step'], color='white')\n",
        "    if 'm_light' in data.columns:\n",
        "        axes[3].plot(data.index, data['m_light'], color='white')\n",
        "    if 'w_light' in data.columns:\n",
        "        axes[4].plot(data.index, data['w_light'], color='white')\n",
        "\n",
        "    plt.tight_layout()  # Make the layout tight\n",
        "    fname = f'{path}{channel_name}/user{user}_{date}_{channel_name}.png'\n",
        "    plt.savefig(fname)\n",
        "    # print(fname)\n",
        "    # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zdMb8wEs8c74"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "channel_name = 'ch5'\n",
        "\n",
        "# train test 데이터 합치기\n",
        "a1 = train[['subject_id', 'lifelog_date']].copy()\n",
        "a2 = test[['subject_id', 'lifelog_date']].copy()\n",
        "val_df = pd.concat([a1,a2]).reset_index(drop=True)\n",
        "print('# train:',len(train))\n",
        "print('# test:',len(test))\n",
        "print('# 전체 데이터:',len(val_df))\n",
        "\n",
        "# 파일명\n",
        "val_df = val_df[['subject_id', 'lifelog_date']].copy()\n",
        "val_df['filename'] = val_df.apply(lambda x: f\"user{x['subject_id']}_{x['lifelog_date']}_{channel_name}.png\", axis=1)\n",
        "\n",
        "# 만들어진 이미지\n",
        "image_dir = f'{path}{channel_name}'\n",
        "image_files = [f for f in os.listdir(image_dir) if f.endswith(f'_{channel_name}.png')]\n",
        "\n",
        "# 남은 샘플\n",
        "val_df = val_df.loc[~val_df['filename'].isin(image_files),:].reset_index(drop=True)\n",
        "print('# 남은 샘플수:',len(val_df))\n",
        "\n",
        "# ====================================\n",
        "# 샘플 테스트\n",
        "# ====================================\n",
        "# rules = (\n",
        "#   (val_df['subject_id']=='id01') & (val_df['lifelog_date'].isin(['2024-07-01']))\n",
        "# )\n",
        "# val_df = val_df.loc[rules,:].copy().head(1)\n",
        "\n",
        "# 이미지 생성\n",
        "bar = tqdm(range(val_df.shape[0]))\n",
        "for idx in bar:\n",
        "    user, date, *rest = val_df.iloc[idx].values\n",
        "    bar.set_description(f'user: {user}, date: {date}')\n",
        "    merged_data = merge_data_for_group(user, date)\n",
        "    plot_time_series(merged_data, user, date, channel_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5_sNQjeX8Zb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNSFIDUvX8dG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hC3NWqhDybU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FHA2ZwrDyuf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📌 모델 학습"
      ],
      "metadata": {
        "id": "8qrXzX72JA0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 경로 설정\n",
        "dataset_path = f'{path}{channel_name}'\n",
        "\n",
        "# 이미지 크기 설정 (Resize에 사용할 값)\n",
        "image_size = 500\n",
        "\n",
        "def find_img_mean_std(dataset_path,image_size):\n",
        "\n",
        "  import torch\n",
        "  import os\n",
        "  from torchvision import transforms\n",
        "  from PIL import Image\n",
        "\n",
        "  # 전처리 파이프라인 (Normalize 제외)\n",
        "  transform = transforms.Compose([\n",
        "      transforms.Resize(image_size),\n",
        "      transforms.ToTensor(),  # [0, 255] -> [0, 1]로 스케일링\n",
        "  ])\n",
        "\n",
        "  # PNG 파일 목록 가져오기\n",
        "  image_files = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if f.endswith('.png')]\n",
        "\n",
        "  # 이미지 로드 및 텐서 변환\n",
        "  images = []\n",
        "  for img_file in image_files:\n",
        "      try:\n",
        "          img = Image.open(img_file).convert('RGB')  # RGB로 강제 변환\n",
        "          tensor_img = transform(img)  # [C, H, W] 형태\n",
        "          images.append(tensor_img)\n",
        "      except Exception as e:\n",
        "          print(f\"이미지 로드 실패: {img_file} - {e}\")\n",
        "\n",
        "  # 모든 이미지를 하나의 텐서로 결합\n",
        "  # shape: [N, C, H, W] (N: 이미지 수, C: 채널, H: 높이, W: 너비)\n",
        "  all_images = torch.stack(images, dim=0)\n",
        "\n",
        "  # 채널별 평균 및 표준편차 계산\n",
        "  # 평균: [N, C, H, W] → [C,] (모든 이미지, 모든 픽셀에 대한 평균)\n",
        "  # 표준편차: 동일한 방식으로 계산\n",
        "  mean = all_images.mean(dim=[0, 2, 3])  # [C,] (예: [R, G, B])\n",
        "  std = all_images.std(dim=[0, 2, 3])    # [C,] (예: [R, G, B])\n",
        "\n",
        "  # 결과 출력\n",
        "  print(\"평균(mean):\", mean.tolist())\n",
        "  print(\"표준편차(std):\", std.tolist())\n",
        "\n",
        "  return mean.tolist(), std.tolist()\n",
        "\n",
        "img_mean, img_std = find_img_mean_std(dataset_path,image_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd5VbPgmnRZC",
        "outputId": "870a2606-f75e-44de-c83f-c73fc312f604"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "평균(mean): [0.04598776996135712, 0.04598776996135712, 0.04598776996135712]\n",
            "표준편차(std): [0.1984541118144989, 0.1984541118144989, 0.1984541118144989]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NTHFNnZH2HJy"
      },
      "outputs": [],
      "source": [
        "def extract_cnn_features(\n",
        "    image_root_dir,\n",
        "    output_csv_path,\n",
        "    img_mean, img_std,\n",
        "    batch_size=32,\n",
        "    image_size=(500, 500),\n",
        "    model_name='resnet50'\n",
        "):\n",
        "    # 이미지 확장자 허용 목록\n",
        "    valid_exts = {'.png'}\n",
        "\n",
        "    # 이미지 경로 수집\n",
        "    def collect_image_paths(root_dir):\n",
        "        image_paths = []\n",
        "        for root, _, files in os.walk(root_dir):\n",
        "            for fname in files:\n",
        "                if os.path.splitext(fname)[1].lower() in valid_exts:\n",
        "                    image_paths.append(os.path.join(root, fname))\n",
        "        return image_paths\n",
        "\n",
        "    # Dataset 정의\n",
        "    class ImageDataset(Dataset):\n",
        "        def __init__(self, image_paths, transform=None):\n",
        "            self.image_paths = image_paths\n",
        "            self.transform = transform\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.image_paths)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            path = self.image_paths[idx]\n",
        "            image = Image.open(path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, os.path.relpath(path)\n",
        "\n",
        "    # Transform & 모델\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=img_mean, std=img_std)\n",
        "    ])\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # 모델 선택\n",
        "    model = getattr(models, model_name)(pretrained=True)\n",
        "    feature_extractor = torch.nn.Sequential(*list(model.children())[:-1]).to(device)\n",
        "    feature_extractor.eval()\n",
        "\n",
        "    # 데이터로더 생성\n",
        "    image_paths = collect_image_paths(image_root_dir)\n",
        "    dataset = ImageDataset(image_paths, transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Feature 추출\n",
        "    all_features = []\n",
        "    all_names = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, names in tqdm(dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            feats = feature_extractor(imgs)\n",
        "            feats = feats.view(feats.size(0), -1).cpu()\n",
        "            all_features.append(feats)\n",
        "            all_names.extend(names)\n",
        "\n",
        "    features_tensor = torch.cat(all_features, dim=0)\n",
        "    df = pd.DataFrame(features_tensor.numpy())\n",
        "    df.insert(0, 'image_path', all_names)\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "    print(f\">> Features saved to: {output_csv_path}\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "channel_name = 'ch5'\n",
        "\n",
        "# 이미지 파생변수 생성\n",
        "img_features = extract_cnn_features(\n",
        "    image_root_dir=f'{path}{channel_name}',\n",
        "    output_csv_path='img_features.csv',\n",
        "    img_mean=img_mean, img_std=img_std,\n",
        "    batch_size=32,\n",
        "    image_size=(500, 500),\n",
        "    model_name='resnet50'  # 다른 모델: 'resnet18', 'resnet101', 'efficientnet_b0' 등도 가능\n",
        ")\n",
        "\n",
        "# check\n",
        "img_features.head()"
      ],
      "metadata": {
        "id": "z-IZ_vjQJEcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52bb947d-c871-46b6-e579-e293e4ea56d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 9/22 [01:34<02:16, 10.53s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zN5PTGH4JEiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TCYm2HuNJEkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iKa9efqNJEm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FCrgC4pSJEpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X_A8ennhJEsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7PZUo6TVJEun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RLT2TiKsJExf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H401mU5CJEzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EckHVeXuJE28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Toukt4KNJE58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rdo8wqFTJFA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x4rBoUTMJFDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YrOr4zTNJFHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiMLigXg-ZuE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCYE6A1N6foi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO6CHRkylX0VecrYSLD5ALA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}