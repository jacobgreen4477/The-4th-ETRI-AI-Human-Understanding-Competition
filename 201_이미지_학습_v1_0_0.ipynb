{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobgreen4477/The-4th-ETRI-AI-Human-Understanding-Competition/blob/main/201_%EC%9D%B4%EB%AF%B8%EC%A7%80_%ED%95%99%EC%8A%B5_v1_0_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTgURBTcpY0Q"
      },
      "source": [
        "> title : ì œ 4íšŒ ETRI íœ´ë¨¼ì´í•´ ì¸ê³µì§€ëŠ¥ ë…¼ë¬¸ê²½ì§„ëŒ€íšŒ <br>\n",
        "> author : hjy <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QhncbejZIfV"
      },
      "source": [
        "In our study, we used smartphones, smartwatches, sleep sensors, and self-recording apps to collect daily life logs and sleep health records of study participants in 2024.The data collection procedures and methods followed a similar approach to those used in previous studies. Here, we puï»¿blicly provide the following 12 data items, which comprise a total of 700 days' worth of lifelog data, strictly for non-commercial and academic research purposes only.\n",
        "- mACStatus: Indicates whether the smartphone is currently being charged.\n",
        "- mActivity: Value calculated by the Google Activity Recognition API.\n",
        "- mAmbience: Ambient sound identification labels and their respective probabilities.\n",
        "- mBle: Bluetooth devices around individual subject.\n",
        "- mGps: Multiple GPS coordinates measured within a single minute using the smartphone.\n",
        "- mLight: Ambient light measured by the smartphone.\n",
        "- mScreenStatus: Indicates whether the smartphone screen is in use.\n",
        "- mUsageStats: Indicates which apps were used on the smartphone and for how long.\n",
        "- mWifi: Wifi devices around individual subject.\n",
        "- wHr: Heart rate readings recorded by the smartwatch.\n",
        "- wLight: Ambient light measured by the smartwatch.\n",
        "- wPedo: Step data recorded by the smartwatch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkY5S7k0ZLFG"
      },
      "source": [
        "For the purpose of training a learning model to predict sleep health, fatigue, and stress, the following six metrics were derived from sleep sensor data and self-reported survey records. Each metric consists of values categorized into either two levels (0, 1) or three levels (0, 1, 2), depending on the specific metric. The detailed classification criteria for each metric's levels will be provided in a separate document.These\n",
        "metrics assign a value of 0 for sleep records that do not meet the recommended guidelines.For instance, the first questionnaire metric (Q1) is assigned a value of 1 on days when an\n",
        "individualâ€™s self-reported sleep quality exceeds their average over the experimental period, and 0 when it\n",
        "falls below that average. Similarly, the second and third metrics (Q2 and Q3) are assigned a value of 0\n",
        "on days when the participantâ€™s fatigue and stress levels, respectively, exceed their average, and a value of\n",
        "1 when these levels are below average.\n",
        "\n",
        "- Q1: Overall sleep quality as perceived by a subject immediately after waking up.\n",
        "- Q2: Physical fatigue of a subject just before sleep.\n",
        "- Q3: Stress level experienced by a subject just before sleep.\n",
        "- S1: Adherence to sleep guidelines for total sleep time (TST).\n",
        "- S2: Adherence to sleep guidelines for sleep efficiency (SE).\n",
        "- S3: Adherence to sleep guidelines for sleep onset latency (SOL, or SL).\n",
        "\n",
        "ìˆ˜ë©´ ê±´ê°•, í”¼ë¡œ, ìŠ¤íŠ¸ë ˆìŠ¤ ì˜ˆì¸¡ì„ ìœ„í•œ í•™ìŠµ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•´, ìˆ˜ë©´ ì„¼ì„œ ë°ì´í„°ì™€ ìê¸° ë³´ê³ ì‹ ì„¤ë¬¸ ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒì˜ 6ê°€ì§€ ì§€í‘œë¥¼ ë„ì¶œí–ˆìŠµë‹ˆë‹¤.\n",
        "ê° ì§€í‘œëŠ” í•´ë‹¹ í•­ëª©ì— ë”°ë¼ ë‘ ìˆ˜ì¤€(0, 1) ë˜ëŠ” ì„¸ ìˆ˜ì¤€(0, 1, 2)ìœ¼ë¡œ êµ¬ë¶„ëœ ê°’ì„ ê°€ì§‘ë‹ˆë‹¤.\n",
        "ê° ì§€í‘œì˜ ì„¸ë¶€ ë¶„ë¥˜ ê¸°ì¤€ì€ ë³„ë„ì˜ ë¬¸ì„œì—ì„œ ì œê³µë  ì˜ˆì •ì…ë‹ˆë‹¤.\n",
        "\n",
        "- Q1: ê¸°ìƒ ì§í›„ ë³¸ì¸ì´ ì¸ì§€í•œ ì „ë°˜ì ì¸ ìˆ˜ë©´ì˜ ì§ˆ\n",
        " - 0: ê°œì¸ í‰ê·  ì´í•˜\n",
        " - 1: ê°œì¸ í‰ê·  ì´ìƒ\n",
        "- Q2: ì·¨ì¹¨ ì§ì „ ë³¸ì¸ì´ ëŠë‚€ ì‹ ì²´ì  í”¼ë¡œ ìˆ˜ì¤€\n",
        " - 0: ë†’ì€ í”¼ë¡œ ìˆ˜ì¤€\n",
        " - 1: ë‚®ì€ í”¼ë¡œ ìˆ˜ì¤€\n",
        "- Q3: ì·¨ì¹¨ ì§ì „ ë³¸ì¸ì´ ëŠë‚€ ìŠ¤íŠ¸ë ˆìŠ¤ ìˆ˜ì¤€\n",
        " - 0: ë†’ì€ ìŠ¤íŠ¸ë ˆìŠ¤ ìˆ˜ì¤€\n",
        " - 1: ë‚®ì€ ìŠ¤íŠ¸ë ˆìŠ¤ ìˆ˜ì¤€\n",
        "- S1: ì´ ìˆ˜ë©´ ì‹œê°„(TST) ê°€ì´ë“œë¼ì¸ì„ ì¤€ìˆ˜í–ˆëŠ”ì§€ 3LEVELS\n",
        " - 0: ê°€ì´ë“œë¼ì¸ ë¯¸ì¤€ìˆ˜\n",
        " - 1: ê°€ì´ë“œë¼ì¸ ë¶€ë¶„ì  ì¤€ìˆ˜\n",
        " - 2: ê°€ì´ë“œë¼ì¸ ì™„ì „ ì¤€ìˆ˜\n",
        "- S2: ìˆ˜ë©´ íš¨ìœ¨(SE) ê°€ì´ë“œë¼ì¸ì„ ì¤€ìˆ˜í–ˆëŠ”ì§€ ì—¬ë¶€\n",
        "- (SE: ì ìë¦¬ì— ëˆ„ì›Œ ìˆì—ˆë˜ ì „ì²´ ì‹œê°„ ëŒ€ë¹„, ì‹¤ì œë¡œ ì ë“  ì‹œê°„ì˜ ë¹„ìœ¨)\n",
        " - 0: ê°€ì´ë“œë¼ì¸ ë¯¸ì¤€ìˆ˜\n",
        " - 1: ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜\n",
        "- S3: ìˆ˜ë©´ ì ë“¤ê¸° ì§€ì—° ì‹œê°„(SOL ë˜ëŠ” SL) ê°€ì´ë“œë¼ì¸ì„ ì¤€ìˆ˜í–ˆëŠ”ì§€ ì—¬ë¶€\n",
        "- (SOL: ì ìë¦¬ì— ëˆ„ìš´ ìˆœê°„ë¶€í„° ì‹¤ì œë¡œ ì ë“œëŠ” ë°ê¹Œì§€ ê±¸ë¦° ì‹œê°„)\n",
        " - 0: ê°€ì´ë“œë¼ì¸ ë¯¸ì¤€ìˆ˜\n",
        " - 1: ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDVNXLQtLU6X"
      },
      "source": [
        "### ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN6iwVhQpR_a",
        "outputId": "f6b475cb-bbfe-480e-d46a-8eccecbb3ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting haversine\n",
            "  Downloading haversine-2.9.0-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading haversine-2.9.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Installing collected packages: haversine\n",
            "Successfully installed haversine-2.9.0\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.15.2 colorlog-6.9.0 optuna-4.3.0\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.8.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.15.3)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (0.14.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Downloading category_encoders-2.8.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.8.1\n"
          ]
        }
      ],
      "source": [
        "! pip install haversine\n",
        "! pip install optuna\n",
        "! pip install category_encoders\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "from tqdm.auto import tqdm\n",
        "from collections import Counter\n",
        "from scipy.stats import entropy\n",
        "from haversine import haversine  # ì„¤ì¹˜ í•„ìš”: pip install haversine\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFvEVmxWsRH4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import ast\n",
        "from tqdm import tqdm  # â† ì¶”ê°€\n",
        "from math import radians, cos, sin, asin, sqrt\n",
        "from datetime import time\n",
        "from datetime import timedelta\n",
        "from functools import reduce\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "import glob\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, f1_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, cross_val_score\n",
        "import lightgbm as lgb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from category_encoders import TargetEncoder\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# seed ê³ ì •\n",
        "SD = 42\n",
        "random.seed(SD)\n",
        "np.random.seed(SD)\n",
        "os.environ['PYTHONHASHSEED'] = str(SD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iVOoFq7pSCM",
        "outputId": "40e24aa9-84fb-48a0-9265-66b756598585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIbC9LQkv6T4"
      },
      "outputs": [],
      "source": [
        "# pandas ì˜µì…˜\n",
        "pd.set_option('display.max_columns', 999)\n",
        "pd.set_option('display.max_rows', 999)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.float_format', lambda x: '%0.4f' % x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38uzdH-UYh_3"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_ZQfubWYiCk"
      },
      "outputs": [],
      "source": [
        "def correct_lifelog_date_for_midnight(df, timestamp_col='timestamp', lifelog_col='lifelog_date'):\n",
        "    df = df.copy()\n",
        "    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
        "    df[lifelog_col] = pd.to_datetime(df[lifelog_col])\n",
        "\n",
        "    # ì¡°ê±´: timestampì˜ ì‹œ(hour)ê°€ 0~5ì‹œì¸ ê²½ìš°ë§Œ í•˜ë£¨ ì°¨ê°\n",
        "    mask = (df[timestamp_col].dt.hour >= 0) & (df[timestamp_col].dt.hour < 6)\n",
        "    df.loc[mask, lifelog_col] = df.loc[mask, lifelog_col] - pd.Timedelta(days=1)\n",
        "\n",
        "    # lifelog_dateë¥¼ ë¬¸ìì—´ë¡œ ë°”ê¾¸ëŠ” ê²½ìš°\n",
        "    df[lifelog_col] = df[lifelog_col].dt.date.astype(str)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ikO0GN_KxyQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EvDe55ejzKR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQgtvPb3jzQv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEHsA6naKx0G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BodxdJFiv_DJ"
      },
      "source": [
        "### ğŸ“¦ ë°ì´í„° ì½ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw0cx3wwpSE2"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/data/ch2025_data_items/'\n",
        "\n",
        "# 1\n",
        "mACStatus = pd.read_parquet(path+'ch2025_mACStatus.parquet')\n",
        "mActivity = pd.read_parquet(path+'ch2025_mActivity.parquet')\n",
        "mAmbience = pd.read_parquet(path+'ch2025_mAmbience.parquet')\n",
        "mBle = pd.read_parquet(path+'ch2025_mBle.parquet')\n",
        "mGps = pd.read_parquet(path+'ch2025_mGps.parquet')\n",
        "mLight = pd.read_parquet(path+'ch2025_mLight.parquet')\n",
        "mScreenStatus = pd.read_parquet(path+'ch2025_mScreenStatus.parquet')\n",
        "mUsageStats = pd.read_parquet(path+'ch2025_mUsageStats.parquet')\n",
        "mWifi = pd.read_parquet(path+'ch2025_mWifi.parquet')\n",
        "wHr = pd.read_parquet(path+'ch2025_wHr.parquet')\n",
        "wLight = pd.read_parquet(path+'ch2025_wLight.parquet')\n",
        "wPedo = pd.read_parquet(path+'ch2025_wPedo.parquet')\n",
        "\n",
        "# 2\n",
        "train = pd.read_csv('/content/drive/MyDrive/data/ch2025_metrics_train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/data/ch2025_submission_sample.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tk45v0V5xiay"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "mACStatus['lifelog_date'] = mACStatus['timestamp'].astype(str).str[:10]\n",
        "mActivity['lifelog_date'] = mActivity['timestamp'].astype(str).str[:10]\n",
        "\n",
        "# def extract_labels_and_probs(row):\n",
        "#     items = row['m_ambience']\n",
        "#     labels = [item[0] for item in items]\n",
        "#     probs = [item[1] for item in items]\n",
        "#     return pd.Series({'labels': labels, 'prob': probs})\n",
        "\n",
        "# mAmbience[['labels', 'prob']]  = mAmbience.apply(extract_labels_and_probs, axis=1)\n",
        "# mAmbience['lifelog_date'] = mAmbience['timestamp'].astype(str).str[:10]\n",
        "# mAmbience = mAmbience.drop(columns=['m_ambience'])\n",
        "\n",
        "# def extract_mble_info(row):\n",
        "#     m_data = row['m_ble']\n",
        "#     address = [item['address'] for item in m_data]\n",
        "#     device_class = [item['device_class'] for item in m_data]\n",
        "#     rssi = [item['rssi'] for item in m_data]\n",
        "#     return pd.Series({'address': address, 'device_class': device_class, 'rssi': rssi})\n",
        "\n",
        "# mBle[['address','device_class','rssi']] = mBle.apply(extract_mble_info, axis=1)\n",
        "# mBle['lifelog_date'] = mBle['timestamp'].astype(str).str[:10]\n",
        "\n",
        "# def extract_gps_info(row):\n",
        "#     m_data = row['m_gps']\n",
        "#     altitude = [item['altitude'] for item in m_data]\n",
        "#     latitude = [item['latitude'] for item in m_data]\n",
        "#     longitude = [item['longitude'] for item in m_data]\n",
        "#     speed = [item['speed'] for item in m_data]\n",
        "#     return pd.Series({'altitude': altitude, 'latitude': latitude, 'longitude': longitude, 'speed': speed})\n",
        "\n",
        "# mGps[['altitude','latitude','longitude','speed']] = mGps.apply(extract_gps_info, axis=1)\n",
        "# mGps['lifelog_date'] = mGps['timestamp'].astype(str).str[:10]\n",
        "# mGps = mGps.drop(columns=['m_gps'])\n",
        "\n",
        "mLight['lifelog_date'] = mLight['timestamp'].astype(str).str[:10]\n",
        "mScreenStatus['lifelog_date'] = mScreenStatus['timestamp'].astype(str).str[:10]\n",
        "\n",
        "# def extract_mUsageStats_info(row):\n",
        "#     m_data = row['m_usage_stats']\n",
        "#     app_name = [item['app_name'] for item in m_data]\n",
        "#     total_time = [item['total_time'] for item in m_data]\n",
        "#     return pd.Series({'app_name': app_name, 'total_time': total_time})\n",
        "\n",
        "# mUsageStats[['app_name', 'total_time']] = mUsageStats.apply(extract_mUsageStats_info, axis=1)\n",
        "# mUsageStats['lifelog_date'] = mUsageStats['timestamp'].astype(str).str[:10]\n",
        "\n",
        "# def extract_wifi_info(row):\n",
        "#     wifi_data = row['m_wifi']\n",
        "#     bssids = [item['bssid'] for item in wifi_data]\n",
        "#     rssis = [item['rssi'] for item in wifi_data]\n",
        "#     return pd.Series({'bssid': bssids, 'rssi': rssis})\n",
        "\n",
        "# mWifi[['bssid', 'rssi']] = mWifi.apply(extract_wifi_info, axis=1)\n",
        "# mWifi['lifelog_date'] = mWifi['timestamp'].astype(str).str[:10]\n",
        "\n",
        "wHr['lifelog_date'] = wHr['timestamp'].astype(str).str[:10]\n",
        "wLight['lifelog_date'] = wLight['timestamp'].astype(str).str[:10]\n",
        "wPedo['lifelog_date'] = wPedo['timestamp'].astype(str).str[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp0et2XoiBZ7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE-06HN6gtqy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBZKGmxSRBQW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAyS_RTHjjp3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp40u0R88PY_"
      },
      "source": [
        "### ğŸ“Œ ì´ë¯¸ì§€ ìƒì„±\n",
        "- spleeptimeë§Œ ì¶”ì¶œ (00ì‹œë¶€í„° 06ì‹œê¹Œì§€)\n",
        "- ì°¸ê³  : https://github.com/seongjiko/Pixleep/tree/master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-qgvZd-8cir"
      },
      "outputs": [],
      "source": [
        "def filter_by_group_size(df, group_cols=['subject_id', 'lifelog_date']):\n",
        "    # ê·¸ë£¹ë³„ ê±´ìˆ˜ ê³„ì‚°\n",
        "    group_counts = df.groupby(group_cols).size().reset_index(name='count')\n",
        "    # í‰ê·  ê±´ìˆ˜ ê³„ì‚°\n",
        "    mean_count = group_counts['count'].mean()\n",
        "    # í‰ê·  ì´ˆê³¼ ê·¸ë£¹ë§Œ ì¶”ì¶œ\n",
        "    valid_groups = group_counts[group_counts['count'] > mean_count*0.5][group_cols]\n",
        "    # ì›ë³¸ê³¼ inner joinìœ¼ë¡œ í•„í„°ë§\n",
        "    return df.merge(valid_groups, on=group_cols, how='inner')\n",
        "\n",
        "def make_timestamps_unique(df, timestamp_col='timestamp'):\n",
        "    # 'timestamp' ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬\n",
        "    df = df.sort_values(by=[timestamp_col])\n",
        "    # ê° 'timestamp'ê°€ ì¤‘ë³µëœ íšŸìˆ˜ë¥¼ ì„¸ì–´ ë‚˜ë…¸ì´ˆ ë‹¨ìœ„ë¡œ ì¦ê°€ì‹œí‚´\n",
        "    df[timestamp_col] = df[timestamp_col] + pd.to_timedelta(df.groupby(timestamp_col).cumcount(), unit='ns')\n",
        "    return df\n",
        "\n",
        "def average_list_columns(df, list_columns, pk_cols=['subject_id', 'lifelog_date']):\n",
        "\n",
        "    for col in list_columns:\n",
        "\n",
        "        def safe_mean(x):\n",
        "            if isinstance(x, list):\n",
        "                return np.mean(x) if len(x) > 0 else np.nan\n",
        "            elif isinstance(x, (int, float, np.integer, np.floating, type(None))):\n",
        "                return x\n",
        "            elif isinstance(x, (np.ndarray, pd.Series)):\n",
        "                return np.mean(x)\n",
        "            elif pd.api.types.is_scalar(x) and pd.isna(x):\n",
        "                return np.nan\n",
        "            else:\n",
        "                return np.nan\n",
        "\n",
        "        df[col] = df[col].apply(safe_mean)\n",
        "\n",
        "    return df\n",
        "\n",
        "def center_list_values(df, list_columns):\n",
        "    for col in list_columns:\n",
        "        def center(x):\n",
        "            if isinstance(x, list) and len(x) > 0:\n",
        "                mean = np.mean(x)\n",
        "                return [np.round(v - mean,3) for v in x]\n",
        "            return x  # NaNì´ë‚˜ ë¹„ë¦¬ìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€\n",
        "        df[col] = df[col].apply(center)\n",
        "    return df\n",
        "\n",
        "def sleeptime_cutter(data): # ì ìëŠ” ì‹œê°„ ë°ì´í„°ê°€ ë” ì¤‘ìš”í•œì§€ ì‹¤í—˜(ğŸ”¥ğŸ”¥ğŸ”¥)\n",
        "\n",
        "    data_filtered = data.copy()\n",
        "    data_filtered['timestamp'] = pd.to_datetime(data_filtered['timestamp'])\n",
        "    data_filtered['lifelog_date'] = pd.to_datetime(data_filtered['lifelog_date'])\n",
        "\n",
        "    # spleeptimeë§Œ ì¶”ì¶œ (00ì‹œë¶€í„° 06ì‹œê¹Œì§€)\n",
        "    data_filtered = data_filtered[(data_filtered['timestamp'].dt.hour >= 0) & (data_filtered['timestamp'].dt.hour < 6)]\n",
        "\n",
        "    # í•˜ë£¨ ì°¨ê°\n",
        "    data_filtered['timestamp'] = data_filtered['timestamp'] - pd.Timedelta(days=1)\n",
        "    data_filtered['lifelog_date'] = data_filtered['lifelog_date'] - pd.Timedelta(days=1)\n",
        "    # print('>> D-1 í•˜ë£¨ ì°¨ê°! (lifelog_date ì‹¤ì œ ì¼ìëŠ” D+1 ìƒˆë²½(0~6ì‹œ) ë°ì´í„°ì„)')\n",
        "\n",
        "    # lifelog_dateë¥¼ ë‹¤ì‹œ ë¬¸ìì—´ë¡œ\n",
        "    data_filtered['lifelog_date'] = data_filtered['lifelog_date'].dt.date.astype(str)\n",
        "\n",
        "    return data_filtered\n",
        "\n",
        "def merge_data_for_group(user, date):\n",
        "\n",
        "    # ë°ì´í„° ë¡œë“œ\n",
        "    # acc_group = mGps.copy()\n",
        "    activity_group = mActivity.copy()\n",
        "    hr_group = wHr.copy()\n",
        "    wPedo_group = wPedo[['subject_id','timestamp','lifelog_date','step']].copy()\n",
        "    mLight_group = mLight[['subject_id','timestamp','lifelog_date','m_light']].copy()\n",
        "    wLight_group = wLight[['subject_id','timestamp','lifelog_date','w_light']].copy()\n",
        "\n",
        "    # ê±´ìˆ˜ê°€ ì—†ëŠ” ì¼ì ì´ìƒì¹˜ë¡œ íŒë‹¨í•˜ê³  ì œì™¸\n",
        "    activity_group = filter_by_group_size(activity_group)\n",
        "    hr_group = filter_by_group_size(hr_group)\n",
        "    wPedo_group = filter_by_group_size(wPedo_group)\n",
        "    mLight_group = filter_by_group_size(mLight_group)\n",
        "    wLight_group = filter_by_group_size(wLight_group)\n",
        "\n",
        "    # sleeptimeë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ (ğŸ”¥ğŸ”¥ğŸ”¥)\n",
        "    # activity_group = sleeptime_cutter(activity_group)\n",
        "    # hr_group = sleeptime_cutter(hr_group)\n",
        "    # wPedo_group = sleeptime_cutter(wPedo_group)\n",
        "    # mLight_group = sleeptime_cutter(mLight_group)\n",
        "    # wLight_group = sleeptime_cutter(wLight_group)\n",
        "\n",
        "    # í•„í„°\n",
        "    activity_group = activity_group.loc[(activity_group['subject_id']==user) & (activity_group['lifelog_date']==date),:]\n",
        "    hr_group = hr_group.loc[(hr_group['subject_id']==user) & (hr_group['lifelog_date']==date),:]\n",
        "    wPedo_group = wPedo_group.loc[(wPedo_group['subject_id']==user) & (wPedo_group['lifelog_date']==date),:]\n",
        "    mLight_group = mLight_group.loc[(mLight_group['subject_id']==user) & (mLight_group['lifelog_date']==date),:]\n",
        "    wLight_group = wLight_group.loc[(wLight_group['subject_id']==user) & (wLight_group['lifelog_date']==date),:]\n",
        "\n",
        "    # ë¦¬ìŠ¤íŠ¸ í‰ê· ê°’ìœ¼ë¡œ ë³€í™˜\n",
        "    # acc_group = average_list_columns(acc_group, ['altitude', 'latitude', 'longitude','speed'])\n",
        "    hr_group = average_list_columns(hr_group, ['heart_rate'])\n",
        "\n",
        "    # 'timestamp'ë¥¼ ê³ ìœ í•˜ê²Œ ë§Œë“¦\n",
        "    # acc_group = make_timestamps_unique(acc_group)\n",
        "    activity_group = make_timestamps_unique(activity_group)\n",
        "    hr_group = make_timestamps_unique(hr_group)\n",
        "    wPedo_group = make_timestamps_unique(wPedo_group)\n",
        "    mLight_group = make_timestamps_unique(mLight_group)\n",
        "    wLight_group = make_timestamps_unique(wLight_group)\n",
        "\n",
        "    # 'timestamp'ë¥¼ ì¸ë±ìŠ¤ë¡œ ì„¤ì •í•˜ê³  'subject_id'ì™€ 'date' ì»¬ëŸ¼ ì œê±°\n",
        "    # mAcc_data = acc_group.set_index('timestamp').drop(columns=['subject_id', 'lifelog_date']).resample('S').nearest()\n",
        "    activity_data = activity_group.set_index('timestamp').drop(columns=['subject_id', 'lifelog_date']).resample('S').nearest()\n",
        "    e4Hr_data = hr_group.set_index('timestamp').drop(columns=['subject_id', 'lifelog_date']).resample('S').nearest()\n",
        "    wPedo_data = wPedo_group.set_index('timestamp').drop(columns=['subject_id', 'lifelog_date']).resample('S').nearest()\n",
        "    mLight_data = mLight_group.set_index('timestamp').drop(columns=['subject_id', 'lifelog_date']).resample('S').nearest()\n",
        "    wLight_data = wLight_group.set_index('timestamp').drop(columns=['subject_id', 'lifelog_date']).resample('S').nearest()\n",
        "\n",
        "    # í•˜ë£¨ 86400ì´ˆì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±\n",
        "    start_time = datetime.strptime(date, '%Y-%m-%d')\n",
        "    end_time = start_time + timedelta(days=1)\n",
        "    all_timestamps = pd.date_range(start=start_time, end=end_time, freq='S', inclusive='left')\n",
        "    merged_data = pd.DataFrame(index=all_timestamps)\n",
        "    merged_data.index.name = 'timestamp'\n",
        "\n",
        "    # ë°ì´í„° ë³‘í•©\n",
        "    # if not mAcc_data.empty:\n",
        "    #     merged_data = merged_data.join(mAcc_data, how='left')\n",
        "    if not e4Hr_data.empty:\n",
        "        merged_data = merged_data.join(e4Hr_data, how='left')\n",
        "    if not activity_data.empty:\n",
        "        merged_data = merged_data.join(activity_data, how='left')\n",
        "    if not wPedo_data.empty:\n",
        "        merged_data = merged_data.join(wPedo_data, how='left')\n",
        "    if not mLight_data.empty:\n",
        "        merged_data = merged_data.join(mLight_data, how='left')\n",
        "    if not wLight_data.empty:\n",
        "        merged_data = merged_data.join(wLight_data, how='left')\n",
        "\n",
        "    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ìœ ì§€í•˜ê³  NaN ê°’ìœ¼ë¡œ ì±„ìš°ê¸°\n",
        "    # merged_data = merged_data.reindex(columns=['altitude', 'latitude', 'longitude', 'speed', 'heart_rate', 'm_activity', 'step'])\n",
        "    merged_data = merged_data.reindex(columns=['heart_rate', 'm_activity', 'step', 'm_light', 'w_light'])\n",
        "\n",
        "    # ì„ í˜• ë³´ê°„ ì ìš©\n",
        "    merged_data = merged_data.interpolate(method='time')\n",
        "\n",
        "    ### Activity ë°ì´í„°ì˜ ê·¸ë£¹í™” ì ìš©\n",
        "    # group0 : 0 (IN_VEHICLE), 1 (ON_BICYCLE), 2 (ON_FOOT), 7 (WALKING), 8 (RUNNING), 5 (TILTING)\n",
        "    # group1 : 3 (STILL)\n",
        "    # group2 : 4 (UNKNOWN)\n",
        "    activity_mapping = {\n",
        "        0: 1,\n",
        "        1: 1,\n",
        "        2: 1,\n",
        "        7: 1,\n",
        "        8: 2,\n",
        "        5: 1,\n",
        "        3: 0,\n",
        "        4: 0\n",
        "    }\n",
        "    merged_data['m_activity'] = merged_data['m_activity'].map(activity_mapping)\n",
        "\n",
        "    # subject_idì™€ dateë¥¼ ì¶”ê°€\n",
        "    merged_data['subject_id'] = user\n",
        "    merged_data['lifelog_date'] = date\n",
        "\n",
        "    return merged_data\n",
        "\n",
        "def plot_time_series(data, user, date, channel_name):\n",
        "\n",
        "    # xì¶•ì„ 00:00:00ë¶€í„° 23:59:59ê¹Œì§€ ê³ ì •\n",
        "    total_seconds = 86400\n",
        "    time_range = pd.date_range(start=datetime.strptime(date, '%Y-%m-%d'), periods=total_seconds, freq='S')\n",
        "\n",
        "    # ë°ì´í„°ë¥¼ ì‹œê°„ ë‹¨ìœ„ë¡œ ì •ë ¬\n",
        "    data = data.reindex(time_range)\n",
        "\n",
        "    # ì‹œê³„ì—´ ì´ë¯¸ì§€ ìƒì„±\n",
        "    fig, axes = plt.subplots(5, 1, figsize=(5, 5), sharex=True, facecolor='black')\n",
        "    fig.patch.set_facecolor('black')\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.set_facecolor('black')\n",
        "        ax.spines['top'].set_visible(False)           # Hide the top spine\n",
        "        ax.spines['right'].set_visible(False)         # Hide the right spine\n",
        "        ax.spines['left'].set_visible(False)          # Hide the left spine\n",
        "        ax.spines['bottom'].set_visible(False)        # Hide the bottom spine\n",
        "\n",
        "    # ì„¤ì •í•œ ì‹œê°„ ë²”ìœ„ì— ë§ê²Œ xì¶• ì„¤ì •\n",
        "    for ax in axes:\n",
        "        ax.set_xlim([time_range[0], time_range[-1]])\n",
        "\n",
        "    # plot\n",
        "    if 'heart_rate' in data.columns:\n",
        "        axes[0].plot(data.index, data['heart_rate'], color='white')\n",
        "    if 'm_activity' in data.columns:\n",
        "        axes[1].plot(data.index, data['m_activity'], color='white')\n",
        "    if 'step' in data.columns:\n",
        "        axes[2].plot(data.index, data['step'], color='white')\n",
        "    if 'm_light' in data.columns:\n",
        "        axes[3].plot(data.index, data['m_light'], color='white')\n",
        "    if 'w_light' in data.columns:\n",
        "        axes[4].plot(data.index, data['w_light'], color='white')\n",
        "\n",
        "    plt.tight_layout()  # Make the layout tight\n",
        "    fname = f'{path}{channel_name}/user{user}_{date}_{channel_name}.png'\n",
        "    plt.savefig(fname)\n",
        "    # print(fname)\n",
        "    # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zdMb8wEs8c74"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "channel_name = 'ch5'\n",
        "\n",
        "# train test ë°ì´í„° í•©ì¹˜ê¸°\n",
        "a1 = train[['subject_id', 'lifelog_date']].copy()\n",
        "a2 = test[['subject_id', 'lifelog_date']].copy()\n",
        "val_df = pd.concat([a1,a2]).reset_index(drop=True)\n",
        "print('# train:',len(train))\n",
        "print('# test:',len(test))\n",
        "print('# ì „ì²´ ë°ì´í„°:',len(val_df))\n",
        "\n",
        "# íŒŒì¼ëª…\n",
        "val_df = val_df[['subject_id', 'lifelog_date']].copy()\n",
        "val_df['filename'] = val_df.apply(lambda x: f\"user{x['subject_id']}_{x['lifelog_date']}_{channel_name}.png\", axis=1)\n",
        "\n",
        "# ë§Œë“¤ì–´ì§„ ì´ë¯¸ì§€\n",
        "image_dir = f'{path}{channel_name}'\n",
        "image_files = [f for f in os.listdir(image_dir) if f.endswith(f'_{channel_name}.png')]\n",
        "\n",
        "# ë‚¨ì€ ìƒ˜í”Œ\n",
        "val_df = val_df.loc[~val_df['filename'].isin(image_files),:].reset_index(drop=True)\n",
        "print('# ë‚¨ì€ ìƒ˜í”Œìˆ˜:',len(val_df))\n",
        "\n",
        "# ====================================\n",
        "# ìƒ˜í”Œ í…ŒìŠ¤íŠ¸\n",
        "# ====================================\n",
        "# rules = (\n",
        "#   (val_df['subject_id']=='id01') & (val_df['lifelog_date'].isin(['2024-07-01']))\n",
        "# )\n",
        "# val_df = val_df.loc[rules,:].copy().head(1)\n",
        "\n",
        "# ì´ë¯¸ì§€ ìƒì„±\n",
        "bar = tqdm(range(val_df.shape[0]))\n",
        "for idx in bar:\n",
        "    user, date, *rest = val_df.iloc[idx].values\n",
        "    bar.set_description(f'user: {user}, date: {date}')\n",
        "    merged_data = merge_data_for_group(user, date)\n",
        "    plot_time_series(merged_data, user, date, channel_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5_sNQjeX8Zb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNSFIDUvX8dG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hC3NWqhDybU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FHA2ZwrDyuf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ“Œ ëª¨ë¸ í•™ìŠµ"
      ],
      "metadata": {
        "id": "8qrXzX72JA0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ë¯¸ì§€ ê²½ë¡œ ì„¤ì •\n",
        "dataset_path = f'{path}{channel_name}'\n",
        "\n",
        "# ì´ë¯¸ì§€ í¬ê¸° ì„¤ì • (Resizeì— ì‚¬ìš©í•  ê°’)\n",
        "image_size = 500\n",
        "\n",
        "def find_img_mean_std(dataset_path,image_size):\n",
        "\n",
        "  import torch\n",
        "  import os\n",
        "  from torchvision import transforms\n",
        "  from PIL import Image\n",
        "\n",
        "  # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (Normalize ì œì™¸)\n",
        "  transform = transforms.Compose([\n",
        "      transforms.Resize(image_size),\n",
        "      transforms.ToTensor(),  # [0, 255] -> [0, 1]ë¡œ ìŠ¤ì¼€ì¼ë§\n",
        "  ])\n",
        "\n",
        "  # PNG íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
        "  image_files = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if f.endswith('.png')]\n",
        "\n",
        "  # ì´ë¯¸ì§€ ë¡œë“œ ë° í…ì„œ ë³€í™˜\n",
        "  images = []\n",
        "  for img_file in image_files:\n",
        "      try:\n",
        "          img = Image.open(img_file).convert('RGB')  # RGBë¡œ ê°•ì œ ë³€í™˜\n",
        "          tensor_img = transform(img)  # [C, H, W] í˜•íƒœ\n",
        "          images.append(tensor_img)\n",
        "      except Exception as e:\n",
        "          print(f\"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {img_file} - {e}\")\n",
        "\n",
        "  # ëª¨ë“  ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì˜ í…ì„œë¡œ ê²°í•©\n",
        "  # shape: [N, C, H, W] (N: ì´ë¯¸ì§€ ìˆ˜, C: ì±„ë„, H: ë†’ì´, W: ë„ˆë¹„)\n",
        "  all_images = torch.stack(images, dim=0)\n",
        "\n",
        "  # ì±„ë„ë³„ í‰ê·  ë° í‘œì¤€í¸ì°¨ ê³„ì‚°\n",
        "  # í‰ê· : [N, C, H, W] â†’ [C,] (ëª¨ë“  ì´ë¯¸ì§€, ëª¨ë“  í”½ì…€ì— ëŒ€í•œ í‰ê· )\n",
        "  # í‘œì¤€í¸ì°¨: ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ê³„ì‚°\n",
        "  mean = all_images.mean(dim=[0, 2, 3])  # [C,] (ì˜ˆ: [R, G, B])\n",
        "  std = all_images.std(dim=[0, 2, 3])    # [C,] (ì˜ˆ: [R, G, B])\n",
        "\n",
        "  # ê²°ê³¼ ì¶œë ¥\n",
        "  print(\"í‰ê· (mean):\", mean.tolist())\n",
        "  print(\"í‘œì¤€í¸ì°¨(std):\", std.tolist())\n",
        "\n",
        "  return mean.tolist(), std.tolist()\n",
        "\n",
        "img_mean, img_std = find_img_mean_std(dataset_path,image_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd5VbPgmnRZC",
        "outputId": "870a2606-f75e-44de-c83f-c73fc312f604"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "í‰ê· (mean): [0.04598776996135712, 0.04598776996135712, 0.04598776996135712]\n",
            "í‘œì¤€í¸ì°¨(std): [0.1984541118144989, 0.1984541118144989, 0.1984541118144989]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NTHFNnZH2HJy"
      },
      "outputs": [],
      "source": [
        "def extract_cnn_features(\n",
        "    image_root_dir,\n",
        "    output_csv_path,\n",
        "    img_mean, img_std,\n",
        "    batch_size=32,\n",
        "    image_size=(500, 500),\n",
        "    model_name='resnet50'\n",
        "):\n",
        "    # ì´ë¯¸ì§€ í™•ì¥ì í—ˆìš© ëª©ë¡\n",
        "    valid_exts = {'.png'}\n",
        "\n",
        "    # ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘\n",
        "    def collect_image_paths(root_dir):\n",
        "        image_paths = []\n",
        "        for root, _, files in os.walk(root_dir):\n",
        "            for fname in files:\n",
        "                if os.path.splitext(fname)[1].lower() in valid_exts:\n",
        "                    image_paths.append(os.path.join(root, fname))\n",
        "        return image_paths\n",
        "\n",
        "    # Dataset ì •ì˜\n",
        "    class ImageDataset(Dataset):\n",
        "        def __init__(self, image_paths, transform=None):\n",
        "            self.image_paths = image_paths\n",
        "            self.transform = transform\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.image_paths)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            path = self.image_paths[idx]\n",
        "            image = Image.open(path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, os.path.relpath(path)\n",
        "\n",
        "    # Transform & ëª¨ë¸\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=img_mean, std=img_std)\n",
        "    ])\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # ëª¨ë¸ ì„ íƒ\n",
        "    model = getattr(models, model_name)(pretrained=True)\n",
        "    feature_extractor = torch.nn.Sequential(*list(model.children())[:-1]).to(device)\n",
        "    feature_extractor.eval()\n",
        "\n",
        "    # ë°ì´í„°ë¡œë” ìƒì„±\n",
        "    image_paths = collect_image_paths(image_root_dir)\n",
        "    dataset = ImageDataset(image_paths, transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Feature ì¶”ì¶œ\n",
        "    all_features = []\n",
        "    all_names = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, names in tqdm(dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            feats = feature_extractor(imgs)\n",
        "            feats = feats.view(feats.size(0), -1).cpu()\n",
        "            all_features.append(feats)\n",
        "            all_names.extend(names)\n",
        "\n",
        "    features_tensor = torch.cat(all_features, dim=0)\n",
        "    df = pd.DataFrame(features_tensor.numpy())\n",
        "    df.insert(0, 'image_path', all_names)\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "    print(f\">> Features saved to: {output_csv_path}\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "channel_name = 'ch5'\n",
        "\n",
        "# ì´ë¯¸ì§€ íŒŒìƒë³€ìˆ˜ ìƒì„±\n",
        "img_features = extract_cnn_features(\n",
        "    image_root_dir=f'{path}{channel_name}',\n",
        "    output_csv_path='img_features.csv',\n",
        "    img_mean=img_mean, img_std=img_std,\n",
        "    batch_size=32,\n",
        "    image_size=(500, 500),\n",
        "    model_name='resnet50'  # ë‹¤ë¥¸ ëª¨ë¸: 'resnet18', 'resnet101', 'efficientnet_b0' ë“±ë„ ê°€ëŠ¥\n",
        ")\n",
        "\n",
        "# check\n",
        "img_features.head()"
      ],
      "metadata": {
        "id": "z-IZ_vjQJEcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52bb947d-c871-46b6-e579-e293e4ea56d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9/22 [01:34<02:16, 10.53s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zN5PTGH4JEiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TCYm2HuNJEkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iKa9efqNJEm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FCrgC4pSJEpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X_A8ennhJEsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7PZUo6TVJEun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RLT2TiKsJExf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H401mU5CJEzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EckHVeXuJE28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Toukt4KNJE58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rdo8wqFTJFA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x4rBoUTMJFDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YrOr4zTNJFHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiMLigXg-ZuE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCYE6A1N6foi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO6CHRkylX0VecrYSLD5ALA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}